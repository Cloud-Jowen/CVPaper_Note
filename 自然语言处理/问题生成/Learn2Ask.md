# 网络名称（此处应替换为具体的网络名称）https://blog.csdn.net/c9yv2cf9i06k2a9e/article/details/116111814

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/1705.00106.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract

我们研究了阅读理解中从文本段落自动生成问题。 我们为该任务引入了一个基于注意力的序列学习模型，并调查了编码句子级别信息与段落级别信息之间的效果。 与所有以前的工作不同，我们的模型不依赖于手工制作的规则或复杂的NLP流水线； 相反，它可以通过端到端的序列到序列学习进行训练。 自动评估结果表明，我们的系统在很大程度上优于最先进的基于规则的系统。 在人类评估中，我们系统的生成问题也被评为更自然（即语法正确性、流畅性），以及更难回答（即需要更多的句法和词汇差异来回答原始文本以及推理）。

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction

问题生成（QG）旨在从给定的句子或段落中产生自然的问题。 问题生成的一个关键应用是在教育领域——为阅读理解材料生成问题（Heilman 和 Smith，2010）。例如，图1显示了三个由人工生成的问题，用于测试用户对相关文本段落的理解程度。 问题生成系统也可以用作聊天机器人组件（例如，提问以启动对话或请求反馈）（Mostafazadeh 等人，2016），或者可以作为临床工具来评估或改善心理健康（Weizenbaum，1966；Colby等人，1971）。

<img width="233" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/35b04792-51eb-4d97-9f0e-21b72b89a72f">

（图1：文章第二段的样例句子，以及与之相关的自然问题及其答案。）

除了上述应用外，问题生成系统还可以帮助开发自然语言处理（NLP）数据集，例如SQuAD（Rajpurkar等，2016）和MS MARCO（Nguyen等，2016），以促进这些领域的研究。

过去，问题生成主要通过基于规则的方法来解决（例如，Mitkov 和 Ha (2003)；Rus 等人。 (2010)）。这些方法的成功在很大程度上取决于对声明句到疑问句转换的精心设计的规则的存在，通常基于深入的语言知识。

为了改进纯粹基于规则的系统，Heilman 和 Smith（2010）引入了一种过量生成和排名的方法，该方法使用基于规则的方法从输入句子中生成多个问题，然后使用基于监督学习的排名器对它们进行排序。尽管排名算法有助于产生更多可接受的问题，但它严重依赖于手动编写的特征集，而且生成的问题往往与输入句子中的词完全相同，使它们非常容易回答。

Van der Wende (2008) 指出，学习提出好的问题是自然语言处理研究中一项重要的任务，它本身应该包含对声明句法的转换。具体来说，一个听起来很自然的问题通常会压缩其基于的基础句子（例如，图 1 中的问题 3），使用段落中的同义词（例如，在问题 2 中，“形式”代替“生产”，在问题 3 中，“获得”代替“生产”）或引用来自前文的实体或从句（例如，在问题 2 中使用“光合作用”）。其他时候，世界知识被用来产生一个好的问题（例如，在问题 1 中识别“光合作用”作为一个“生活过程”）。简而言之，构建合理难度的自然问题似乎需要一种抽象方法，该方法可以生成流畅的措辞，这些措辞与它们从中提取的文本不完全匹配。

因此，与所有以前的工作不同，我们在这里提出将问题生成任务建模为一个序列到序列学习问题，该问题直接将文本段落中的句子映射到一个问题。重要的是，我们的方法完全基于数据驱动的，它不需要手动生成规则。

更具体地说，受神经机器翻译（Sutskever等人，2014；Bahdanau等人，2015）、摘要（Rush等人，2015；Iyer等人，2016）以及图像标题生成（Xu等人，2015）的启发，我们使用带有全局注意力机制的条件神经语言模型来处理问题生成。我们研究了该模型的各种变体，包括一种考虑阅读段落而非句子级别信息以及其他变体确定预训练与学习词嵌入的重要性。

在使用三个自动评估指标对SQuAD数据集（Ra-jpurkar等人，2016）进行评估时，我们发现我们的系统显著优于一系列强大的基准线，包括基于信息检索的系统（Robertson和Walker，1994），统计机器翻译方法（Koehn等人，2007）以及Heilman和Smith（2010）的过生成和排名方法。

在下面的几节中，我们讨论了相关工作（第2节），指定了任务定义（第3节）并描述了我们的神经序列学习模型（第4节）。我们在第5节解释实验设置。最后，我们给出了评估结果以及详细的分析。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

**阅读理解**  
阅读理解对机器来说是一项具有挑战性的任务，它需要理解和语言知识世界的知识（Rajpurkar等人，2016）。最近已经发布了 许多新的数据集，在这些数据集中，问题都是以合成的方式生成的。例如，bAbI（Weston等人，2016）是一个完全合成的数据集，包含20个不同的任务。 Hermann等人。（2015）通过在CNN / Daily Mail新闻文章的抽象摘要中用占位符替换实体，发布了一个填空风格的问题语料库。陈等人。（2016）声称CNN / Daily Mail数据集比以前认为的要容易，并且他们的系统几乎达到了天花板性能。理查德森等人。（2013）整理了MCTest，在其中，众包工作者的问题与四个答案选项配对。虽然MCTest包含有挑战性的自然问题，但它对于训练需要大量数据的问题回答模型来说太小了。

最近，Rajpurkar等人。 (2016) 发布了斯坦福问答数据集（SQuAD），该数据集克服了上述的小型化和半合成问题。这些问题是由众包工作者提出的，并且质量相对较高。我们在工作中使用了SQuAD，同样地，我们专注于通过自动方式生成自然的问题以供阅读理解材料。

**问题生成**  
自Rus等人（2010年）的工作以来，问题生成引起了自然语言生成（NLG）社区的关注。

大多数工作都采用基于规则的方法来解决该任务。通常，他们首先将输入句子转换为其句法表示形式，然后使用这些表示形式来生成问题。这种方法依赖于手动构建规则，并且需要对自然语言处理有深入的理解。

Heilman 和 Smith (2010) 提出了一个“生成并排序”的方法：他们的系统首先生成问题，然后对它们进行排序。尽管他们使用了学习到的排名，但该系统的性能仍然严重依赖于手工构建的生成规则。Mostafazadeh 等人。(2016) 引入了一个视觉问答任务，以探索语言与视觉之间的深层联系。Serban 等人。(2016) 提出从逻辑三元组（主语、关系、客体）生成简单事实性问题。他们的任务解决了从结构化表示到自然语言文本的映射，而且他们生成的问题在格式上具有一致性，并且比我们的少了很多分歧。

据我们所知，以往的研究没有使用端到端的方式对阅读理解进QG框架的构建，也没有采用深度序列到序列学习方法来生成问题。


<a id="3.任务定义TaskDeﬁnition"></a>
## 3.任务定义 Task Deﬁnition

在这一部分，我们定义了问题生成任务。给定输入句子x，我们的目标是生成一个与句子信息相关的自然问题y，它可以是一个任意长度的序列：$` \left [ y_1,y_2,...,y_{\left | y \right | } \right ]  `$。假设输入句子的长度为M，则x可以表示为标记序列$` \left [ x_1,x_2,...,x_m  \right ]  `$。QG任务被定义为找到$`\bar{y}`$，使得：
```math
\bar{y} = argmax_y P(y|x)
```
其中P(y|x)是给定输入x的预测问题序列y的条件对数似然。在第4.1节中，我们将详细讨论用于建模P(y|x)的全局注意力机制。

<a id="4.模型Model"></a>
## 4.模型 Model

我们的模型部分灵感来自于人类解决任务的方式。为了提出一个自然的问题，人们通常会关注输入句子的某些部分，并从段落中关联上下文信息。我们使用基于RNN编码器-解码器架构（Bahdanau等人，2015；Cho等人，2014）来建模条件概率，并采用全局注意力机制（Luong等人，2015a）使模型在解码时生成每个词时都专注于输入的某些元素。

在这里，我们研究了两种模型变体：一种只对句子进行编码，另一种则同时对句子和段落级信息进行编码。

### 4.1 解码 Decoder

与Sutskever等人(2014年)和Chopra等人(2016年)类似，我们将方程1中的条件因子分解为词级预测的乘积：

```math
P(y|x) = \prod_{t=1}^{\left | y \right | }P(y_t|x,y<t)
```

其中，每个yt的概率基于所有先前生成的单词（即y<t）以及输入句子x来预测。

更具体的说：
```math
P(y_t|x,y<t) = softmax(W_stanh(W_t[h_t;c_t]))
```

其中，w 是时间步长 t 的递归神经网络状态变量，c 是在解码时间步长 t（见第 4.2 节）对 x 进行注意力编码的结果。W_s 和 W_t 是要学习的参数。

在这里
```math
h_t = LSTM_l(y_{t-1},h_{t-1})
```

长短期记忆网络（LSTM）是一种循环神经网络，由 Hochreiter 和 Schmidhuber 于 1997 年提出。它根据之前生成的词yt−1（从词表中获取）以及前一状态ht−1来产生新的状态ht。

解码器隐藏状态的初始化区分了我们的基本模型和包含段落级信息的模型。

对于基本模型，它是由句子编码器（第4.2节）获得的句子表示s初始化的。 对于我们的段落级别模型，是通过将句子表示与段落表示连接起来来初始化的。

### 4.2 编码 Encoder

基于注意力的句子编码器被用于我们的两个模型，而段落编码器仅在包含段落级信息的模型中使用。

**Attention-based sentence encoder**  
我们使用双向LSTM来编码句子

```math
\overrightarrow{b_t} = \overrightarrow{LSTM}_2(x_t,\overrightarrow{b_{t-1}})  
\overleftarrow{b_t} = \overleftarrow{LSTM}_2(x_t,\overleftarrow{b_{t+1}})  
```

其中，$` \overrightarrow{b_t}`$ 是在前向 LSTM 中的第 t 步隐藏状态，$` \overleftarrow{b_t}`$ 是在后向 LSTM 中的第 t 步隐藏状态。

为了在解码时间步长t获得基于注意力的编码器输出$`c_t`$，即x的表示，我们首先通过$`b_t = \left | \overrightarrow{b_t};\overleftarrow{b_t}  \right | `$得到上下文相关的标记表示，然后我们在$`b_t(t=1,...|x|) `$上进行加权平均。

```math
c_t = \sum_{i=1,...|x|}a_{i,t}b_i
```


<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion










