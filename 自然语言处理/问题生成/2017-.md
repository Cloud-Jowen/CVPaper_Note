# Neural Question Generation from Text: A Preliminary Study

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
&emsp;[**3.1**](#3.1)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/1704.01792.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract

自动生成问题的目标是从文本段落中生成可以由给定文本的某些子span回答的问题。传统方法主要使用僵化的启发式规则将句子转换为相关问题。在本文中，我们提出使用神经编码器-解码器模型从自然语言句子生成有意义且多样的问题。编码器读取输入文本和答案位置，以产生一个答案感知的输入表示，该表示馈送到解码器以生成一个关注答案的问题。我们在SQuAD数据集上对神经问题生成进行了初步研究，实验结果表明我们的方法可以产生流利且多样化的问题。


<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction

自然语言文本的自动问题生成旨在根据文本作为输入生成问题，具有教育目的的潜在价值（海曼，2011）。作为问答任务的反向任务，问题生成也有可能提供大量的问答对语料库。

以前的工作主要使用严格的启发式规则将句子转换成相关问题。然而，这些方法严重依赖于人工设计的转换和生成规则，不能轻易应用于其他领域。相反，Serban等人(2016)提出了一种神经网络方法，从结构化数据中生成事实性问题。

在这项工作中，我们对使用神经网络从文本生成问题进行了初步研究，这被称为神经问题生成（NQG）框架，用于在没有预定义规则的情况下从文本文档生成自然语言问题。神经问题生成框架通过在编码器中添加答案和词汇特征来扩展序列到序列模型，以生成关注答案的问题。具体来说，编码器不仅读取输入句子，还读取答案位置指示符和词汇特征。答案位置特征表示输入句子中的答案范围，这对于生成与答案相关的问题至关重要。词汇特征包括词性标记（POS）和命名实体（NER）标签，有助于更好地编码句子。最后，具有注意力机制的解码器（Bahdanau等人，2015年）根据句子生成一个特定的答案问题。

大规模的人工注释的段落与问题对在开发问题生成系统中起着关键作用。我们建议使用最近发布的斯坦福问答数据集（SQuAD）(Rajpurkar等人，2016年)作为问题生成任务的训练和开发数据集。在SQuAD中，答案通过众包被标记为给定句子中的子序列，并且包含超过10万个问题，这使得训练我们的神经网络模型成为可能。我们在SQuAD上进行实验，实验结果表明神经网络模型能够从文本中产生流畅而多样的问题。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

在这部分，我们介绍了NQG框架，它由功能丰富的编码器和基于注意力的解码器组成。图1提供了我们NQG框架的概述。

2.1 丰富的特征编码器

在NQG框架中，我们使用门控循环单元（GRU）（Cho等人，2014年）来构建编码器。为了捕获更多的上下文信息，我们使用双向GRU（BiGRU）以正向和反向的顺序读取输入。受Chen和Manning（2014）、Nallapati等人（2016）的启发，BiGRU 编码器不仅阅读句子中的单词，还阅读手工制作的特征，生成一个单词和特征向量序列。我们将词向量、词汇特征嵌入向量和答案位置指示符嵌入向量连接起来作为 BiGRU 编码器的输入。具体来说，BiGRU 编码器读取连接后的句子词向量、词汇特征和答案位置特征 $x = (x_1, x_2, \ldots, x_n)$ 来产生两个隐藏状态序列，即前向序列 $(e_{h,1}, e_{h,2}, \ldots, e_{h,n})$ 和后向序列 $(h_1, h_2, \ldots, h_n)$。最后，编码器的输出序列是这两个序列的拼接，即 $h_i = [e_{h,i}; h_i]$。

答案位置特征为了针对句子中的特定答案生成问题，我们建议使用答案位置特征来定位目标答案。在这项工作中，使用BIO 标记方案对目标答案的位置进行标记。在这种方案中，标签B表示答案的开始，标签I 继续该答案，标签O 表示不构成答案的单词。答案位置的BIO 标签嵌入到实值向量中并通过丰富的编码器馈送。通过BIO 标记功能，答案位置被编码为隐藏向量并用于生成答案聚焦的问题。

词汇特征除了句子单词之外，我们还向编码器馈入其他词汇特征。为了编码更多的语言信息，我们选择词性、POS 和命名实体识别（NER）标签作为词汇特征。在完整的句法分析中，POS 标签功能在许多自然语言处理任务中都很重要，例如信息提取和依存关系分析（Manning 等人，1999 年）。考虑到 SQuAD 是使用维基百科文章构建的，其中包含大量命名实体，因此我们添加了 NER 特征以帮助检测它们。

2.2 基于注意力的解码器
我们使用基于注意力的 GRU 解码器来对句子进行解码并生成问题。在时间步 t，GRU 解码器读取前一个词嵌入 wt−1 和上下文向量 ct−1 来计算新的隐藏状态 st。我们通过带有最后一个反向编码器隐藏状态 h1r 的线性层来初始化解码器 GRU 隐藏状态。当前时间步 t 的上下文向量 ct 是通过连接注意力机制（Luong 等人，2015）计算得到的，该方法会根据当前解码器状态 st 和每个编码器隐藏状态 hi 计算重要性分数。然后通过对重要性分数进行加权求和来规范化得到当前上下文向量：


<a id="3.网络结构"></a>
## 3.网络结构

3 实验与结果
我们使用SQuAD数据集作为训练数据。SQuAD由536篇维基百科文章中众包工作者提出的超过10万个问题组成。我们提取句子-答案-问题三元组来构建训练、开发和测试集。由于测试集尚未公开，因此我们将开发集随机减半以构建新的开发和测试集。从训练集中提取的三元组包含86635个，开发集包含8965个，测试集包含8964个。我们在附录中介绍了实现细节。

<a id="3.1"></a>
### 3.1


<a id="4.实验Experiments"></a>
## 4.实验 Experiments

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion










