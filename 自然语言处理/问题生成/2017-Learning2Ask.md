# Learning to Ask: Neural Question Generation for Reading Comprehension

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.任务定义 Task Deﬁnition**](#3.任务定义TaskDeﬁnition)  
[**4.模型 Model**](#4.模型Model)  
  [**4.1 解码 Decoder**](#4.模型Model)  
  [**4.2 编码 Encoder**](#4.模型Model)  
  [**4.3 训练和推理 Training and Inference**](#4.模型Model)  
[**5.实验设置 Experimental Setup**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/1705.00106.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract

我们研究了阅读理解中从文本段落自动生成问题。 我们为该任务引入了一个基于注意力的序列学习模型，并调查了编码句子级别信息与段落级别信息之间的效果。 与所有以前的工作不同，我们的模型不依赖于手工制作的规则或复杂的NLP流水线； 相反，它可以通过端到端的序列到序列学习进行训练。 自动评估结果表明，我们的系统在很大程度上优于最先进的基于规则的系统。 在人类评估中，我们系统的生成问题也被评为更自然（即语法正确性、流畅性），以及更难回答（即需要更多的句法和词汇差异来回答原始文本以及推理）。

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction

问题生成（QG）旨在从给定的句子或段落中产生自然的问题。 问题生成的一个关键应用是在教育领域——为阅读理解材料生成问题（Heilman 和 Smith，2010）。例如，图1显示了三个由人工生成的问题，用于测试用户对相关文本段落的理解程度。 问题生成系统也可以用作聊天机器人组件（例如，提问以启动对话或请求反馈）（Mostafazadeh 等人，2016），或者可以作为临床工具来评估或改善心理健康（Weizenbaum，1966；Colby等人，1971）。

<img width="233" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/35b04792-51eb-4d97-9f0e-21b72b89a72f">

（图1：文章第二段的样例句子，以及与之相关的自然问题及其答案。）

除了上述应用外，问题生成系统还可以帮助开发自然语言处理（NLP）数据集，例如SQuAD（Rajpurkar等，2016）和MS MARCO（Nguyen等，2016），以促进这些领域的研究。

过去，问题生成主要通过基于规则的方法来解决（例如，Mitkov 和 Ha (2003)；Rus 等人。 (2010)）。这些方法的成功在很大程度上取决于对声明句到疑问句转换的精心设计的规则的存在，通常基于深入的语言知识。

为了改进纯粹基于规则的系统，Heilman 和 Smith（2010）引入了一种过量生成和排名的方法，该方法使用基于规则的方法从输入句子中生成多个问题，然后使用基于监督学习的排名器对它们进行排序。尽管排名算法有助于产生更多可接受的问题，但它严重依赖于手动编写的特征集，而且生成的问题往往与输入句子中的词完全相同，使它们非常容易回答。

Van der Wende (2008) 指出，学习提出好的问题是自然语言处理研究中一项重要的任务，它本身应该包含对声明句法的转换。具体来说，一个听起来很自然的问题通常会压缩其基于的基础句子（例如，图 1 中的问题 3），使用段落中的同义词（例如，在问题 2 中，“形式”代替“生产”，在问题 3 中，“获得”代替“生产”）或引用来自前文的实体或从句（例如，在问题 2 中使用“光合作用”）。其他时候，世界知识被用来产生一个好的问题（例如，在问题 1 中识别“光合作用”作为一个“生活过程”）。简而言之，构建合理难度的自然问题似乎需要一种抽象方法，该方法可以生成流畅的措辞，这些措辞与它们从中提取的文本不完全匹配。

因此，与所有以前的工作不同，我们在这里提出将问题生成任务建模为一个序列到序列学习问题，该问题直接将文本段落中的句子映射到一个问题。重要的是，我们的方法完全基于数据驱动的，它不需要手动生成规则。

更具体地说，受神经机器翻译（Sutskever等人，2014；Bahdanau等人，2015）、摘要（Rush等人，2015；Iyer等人，2016）以及图像标题生成（Xu等人，2015）的启发，我们使用带有全局注意力机制的条件神经语言模型来处理问题生成。我们研究了该模型的各种变体，包括一种考虑阅读段落而非句子级别信息以及其他变体确定预训练与学习词嵌入的重要性。

在使用三个自动评估指标对SQuAD数据集（Ra-jpurkar等人，2016）进行评估时，我们发现我们的系统显著优于一系列强大的基准线，包括基于信息检索的系统（Robertson和Walker，1994），统计机器翻译方法（Koehn等人，2007）以及Heilman和Smith（2010）的过生成和排名方法。

在下面的几节中，我们讨论了相关工作（第2节），指定了任务定义（第3节）并描述了我们的神经序列学习模型（第4节）。我们在第5节解释实验设置。最后，我们给出了评估结果以及详细的分析。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

**阅读理解**  
阅读理解对机器来说是一项具有挑战性的任务，它需要理解和语言知识世界的知识（Rajpurkar等人，2016）。最近已经发布了 许多新的数据集，在这些数据集中，问题都是以合成的方式生成的。例如，bAbI（Weston等人，2016）是一个完全合成的数据集，包含20个不同的任务。 Hermann等人。（2015）通过在CNN / Daily Mail新闻文章的抽象摘要中用占位符替换实体，发布了一个填空风格的问题语料库。陈等人。（2016）声称CNN / Daily Mail数据集比以前认为的要容易，并且他们的系统几乎达到了天花板性能。理查德森等人。（2013）整理了MCTest，在其中，众包工作者的问题与四个答案选项配对。虽然MCTest包含有挑战性的自然问题，但它对于训练需要大量数据的问题回答模型来说太小了。

最近，Rajpurkar等人。 (2016) 发布了斯坦福问答数据集（SQuAD），该数据集克服了上述的小型化和半合成问题。这些问题是由众包工作者提出的，并且质量相对较高。我们在工作中使用了SQuAD，同样地，我们专注于通过自动方式生成自然的问题以供阅读理解材料。

**问题生成**  
自Rus等人（2010年）的工作以来，问题生成引起了自然语言生成（NLG）社区的关注。

大多数工作都采用基于规则的方法来解决该任务。通常，他们首先将输入句子转换为其句法表示形式，然后使用这些表示形式来生成问题。这种方法依赖于手动构建规则，并且需要对自然语言处理有深入的理解。

Heilman 和 Smith (2010) 提出了一个“生成并排序”的方法：他们的系统首先生成问题，然后对它们进行排序。尽管他们使用了学习到的排名，但该系统的性能仍然严重依赖于手工构建的生成规则。Mostafazadeh 等人。(2016) 引入了一个视觉问答任务，以探索语言与视觉之间的深层联系。Serban 等人。(2016) 提出从逻辑三元组（主语、关系、客体）生成简单事实性问题。他们的任务解决了从结构化表示到自然语言文本的映射，而且他们生成的问题在格式上具有一致性，并且比我们的少了很多分歧。

据我们所知，以往的研究没有使用端到端的方式对阅读理解进QG框架的构建，也没有采用深度序列到序列学习方法来生成问题。


<a id="3.任务定义TaskDeﬁnition"></a>
## 3.任务定义 Task Deﬁnition

在这一部分，我们定义了问题生成任务。给定输入句子x，我们的目标是生成一个与句子信息相关的自然问题y，它可以是一个任意长度的序列：$` \left [ y_1,y_2,...,y_{\left | y \right | } \right ]  `$。假设输入句子的长度为M，则x可以表示为标记序列$` \left [ x_1,x_2,...,x_m  \right ]  `$。QG任务被定义为找到$`\bar{y}`$，使得：
```math
\bar{y} = argmax_y P(y|x)
```
其中P(y|x)是给定输入x的预测问题序列y的条件对数似然。在第4.1节中，我们将详细讨论用于建模P(y|x)的全局注意力机制。

<a id="4.模型Model"></a>
## 4.模型 Model

我们的模型部分灵感来自于人类解决任务的方式。为了提出一个自然的问题，人们通常会关注输入句子的某些部分，并从段落中关联上下文信息。我们使用基于RNN编码器-解码器架构（Bahdanau等人，2015；Cho等人，2014）来建模条件概率，并采用全局注意力机制（Luong等人，2015a）使模型在解码时生成每个词时都专注于输入的某些元素。

在这里，我们研究了两种模型变体：一种只对句子进行编码，另一种则同时对句子和段落级信息进行编码。

<a id="4.1解码"></a>
### 4.1 解码 Decoder

与Sutskever等人(2014年)和Chopra等人(2016年)类似，我们将方程1中的条件因子分解为词级预测的乘积：  

```math
P(y|x) = \prod_{t=1}^{\left | y \right | }P(y_t|x,y < t)
```
其中，每个$`y_t`$的概率基于所有先前生成的单词（即y<t）以及输入句子x来预测。

更具体的说：
```math
P(y_t|x,y < t) = softmax(W_{s}tanh(W_t[h_t;c_t]))
```


其中，w 是时间步长 t 的递归神经网络状态变量，c 是在解码时间步长 t（见第 4.2 节）对 x 进行注意力编码的结果。W_s 和 W_t 是要学习的参数。

在这里
```math
h_t = LSTM_l(y_{t-1},h_{t-1})
```

长短期记忆网络（LSTM）是一种循环神经网络，由 Hochreiter 和 Schmidhuber 于 1997 年提出。它根据之前生成的词yt−1（从词表中获取）以及前一状态ht−1来产生新的状态ht。

解码器隐藏状态的初始化区分了我们的基本模型和包含段落级信息的模型。

对于基本模型，它是由句子编码器（第4.2节）获得的句子表示s初始化的。 对于我们的段落级别模型，是通过将句子表示与段落表示连接起来来初始化的。

<a id="4.2编码"></a>
### 4.2 编码 Encoder

基于注意力的句子编码器被用于我们的两个模型，而段落编码器仅在包含段落级信息的模型中使用。

**Attention-based sentence encoder**  
我们使用双向LSTM来编码句子

```math
\overrightarrow{b_t} = \overrightarrow{LSTM}_2(x_t,\overrightarrow{b_{t-1}})  
\overleftarrow{b_t} = \overleftarrow{LSTM}_2(x_t,\overleftarrow{b_{t+1}})  
```

其中，$` \overrightarrow{b_t}`$ 是在前向 LSTM 中的第 t 步隐藏状态，$` \overleftarrow{b_t}`$ 是在后向 LSTM 中的第 t 步隐藏状态。

为了在解码时间步长t获得基于注意力的编码器输出$`c_t`$，即x的表示，我们首先通过$`b_t = \left | \overrightarrow{b_t};\overleftarrow{b_t}  \right | `$得到上下文相关的标记表示，然后我们在$`b_t(t=1,...|x|) `$上进行加权平均。

```math
c_t = \sum_{i=1,...|x|}a_{i,t}b_i
```

注意力权重由双线性评分函数和softmax归一化计算。
```math
a_{i,t} = \frac{exp(h_t^TW_bb_i)}{\sum_{j}exp(h_t^TW_bb_j)}
```

为了获得句子编码器的输出，用于解码隐藏状态的初始化，我们将正向和反向传递的最后一个隐藏状态连接起来，即$`s = \left | \overrightarrow{b_x};\overleftarrow{b_1}  \right | `$

**段落编码器**

给定句子x，我们希望对包含x的段落进行编码。由于在实践中段落非常长，因此我们设置一个长度阈值L，并在第L个 token 处截断段落。我们将截断后的段落称为“段落”。

我们用另一个双向LSTM来编码段落z，
```math
\overrightarrow{d_t} = \overrightarrow{LSTM}_3(z_t,\overrightarrow{d_{t-1}})  
\overleftarrow{d_t} = \overleftarrow{LSTM}_3(z_t,\overleftarrow{d_{t+1}})
```

我们使用前向和后向传递的最后一个隐藏状态，将 $`\left | \overrightarrow{d_{|z|}};\overleftarrow{d_1}  \right | `$ 连接作为段编码器输出 s'。

<a id="4.3训练和推理"></a>
### 4.3 训练和推理 Training and Inference

给定一个句子-问题对的训练语料库：$`S = \left \{ (x^{(i)},y^{(i)}) \right \}_{i=1}^S `$，我们的模型在训练时的目标是最小化所有参数相对于训练数据的负对数似然，记为θ。

```math
\iota = -\sum_{i=1}^{S}logP(y^{(i)}|x^{(i)};\theta)
      = \sum_{i=1}^{S}\sum_{j=1}^{|y^{(i)}|}logP(y^{(i)}_j|x^{(i)},y_{< j }^{(i)};\theta)
```


训练好模型后，我们使用束搜索进行推断。束搜索的参数由可能路径数 k 决定。

由于输入句子中可能会出现不在目标词典中的罕见词汇，在解码过程中会输出大量 unk 标记。因此有必要在解码后进行替换 unk 的操作。与Luong等人(2015年)不同，我们为我们的任务使用了一个更简单的替换策略。对于时间步 t 处被解码为 unk 的标记，我们将其替换为注意力分数最高的输入句子中的标记，其索引为$`argmax_{i} a_{i,t}`$

<a id="5.实验设置ExperimentalSetup"></a>
## 5.实验设置 Experimental Setup

我们在处理过的SQuAD数据集上对我们的神经问题生成模型进行了实验。在本节中，我们首先描述了任务的语料库。然后给出了我们的神经生成模型的实现细节、与之比较的基准以及它们的实验设置。最后，我们介绍了通过自动度量和人类评分者进行评估的方法。

<a id="5.1数据集"></a>
### 5.1 数据集 Dataset

我们使用Rajpurkar等人（2016）发布的SQuAD数据集，在该数据集中提取句子并将其与问题配对。我们使用句子-问题对来训练我们的模型。该数据集包含536篇文章，其中关于这些文章提出了超过10万个问题。作者聘请了亚马逊机械Turks众包工人根据维基百科文章创建问题。鼓励工人用自己的话提问，不得复制段落中的短语。后来，还聘请其他众包工人提供问题的答案。答案是文章中的一组标记。

由于原始SQuAD的隐藏部分我们无法访问，因此我们将可访问的部分（约90％）视为整个数据集。

我们首先运行Stanford CoreNLP（Manning等人，2014年）进行预处理：分词和句子拆分。 然后我们将整个数据集转换为小写。 根据每个问题的答案，我们可以定位包含答案的句子，并将其用作输入句子。 在某些情况下（在训练集中少于0.17％），答案跨越两个或多个句子，然后我们使用这些句子的连接作为输入“句子”。

<img width="245" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/8c738588-5d49-439f-86cc-2111e4466dd1">

（图2：训练集中句子-问题对的重叠百分比。y轴是非停顿词与问题中总词数的比率（一个百分比）；x轴是在给定的重叠百分比范围内句子-问题对的数量。）

图2显示了句子-问题对中标记重叠百分比的分布。尽管大多数对具有超过50％的重叠率，但大约有6.67％的对没有共同的非停顿词，这主要是因为在注释过程中引入的答案偏移误差所致。因此，我们根据约束条件剪枝训练集：句子-问题对必须至少有一个共同的非停顿词。最后，我们在句子开头添加，并在它们结尾添加。

我们在文章级别随机将数据集分为训练集（80％）、扩展集（10％）和测试集（10％）。我们在测试集中报告结果。

<img width="207" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/6c317cda-6617-45a6-be9c-65881fe0eec0">

（表1：数据集（处理后）统计。 句子平均标记数、问题平均标记数以及句子平均问题数的统计数据来自训练集。 这些平均值接近开发集和测试集上的统计信息。）

表1提供了处理过的数据集的一些统计信息：有大约7万个训练样本，句子约为30个标记，问题平均约10个标记。对于每个句子，可能有多个对应的问题，平均每句有1.4个问题。

<a id="5.2实现细节"></a>
### 5.2 实现细节 Implementation Details

我们在新发布的OpenNMT系统（Klein等人，2017）上实现了我们的模型在Torch7。

对于源语言词汇表V，我们只保留了最频繁的 45k 个标记（包括 、 和占位符）。类似地，对于目标语言词汇表U，我们也保留了最频繁的 28k 个标记。词汇表之外的所有其他标记都用未知符号来代替。我们选择使用 300 维的词嵌入，并在初始化时使用 glove.840B.300d 预训练嵌入（Pennington 等人，2014年）。我们在训练过程中固定了单词表示。

我们设置LSTM隐藏单元大小为600，并在编码器和解码器中都设置了两个LSTM层。优化使用随机梯度下降（SGD）进行，初始学习率为1.0。我们在第8个时期开始将学习率减半。更新的小批量大小设置为64。在垂直LSTM堆栈之间应用了概率为0.3的丢弃法。当其范数超过5时，我们会截断梯度。

我们的所有模型都是在单个 GPU 上训练的。 我们运行训练长达 15 轮，这大约需要 2 小时。 我们选择在开发集上具有最低困惑度的模型。

在解码时，我们使用大小为3的束搜索。 当堆栈中的每个束都生成了EOS token时，解码停止。

我们模型的所有超参数都使用扩展集进行调整。结果在测试集上报告。

<a id="5.3Baseline"></a>
### 5.3 Baseline

为了证明我们系统的有效性，我们将它与几个竞争系统进行了比较。接下来，我们简要介绍了它们的方法以及在我们的问题上运行它们的实验设置。结果如表2所示。

**IR**

IR 是我们的信息检索基准。与 Rush 等人（2015 年）类似，我们实现了 IR 基线来控制从训练集中记住的问题。我们使用两个指标来计算问题和输入句子之间的距离，即 BM-25（Robertson 和 Walker，1994 年）和编辑距离（Levenshtein，1966 年）。根据该度量，系统会检索训练集以查找分数最高的问题。

**MOSES+**

MOSES+是一个广泛使用的短语统计机器翻译系统。在这里，我们将句子视为源语言文本，将问题视为目标语言文本，并执行从句子到问题的翻译。我们在目标侧文本上训练三元语言模型，并使用MERT在开发集上对系统进行调优。性能结果是在测试集上报告的。

**DirectIn**

DirectIn 是一种直观且有意义的基准，其中句子中最长的子句直接用作预测问题。为了将句子分割为子句，我们使用了一组分隔符，即 {"?"、"!"、","、"."、";"}。

**H&S**
H&S 是一个基于规则的生成式排名系统，如第 2 节所述。运行该系统时，我们将参数 just-wh 设置为 true（以限制系统的输出仅限 wh-问题），并将最大长度设置为训练集中最长的句子。我们还将 downweight-pro 设置为 true，以降低未解决代词的问题权重，使其出现在排名列表的末尾。为了与我们的系统进行比较，我们选择了排名列表中的顶部问题。

**Seq2seq**。

Seq2seq（Sutskever等人，2014年）是一种基本的编码器-解码器序列学习系统，用于机器翻译。我们在TensorFlow中实现了他们的模型。在训练或翻译之前，输入序列被反转。我们使用开发集来调整超参数。我们在开发集上选择困惑度最低的模型。

<a id="5.4自动评估"></a>
### 5.4 自动评估 Automatic Evaluation

我们使用陈等人。 (2015)，最初用于对图像标题进行评分的评估包。该软件包包括 BLEU 1、BLEU 2、BLEU 3、BLEU 4（Papineni 等人，2002）、METEOR（Denkowski 和 Lavie，2014）和 ROUGE-L（Lin，2004）的评估脚本。 BLEU 在参考句子集合上测量平均 n-gram 精度，并对过短的句子进行惩罚。 BLEU-n 是使用 n-gram 计数共现的 BLEU 分数。 METEOR 是一种基于召回的度量，它通过考虑同义词、词干提取和释义来计算生成物与参考之间的相似性。 ROUGE 通常用于评估摘要中包含金标准句子的 n-gram 召回率。报告了 ROUGE-L（基于最长公共子序列的度量）结果。

<a id="5.5人类评估"></a>
### 5.5 人类评估 Human Evaluation

我们还进行人类评估研究，以衡量我们的系统和HS 系统生成的问题的质量。 我们考虑两种模式：自然度，这表明语法的正确性和流利性； 困难程度，衡量句子问题句法差异以及回答问题所需的推理能力。 我们随机抽取了100个句子-问题对。 我们要求四位专业英语人士根据上述模式在1到5分（5分最好）的范围内对这些对 进行评分。 然后请人工评标员按照整体质量给问题排名，允许打平。

<a id="6实验结果与分析"></a>
## 6 实验结果与分析 Results and Analysis

<img width="503" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/29838749-e7f7-4340-93af-02451e06ef42">

（表2：不同系统通过BLEU 1-4、METEOR和ROUGE-L进行的自动评估结果。有关基线系统的详细说明，请参见第5.3节。每个列中性能最佳的系统用粗体突出显示。我们只使用预训练词嵌入来编码句子的系统在所有指标上都取得了最佳性能。）

表2显示了我们的模型和基准线的自动度量评估结果。仅编码句子级信息的模型实现了在所有指标上都表现最好。我们注意到，IR 的性能较差，这表明仅记住训练集不足以完成这项任务。DirectIn 基线在 BLEU 和 METEOR 上表现得相当不错，考虑到句子与问题之间的重叠统计信息（图 2），这是合理的。H&S 系统的性能与 DirectIn 相当，因为它基本上没有进行句法变换而只是改写，而且重叠率也很高。

通过观察我们的三个模型的性能，可以清楚地看到添加预训练嵌入通常是有帮助的。虽然编码段落会导致性能下降一点，但这是有道理的，因为除了有用的信息之外，段落还包含很多噪声。

<img width="251" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/f6c839f1-97d6-4ba2-9372-4e2ebdc9a192">  
（表3：对于问题生成的人类评估结果。自然度和难度的评分范围为1到5分（5分为最好）。与H&S方法相比，我们的方法的双尾t检验结果如下所示（统计显着性用*表示（p < 0.005），**表示（p < 0.001））。）

表3显示了人类评估的结果。 我们看到我们的系统在所有方面都优于H&S。 在38.4％的评价中，我们的系统排名第一，平均排名为1.94。 总体排名实现了Krippendorff's Alpha 的一致性系数为0.236。 结果表明，我们的模型可以生成比HS系统更好的问题质量。 这里一个有趣的现象是，人类评分者给我们的系统输出打了更高的分数比人类的问题。 一个可能的解释是，我们的系统是针对输入句子的所有句子-问题对进行训练的，而我们随机选择一个句子中的一个问题作为人工生成的问题进行评级。 因此，我们的系统的预测往往更加多样化。

![image](https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/35b821ce-d9c8-4e99-b5e2-6436c7589dad)  
(图3：由人类（参考问题）、我们的系统和H＆S系统生成的样本输出问题。)

在我们的定性分析中，我们检查了样本输出以及输入和输出之间的对齐可视化。在图3中，我们展示了由H＆S系统和我们最好的模型生成的一些样例问题。我们可以看到我们的结果与H＆S的结果之间存在很大的差距。例如，在第一个示例中，重点应该放在“最大”的部分上。我们的模型成功地捕获了这些信息，而H＆S只对输入进行了某些句法转换而不进行改写。然而，我们的系统的输出并不总是“完美”，例如，在第6个配对中，我们的系统提出关于鸟类为什么仍然生长的问题，但最相关的问题是为什么许多物种仍在生长。但从另一个角度来看，我们的问题更具挑战性（读者需要理解鸟类是一种物种），这支持了我们在人类评估中列出的系统性能（见表3）。进一步研究如何解释为什么某些不相关的词被生成在问题是有趣的。图4显示了当生成每个问题中的每个标记时，输入句子的注意力权重(αi,t)。我们看到输出中的关键词（如“introduced”、“tele-text”等）与输入句子中的关键词对齐得很好。

最后，我们进行了数据集分析和细粒度系统性能分析。我们在开发集中随机采样了346个句子-问题对，并给每个对打上类别标签。这四个类别是根据询问问题所需的信息量来确定的。具体来说，“w/sentence”意味着只需要句子本身就可以提出问题；“w/paragraph”意味着需要段落中的其他信息才能提出问题；“w/article”类似于“w/paragraph”；而“not askable”则表示需要世界知识才能提出问题，或者由于注释错误导致句子和问题之间存在不匹配。

<img width="490" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/9266b50b-d8e8-458b-92fc-4f859444c09d">  
（表4：处理数据集的问题类别估计和系统按类别的性能比较。该估计基于我们对开发集中的 346 对进行的分析。类别由生成问题所需的信息决定。粗体数字表示在给定度量方面表现最佳的方法。在这里，我们省略了“带有文章”的类别（2个样本，0.58%）和“不可回答”的类别（33个样本，9.54%）的性能结果。）

表4显示了系统在各个类别上的性能。 我们的模型通过编码段落信息，在“带有段落”的问题类中取得了最佳效果。 这验证了我们的段落级模型在处理句子以外的信息的问题上的有效性。

<a id="7结论与未来工作"></a>
## 7 结论与未来工作 Conclusion and Future Work

我们提出了一种完全基于数据驱动的神经网络方法来实现阅读理解的自动问题生成。我们在任务中使用了基于注意力机制的神经网络，并研究了编码句子级别信息和段落级别信息的效果。我们的最佳模型在自动评估和人工评估方面均取得了最先进的性能。

在这里，我们指出了几个有趣的未来研究方向。目前，我们的段落级模型并不适用于所有类别的问题。我们希望探索如何更好地利用段落级别的信息来提高QG系统对所有类别问题的表现。此外，考虑在我们的模型中纳入其他语言生成任务（例如对话生成中的复制机制）的机制也将是有趣的，以进一步提高生成问题的质量。
