# All You May Need for VQA are Image Captions

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/2205.01883.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
视觉问答（VQA）得益于越来越复杂的模型，但在数据生成方面并没有享受到同等程度的参与。在本文中，我们提出了一种方法，通过利用现有图像标题注释的丰富性，并结合用于文本问题生成的神经模型来自动产生大量的 VQA 示例。我们证明了结果数据的质量很高。使用我们的数据训练的 VQA 模型在零样本准确率上比最先进的模型提高了两位数，并且达到了相同模型在人类标记的 VQA 数据上所缺乏的鲁棒性水平。

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
视觉问答（VQA）是一项复杂的多模态任务，为了成功建模和评估，需要大量的注释数据，而这些数据不能自然地由现有的业务流程产生，就像翻译对齐注释（Guo等人，2018年）或图像alt文本注释（Sharma等人，2018年）那样。  

目前，开发有助于下游应用（如盲人、医学和教育领域）的鲁棒视觉问答系统的瓶颈似乎是缺乏数百万级的大规模图像-问题-答案训练三元组。对这些三元组进行手动注释成本高、耗时长，并且容易受到各种人类偏见的影响，而这些人类偏见很难解释清楚（Yuan, 2021）。此外，针对此类人工标注进行训练的视觉问答系统的脆弱性已被充分理解并记录在案（Agrawal et al., 2018；Kafla 和 Kanan, 2017）。

为了克服数据限制，我们转向了一个创造视觉问答示例的潜在来源：图像英语标题对（陈等，2015年；夏尔马等，2018年）。大规模图像标题数据集存在数百万（昌平诺等，2021）、数亿（拉德福德等，2021）甚至数十亿（贾等，2021）个例子。标题主要以陈述句的形式出现，例如“两只熊正在冰上躺着”。然而，将陈述性标题转换为视觉问答问题/答案对的任务仍然很大程度上未被探索。它需要根据标题文本自动诱导适合于视觉问答任务的答案候选项及其相应的问题（图1）。我们注意到将陈述形式转换为疑问形式加上答案似乎至关重要，因为有证据表明在声明语言数据(人类标注的陈述性数据)上训练的视觉-语言模型不能成功地适应或转移“out-of-the-box”到视觉问答（王等，2021）。

<img width="218" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/614e3959-e8da-4259-b97b-39a031e2e12f">

（图 1：给定一个英文标题（以及相应的图像），我们的 VQ2A 方法可以生成高质量的问题-答案对。这些图像-问题-答案三元组数据可以自动产生，并且以百万计的数据量使用，用于有效地训练视觉问答系统。）

在这篇论文中，我们探讨了使用神经模型进行文本问题生成和问答来自动创建数百万个VQA训练数据的方法。我们将此方法称为VQ2A，即带有答案验证的视觉问题生成。我们证明了在这种数据上训练的VQA模型，在完全没有接触过人类标注的VQA数据的情况下，表现出很高的零样本性能。我们的最佳模型在VQA2.0上的准确率为61.1％，在GQA上的准确率为52.1％，比以前的最佳零样本结果高出约15-17个百分点，并接近完全监督性能。此外，将我们生成的例子作为测试集，我们提供了进一步的证据，说明使用人工标注示例构建的VQA系统易受攻击，以及使用自动生成的VQ2A数据构建的VQA系统的鲁棒性。


<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

<a id="2.1自然语言处理中的问题生成"></a>
### 2.1 自然语言处理中的问题生成 Question generation in NLP

问题生成（QG）是自然语言处理中一个活跃的研究课题。它被探索为一项独立的任务（Heilman 和 Smith，2009；Nema 等人，2019）、用于语言模型预训练的任务（Narayan 等人，2020），以及作为其他文本任务解决方案的组件，如问答（Al-berti 等人，2019；Puri 等人，2020）、信息检索（Mass 等人，2020；Gaur 等人，2021）和生成评估（Durmus 等人，2020；Wang 等人，2020；Honovich 等人，2021）。QG 的两个主要方向是基于模板的（Heil-man 和 Smith，2009；Lyu 等人，2021；Dhole 和 Manning，2020）和神经网络的，后者取得了最先进的结果（Alberti 等人，2019；Narayan 等人，2020）。

<a id="2.2计算机视觉中的问题生成"></a>
### 2.2 计算机视觉中的问题生成 Question generation in computer vision

计算机视觉中的问题生成旨在为给定的图像（或视频）生成关于图像的视觉问题，这些问题可以是为了不知道答案而生成的问题（Mostafazadeh 等人，2016；Zhang等人，2017；Yang等人，2018；Uehara等人，2018；Krishna等人，2019），例如由人类来回答，或者为了帮助提高视觉问答任务（Kafla 等人，2017；Li等人，2018；Shah等人，2019；Xu等人，2021；Kil等人，2021；Akula等人，2021），例如额外评估和作为数据增强手段。此类 QG 模型通常基于 VQA 三元组训练数据，其语言复杂性往往有限，或者需要收集视觉 QG 数据（Mostafazadeh 等人，2016）。我们采取了不同的方法，利用在文本 QA 数据集上训练好的模型。

多个工作利用图像字幕或视频转录作为训练源（Ren等人，2015年a；Banerjee等人，2021年；Yang等人，2021年a；Lee等人，2021年）。这种方法从文本中自动生成问题-答案对，忽略视觉来源，并将其与相关的图像/视频组合以产生图像-问题-答案三元组。Banerjee等人（2021）提出了WeaQA，在其中他们使用改进的基于模板的方法在CO-COQA（Ren等人，2015年a）以及QA-SRL方法中从MSCOCO图像字幕（Chen等人，2015）生成问题，增强了解释性语言变化的改写和回译。Lee等人（2021）类似地从MSCOCO Caption衍生的问题-答案对训练VQA模型，但仅使用名词短语作为候选答案，专注于用它来验证生成的字幕，而不是VQA任务本身。Yang等人（2021年a）从教学视频ASR转录中生成问题-答案对，然后与相关视频配对。

在这项工作中，我们遵循这一方向，调查了在图像领域为视觉问答任务生成数据所需的覆盖范围。我们证明了基于神经网络的文本问题生成方法与标题相比具有更高的有效性。此外，与以前的工作不同，我们还探索了自动收集的域外图像文本数据源。

<a id="2.3图像问答中的迁移学习"></a>
### 2.3 图像问答中的迁移学习 Transfer learning for and in VQA

现有工作还探索了图像字幕生成任务与无需提问的视觉问答(VQA)任务之间的关系（第 2.2 节）。Fisch 等人 (2020) 预测视觉问题来执行图像字幕生成，即使用 VQA 数据作为额外监督并在推理后进行评估。Wu 等人 (2019) 生成与问题相关的图像字幕以帮助 VQA。Yang 等人 (2021b) 利用生成的标签和一些 VQA 示例提示 GPT-3 (Brown 等人, 2020) 回答基于知识的视觉问题。

证据表明，在大规模情况下进行的图像文本预训练有助于包括视觉问答在内的视觉语言任务（Lu等人，2019；Li等人，2019；Chen等人，2020；Tan和Bansal，2019；Su等人，2020；Lu等人，2020；Zhou等人，2020；Li等人，2020；Zhang等人，2021；Cho等人，2021；Wang等人，2021；Yuan等人，2021）。然而，这些方法在没有对下游 VQA 数据进行微调的情况下效果不佳（Wang等，2021）。此外，从预先训练好的图像文本模型中进行提示式的推断（Liu等，2021）并用于解决视觉问答仍然是一个开放的研究问题。相比之下，我们的方法直接处理训练数据，并显式地将其转换成可询问的形式。

我们的重点是在WeaQA（Banerjee等人，2021）中的零样本迁移设置，在训练期间没有可用的人工创建的VQA三元组。请注意，这里的“零样本”与（Teney和Hengel，2016）中使用的术语不同，在那里模型仍然可以访问人工创建的VQA三元组，但在测试时用未见过的问题进行评估。类似地，Chao等人。（2018年b）探索跨数据集的视觉问答，但他们只关注人类注释的数据以及转移方法。

<a id="3.TextualQuestionGenerationforVQA"></a>
## 3.Textual Question Generation for VQA

我们研究了是否可以利用现有的图像文本资源自动生成视觉问答（VQA）注释，从而减轻或完全取代手动数据注释的需求。本文仅关注英语。为此，我们在第2.2节中遵循并改进了一些最近的自动问答生成方向。

我们从给定的图像标题对数据集D开始，`$ D = {img_i,cap_i}_{i=1}^N $`。 我们做出的重要假设是，在绝大多数情况下，图中包含的与标题传达的信息是一致的，即标题中没有包含大量的外部世界或个人知识（例如，“我的朋友在我生日派对上）。

对于每一对`${img_i, cap_i}$`, 首先从`$cap$`中自动推导出一组候选答案$`{a_i,j}_{j=1}^M$`。 对于每个这样的候选答案，由神经模型`$q_{i,j}=QG(a_{i,j}, cap_i)$`生成一个问题。 每个生成的问题回答对都经过验证步骤，并且如果被验证，则与相应的图像`$img_i$`结合，以诱导一个VQA示例三元组`${img_i, q_{i,j}, a_{i,j}}$`。

我们把这种方法称为VQ2A（带答案验证的视觉问题生成）。图2提供了我们的方法概述。接下来，我们将详细介绍VQ2A中的步骤。

![image](https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/c2c092a7-2ed2-4976-91b5-329797caee90)  
(图2：带有问题回答验证的视觉问题生成（VQ2A）有三个主要阶段：候选答案提取（第3.1节），问题生成（第3.2节），以及问题回答过滤（问答+答案验证，第3.3节）。)


<a id="3.1候选答案提取"></a>
### 3.1 候选答案提取 Candidate Answer Extraction
我们在李等人（2021）中了解到的唯一关于神经元从描述中生成问题的工作，重点关注的是候选答案中的名词短语。然而，这些并不足以涵盖典型的视觉问答基准测试，例如VQA2.0（参见第5.1节），其中包括布尔值、属性和动词等类型的答案，例如“有没有……”，“什么颜色……”，“狗在做什么”。我们提出了一种方法，涵盖了所有这些答案类型。

要从给定的标题中提取候选答案，我们使用spaCy1对其进行解析，然后根据词性标注（Part-of-Speech POS）和依存关系树（dependency parse tree）的注释来提取候选答案：

**名词短语**：我们提取所有由 spaCy 标注的名词短语，包括命名实体。

**词性标记序列**：我们提取以开放类词性（名词、动词、形容词和副词）开头、以开放类词性或从属连词结尾、中间不包含其他词性的词序列，除了闭合类词性（用于限定词、介词和连词）。

**解析树片断**：我们考虑所有包含至少一个开放类词性标记的子树，但总体上不超过三个单词。我们只提取最大片断，即不提取完全包含在其他提取的子树中的子树。

**布尔值**：布尔问题在视觉问答基准测试中很常见（Goyal等人，2017）。然而，“是”和“否”并不出现在标题中，因此不能通过从标题中提取文本片断来提取候选者。为此，我们还添加了“是”和“否”作为候选答案，并为每个候选者生成一个问题（见第3.2节）。

**多少个？ 0**： 标题通常不包含“零”对象计数的提及。 因此，在标题中标记跨度不会生成答案为“0”的问题。 因此，我们从另一个标题中随机采样一个生成的“有多少？”的问题（带有非零答案），并将答案更改为“零”，添加到目标标题的候选集中。 这种方法可能会产生噪音，因为针对所选问题的答案对于目标图像也可能是非零的。 通过手动检查 200 个此类问题，我们发现这种情况很少发生——大约占 4.5%。

我们的提取方法涵盖了各种答案候选项，例如复合名词、名词短语、命名实体、布尔型答案、基数和序数、动词及其复合形式、多字形容词和介词短语等。表1提供了各种类型的候选项示例以及用于提取它们的方法机制。

<img width="243" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/b5bbc8d5-b746-4cb2-a5b2-e4a500be95ff">

（表1：从句子“两只熊躺在冰上”中提取的候选答案以及用于提取它们的机制。）

<a id="3.2问题生成"></a>
### 3.2 问题生成 Question Generation

我们的问题生成模型，q = QG（a，cap），输入一个标题、cap 和其中的候选答案span，a，并生成一个问题q，其给定输入标题的答案是输入答案span。重要的是，答案a不需要在标题中以字面形式出现，从而可以为布尔型和零计数等类型的问题生成答案（参见第 3.1 节）。

由于神经文本生成技术的进步，包括像T5（Raffel等人，2020年）这样的模型，我们选择使用神经生成模型作为QG。具体来说，我们使用T5-XXL模型并在SQuAD1.1（Rajpurkar等人，2016年）上对其进行微调以进行问题生成。对于每个标题-答案输入，我们选取最高得分的生成问题。我们注意到我们的QG模型是在一个问答数据集上训练的，而不是针对标题输入优化的，因此并不是专门为标题输入而设计的。从数百个生成问题的人工检查来看，我们的QG模型对标题输入表现良好；详见表格2和第3.5节。

<img width="605" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/3ab73a9a-86eb-4c7a-8991-96234477e237">  
表2：来自句子“两只熊在冰上躺下”的问题/答案对以及过滤决策。对于候选答案‘零’，没有进行验证。

<a id="3.3问答过滤"></a>
### 3.3 问答过滤 Question-Answer Filtering

生成模型可能会产生幻觉，即产生物与输入源不一致的内容（Alberti等人，2019；Honovich等人，2021）。为了缓解这种情况，我们遵循（Alberti等人，2019）并使用循环一致性通过问答模型回答caption文本中生成的问题。如果答案与作为输入提供给问题生成模型的答案候选者不匹配，则丢弃生成的问题。

我们使用基于标记的F1分数（Wang等人，2020）来确定候选答案与QA模型的答案是否匹配；如果分数高于阈值（手动设置为0.54，如表2所示），则该对就是匹配。对于问答，我们使用T5-XXL模型，并在SQuAD2.0（Rajpurkar等人，2018）和Natural Questions（Kwiatkowski等人，2019）上对其进行微调。

<a id="3.4源自图片/标题的数据来源"></a>
### 3.4 源自图片/标题的数据来源 Sources of Image/Caption Data

为了了解VQ2A的潜在性能，我们从两个图像说明来源生成了VQA三元组：MSCOCO说明（COCO-CAP）（陈等，2015年）和Conceptual Caption（CC3M）（Sharma等，2018年）。COCO-CAP说明包含来自COCO数据集（Lin等，2014年）的123,287张图片，每张图片都有5个由仔细指导的评分员创建的黄金说明。CC3M包含从网络自动收集的3.32M张图片，每张图片都附带一个关联的alt-text，我们将其视为银色说明。

这些数据集是非常不同的。CC3M 的图像数量和领域都更大，其标题看起来更可信，可以捕捉到更多对象/属性/动作注释的集合。另一方面，COCO-CAP 的标题更干净，并且更能充分代表图像的内容（参见第 3.5 节）。因此，使用 COCO-CAP 可以显示在“更干净”的零样本设置中使用 VQ2A 训练 VQA 模型的潜力，其中标题由人类精心挑选。使用 CC3M 表明可以在嘈杂的网络图像-alt-text 对上进行训练的潜力，其中可以扩展到数十亿个示例。

为了量化我们方法的影响，我们在 VQA2.0 (Goyal等人，2017)，GQA (Hudson和Manning，2019) 和 OKVQA (Marino等人，2019) 的基准测试上关注分类任务。因此，我们将我们的分类器限制为来自这些基准测试的统一答案词汇表中的前5,971个答案（附录B.1中有详细说明）。为此，我们删除了答案不在目标答案词汇表中的三元组，并将研究所有生成的三元组留待未来工作。然后，我们将数据集分为训练/验证集。具体来说，由于VQA2.0中的图像来自COCO，我们根据Jiang等人提出的标准VQA2.0训练/验证拆分对COCO数据集进行了拆分。

表3显示了生成的数据集的大小，分别称为VQ2A-COCO和VQ2A-CC3M，以及我们在实验中使用的视觉问答数据集。

<img width="289" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/6a1cab30-dfc3-4a42-898d-81f534522996">  
表3：我们生成的VQ2A数据（前两行）和我们在实验中使用的VQA数据集的大小。

<a id="3.5质量分析"></a>
### 3.5 质量分析 Quality Analysis

为了衡量生成的数据集的质量，我们在 VQ2A-COCO 和 VQ2A-CC3M 数据集中分别随机抽取了 800 个样本。这些样本被分发给四位作者进行评估，他们根据图像来判断问题的答案是否合理。对于每个数据集，所有评价者都对 50 个样本进行了评分，导致 VQ2A-COCO 的自由边 Kappa 得分为 0.71，VQ2A-CC3M 的得分为 0.59，这表明评价者之间具有很高的相互一致同意度。测量的正确三元组百分比为 VQ2A-COCO 的 87.3%，VQ2A-CC3M 的 66.0%。这显示了高质量的 COCO-CAP 标题与嘈杂的基于网络的 CC3M 标题之间的差异。

图 3 显示了在 VQ2A 数据集中生成的问题的多样性。可以看到，由 VQ2A 生成的大量用于共享的 VQA2.0/COCO 图像在 VQA2.0 中并未出现。附加分析和示例请参见附录 A。

<img width="818" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/4a855a5d-fdaf-451e-a40e-8afaac69cdd5">  
（图3：来自VQ2A-COCO（上）和VQ2A-CC3M（下）的示例。带有绿色背景的问题也出现在VQA2.0中。）

<a id="4.VisualQuestionAnswering(VQA)"></a>
## 4. Visual Question Answering(VQA)

为了评估我们自动生成VQA注释的有效性，我们在各种已建立的VQA基准测试上测量了生成数据的影响，并对其进行了外部评估。首先描述模型，然后介绍实验设置和结果。

<a id="4.1VQA的形式化与模型"></a>
### 4.1 VQA 的形式化与模型 VQA Formulation and Model

根据文献，我们把视觉问答（VQA）看作是一种分类任务，即基于词汇的 VQA。具体来说，我们将目标答案视为标签，一个标签可以包含多个词元（例如，“圣诞树”、“黑白”、“打网球”）。我们定义我们的标签集基于下游 VQA 数据集训练集中的前几名答案，这使得我们可以公平地与 Antol 等人(2015)之后的大多数 VQA 文献进行比较。

由于我们的工作探索了自动生成的训练数据对 VQA 模型的影响，因此我们在所有实验条件下都固定了 VQA 模型架构。我们的模型将输入图像和问题融合在一起（见图 4）。在图像侧，我们从 ImageNet 上预训练的 ResNet-152 提取全局图像特征，并从在 Visual Genome 上预训练的 Faster R-CNN 中提取 16 个感兴趣区域的图像特征，遵循自下而上的特征表示方法。在问题侧，我们使用预先训练好的 T5 基础检查点的编码器。给定图像特征和问题编码器的输出标记嵌入，一个多模态中间表示的 Transformer 将它们融合并分类到预定义的答案空间。我们随机初始化融合编码器和文本编码器，并使用标准交叉熵损失端到端地训练它们。ResNet 和 Faster R-CNN 的参数在训练过程中被冻结。更多细节请参见附录 B.2。

<a id="4.2实验设置"></a>
### 4.2 实验设置 Experimental Setup

我们考虑了三个视觉问答基准数据集：VQA2.0（Goyal 等，2017），GQA（Hudson 和 Manning，2019）和OKVQA（Marino 等，2019）。 这些数据集具有各自的特性，因此测试不同的视觉问答模型的能力。例如，GQA 强调推理，而 OKVQA 则强调外部知识，而 VQA2.0 更加通用；VQA2.0 和 GQA 的规模比 OKVQA 大一个数量级；GQA 是使用问题引擎生成的，而 VQA2.0 和 OKVQA 是人工注释的。 对于在 VQA2.0 上进行训练和评估，我们使用标准的训练/开发拆分*train2014 和 mini-val2014（Jiang 等，2018）。对于 GQA，我们使用平衡的 v1.2 并将训练和验证拆分组合在一起进行训练，并根据官方指南3和(Tan 和 Bansal，2019) 使用 testdev 拆分进行评估。对于 OKVQA，我们使用训练/验证拆分进行训练/评估。表 3 总结了不同数据集的大小。

**评价设置和基线**  
我们实验的主要目标是为了探索VQ2A数据在迁移学习中的应用，作为训练或评估数据。

本文的主要关注点在于零样本评估。尽管如此，微调仍将为使用我们生成的数据进行预训练提供额外的见解。因此，我们在 (Banerjee et al., 2021) 的基础上，对生成的 VQ2A 数据上的视觉问答模型进行微调，并在两个设置中对其进行评估：(i) 零样本评估，即在 VQA2.0、GQA 或 OKVQA 的开发数据集上直接评估我们的模型；以及(ii) 全监督微调评估，在此设置下，我们在 VQ2A、GQA 或 OKVQA 的训练数据集上进一步微调模型，然后再对其进行评估。当在 VQ2A 数据上训练时，我们探索仅在 VQ2A-COCO 上训练、仅在 VQ2A-CC3M 上训练，以及两阶段训练 VQ2A-CC3M 然后在 VQ2A-COCO（VQ2A CC3M→COCO）上训练的方法。

我们的基线，不使用 VQ2A 数据，包括：(i) 我们在模板生成数据 COCOQA4（Ren 等人，2015 年）上训练的视觉问答模型；(ii) 零样本 WeaQA（Banerjee 等人，2021 年）及其完全监督变体；以及 (iii) 我们在每个目标基准的训练数据上进行监督训练的视觉问答模型。

**度量**  
为了与之前的工作保持一致，我们在 VQA2.0 和 OKVQA 上测量标准的 VQA 准确率。它是对五个答案中的九个子集的平均分数，其中每个分数都是：最小(出现次数 # ， 1)。在 GQA 中，我们通过单个正确答案来衡量 Top-1 准确率。

<a id="5.结论Results"></a>
## 5.结论 Results

在本节中，我们报告了多组实验结果，这些结果既揭示了 VQ2A 数据上训练的视觉问答模型的准确性和鲁棒性，附录C 中有额外的结果、分析和消除研究。

<a id="5.1零样本学习设置"></a>
### 5.1 零样本学习设置 Zero-Shot Setting

在零样本学习设置下，我们的 VQA 模型在各种基准测试上的表现总结在表 4 中。我们的主要结果是，VQ2A 模型在零样本转移学习设置方面实现了新的最佳性能。性能提升幅度很大：根据我们的了解，以前的最佳零样本准确性结果是在 VQA2.0 上达到 46.8%，在 GQA 上达到 33.7%，由 WeaQA（Banerjee 等人，2021）通过从 COCO Caption 中诱导其训练 VQA 数据实现。我们的 VQ2A-COCO 模型在 VQA2.0 上达到 60.0%，在 GQA 上达到 51.3%，分别绝对提高了 +13.2% 和 +17.6%。即使在零样本设置下的更高精度 - VQA2.0 上的 61.1% 和 GQA 上的 52.1% - 可以通过 VQ2A CC3M-→COCO 模型实现（首先在衍生自 CC3M 的数据上进行训练，然后在衍生自 COCO 的数据上进行微调），从而建立新的最佳结果。

<img width="405" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/9d71fe20-c879-4d4e-a383-216fcebdcc93">  
（表4：VQ2A作为训练数据。零样本和完全监督设置下的准确率。所有结果都使用我们的架构，除了WeaQA ZSL和WeaQA FSL，它们分别是(Banerjee等人，2021年)中的零样本（ZSL+Patches+Encoder）和完全监督（FSL+Patches+Encoder）模型。+D表示重新获取的带有数字的原始CC3M标题文本。）

在完全监督的情况下，对相同模型架构进行训练，在手动构建的VQA2.0和GQA训练集上分别达到68.8%和61.8%的准确率。因此，我们的结果显著缩小了自动产生和手工构建训练源之间的性能差距，表明VQ2A方法可以减少对人类标记的VQA训练示例的需求。

尽管存在这些差异，COCO 基于和 CC3M 基于 VQ2A 模型之间的差距并不大，在 VQA2.0 上为 60.0％ 对 56.5％，在 GQA 上为 51.3％ 对 49.9％。这一结果加强了我们之前的观察，即似乎不重要的是起始说明文字是否是手动注释的；看来像 CC3M 提供的“银”注释这样的注释在零样本 VQA 性能方面具有竞争力。

为了涵盖VQA基准测试中出现的各种答案类型，需要对各种问题/答案类型进行彻底的提取（第3节）。例如，QACE模型（Lee等人，2021）只关注名词短语作为答案类型。通过分析VQA2开发集，我们发现其答案只有32％是名词。因此，在仅限于这种答案类型的情况下，VQA准确率从VQ2A-COCO的60％降低到10.5％。另一个例子是，我们的模型训练了COCOQA（Ren等人，2015年），该模型专注于几种答案类型和单个单词的答案，几乎无法超过我们的COCO，仅使用名词的基线精度。出于类似的原因，即使发布的注释已删除数字和数字，我们也希望能够从CC3M数据生成“多少”的问题。为了解决这个问题，我们从CC3M URL 中恢复了原始说明文字，生成了“多少”的问题，并训练了一个额外的VQ2A-CC3M + D模型。表4中的结果显示，与纯VQ2A-CC3M相比，这带来了小幅但一致的改进，进一步缩小了使用经过策划的“黄金”标题和更嘈杂的“白银”标题之间的差距VQ2A模型。

为了获得更深入的见解，我们在表5中提供了VQA2.0问题类型的每个问题的VQA准确率。布尔型问题是最容易的，所有模型都表现良好。更具挑战性的问题类型是“有多少？”和“是什么”。一个原因可能是各种答案的有效性，例如计数的答案“几个”。 “几点了？”是最难的，可能是因为字幕中缺少这样的信息。

<img width="406" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/9a7c1e24-ba82-47c2-b55e-7e0975ab61a3">
（表5：在VQA2.0上对最常见问题类型的聚合平均准确率。）

最后，我们在更具挑战性的OKVQA基准上提供了零样本结果。在这种情况下，监督模型达到了22.1％的准确性，而VQ2A模型在零样本设置下实现了接近该水平的成绩 - 使用COCO达到18.0％，使用CC3M达到19.1％，而它们的组合达到19.7％，比监督级别低2.4％。这个结果也支持了使用VQ2A方法创建训练数据可以很好地替代小规模监督训练数据的结论。

<a id="5.2全监督设置"></a>
### 5.2 全监督设置 Fully-Supervised Setting

我们希望评估的 VQ2A 方法的另一个方面是，它是否产生了与人类注释数据相似的训练数据，还是与之互补。为此，我们在使用 VQ2A 数据训练模型后，在全监督方式下微调该模型，同时使用人类注释的训练数据。

表 4 中“全监督”部分的结果告诉我们两个故事。对于 VQA2.0 和 GQA，相对于直接在有标签数据上进行监督训练的模型（未使用 VQ2A 标记），微调后的模型在每个基准测试中都有小但一致的提升。这表明，至少对于这两个基准测试，人类注释的数据和 VQ2A 数据之间的信号性质高度重叠。


OKVQA的结果显示出不同的趋势。在这里，首先使用 VQ2A 进行训练，与没有使用 VQ2A 的监督训练相比，性能提高了+17.2%(22.1% → 39.3%)。 OKVQA 训练集的规模很小（表 3），这肯定有助于这种效果，但它也指出了另一个方面：只有通过不受人类注释过程瓶颈的方式，才能使包含世界知识的问题-答案对在大规模上提供给模型。

<a id="5.3现有视觉问答训练集的稳健性"></a>
### 5.3 现有视觉问答训练集的稳健性 Robustness of Existing VQA Training Sets

到目前为止，我们已经评估了在 VQ2A 数据上训练的模型的能力。作为一个补充研究，我们使用每个 VQ2A 数据集的开发部分中的 500 个经过验证的随机样本（见第 3.5 节）来评估各种训练设置下的视觉问答 (VQA) 的鲁棒性。我们在 VQ2A 数据集上使用 VQA 准确度指标（10 个目标答案，请参阅第 3.4 节），并在 COCOQA 上使用 Top-1 准确率（一个目标答案）。

<img width="291" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/5d1f9f94-734c-46ea-8270-f9d2c952ac6d">
（表6：手动验证的VQ2A数据用于稳健性评估。训练在“行”上，测试在“列”上；对角线（灰色）数字表示监督设置，非对角线数字表示零样本跨数据集设置。最佳零样本效果加粗显示。）

表6显示了结果。 全监督模型（对角线，相似的训练和测试分布）在域内精度达到约70%，VQ2A CC3M 达到略高的76.4%精度。 然而，在域外（非对角线）测试时，每个模型的表现都会出现不同程度的下降。 首先，基于模板生成的COCOQA 模型根本不具有泛化能力。 其次，VQA2.0 模型在COCO (44.4%) 和COCOQA (35.9%) 上出现了显著的准确率下降，这两个数据集与VQA2.0 的图像领域相似。 这一结果为在VQA2.0 数据集上取得的进步并不能完全反映在VQA任务上的进步提供了另一个证据（Chao et al.，2018a；Bras et al.，2020）。

相比之下，VQ2A-COCO 和 VQ2A-CC3M 的表现更加稳健，性能下降幅度较小。例如，在 COCOQA 上，尽管是在域外图像上进行测试，但 VQ2A-CC3M 仍然比 VQA2.0（42.1% VS 35.9%）表现得更好。这表明，VQ2A 训练数据具有更高的问题变异性、更好的答案覆盖率，并且与手动整理的 VQA2.0 训练数据相比，表现出较少的偏差，至少足以解决这些不同的基准。

<a id="6考虑事项和限制"></a>
### 6 考虑事项和限制 Considerations and Limitations

自动数据生成可能会出现错误输出。在VQ2A中，这些错误包括生成模型的幻觉、错误的负采样以及糟糕的答案提取。此外，图像说明可能包含不在图像中的细节，例如只有照片拍摄者才知道的额外细节或者个人意见，或者由于人类错误和偏见导致的信息与图像不一致。我们通过问答循环验证来自动过滤掉坏的生成结果。此外，分类任务本身通过使用固定的答案词汇来抑制这种错误的影响。然而，为了使自动生成更加强大，需要开发更多的方法来缩小错误或不匹配的范围。

最终得到的视觉问答模型可能会强化数据中存在的偏见和刻板印象。例如，它可能会学习到回答“这个人是什么性别？”这样的问题是一个二元选择，取决于浅薄的线索，或者“这个房间是为谁装饰的？”的答案取决于房间里呈现的特征（存在或不存在）。对于这些问题的缓解策略超出了本论文的范围，但我们鼓励研究界考虑解决这些问题作为技术成功部署的核心问题。

<a id="7总结"></a>
### 7 总结 Conclusions

在这篇论文中，我们展示了可以从现有的图像描述数据集中自动地大规模生成高质量的视觉问答训练数据。我们的方法 VQ2A 使用语句解析来标注候选答案，然后使用神经模型进行问题生成和问题验证。我们证明了仅使用此类数据进行训练的视觉问答模型在 VQA2.0 和 GQA 上具有很高的零样本性能，并且提供了使用 VQ2A 从自动产生的图像-问题-答案三元组构建的视觉问答系统与使用人工注释示例构建的系统的脆弱性的证据。

未来的工作包括探索包含数十亿个示例的更大规模的自动化图像文本数据集，并测试 VQ2A 在英语以外的语言中的适用性，因为在这些语言中缺乏人工注释的视觉问答数据。

