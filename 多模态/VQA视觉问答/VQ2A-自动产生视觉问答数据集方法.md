# All You May Need for VQA are Image Captions

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/2205.01883.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
视觉问答（VQA）得益于越来越复杂的模型，但在数据生成方面并没有享受到同等程度的参与。在本文中，我们提出了一种方法，通过利用现有图像标题注释的丰富性，并结合用于文本问题生成的神经模型来自动产生大量的 VQA 示例。我们证明了结果数据的质量很高。使用我们的数据训练的 VQA 模型在零样本准确率上比最先进的模型提高了两位数，并且达到了相同模型在人类标记的 VQA 数据上所缺乏的鲁棒性水平。

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
视觉问答（VQA）是一项复杂的多模态任务，为了成功建模和评估，需要大量的注释数据，而这些数据不能自然地由现有的业务流程产生，就像翻译对齐注释（Guo等人，2018年）或图像alt文本注释（Sharma等人，2018年）那样。  

目前，开发有助于下游应用（如盲人、医学和教育领域）的鲁棒视觉问答系统的瓶颈似乎是缺乏数百万级的大规模图像-问题-答案训练三元组。对这些三元组进行手动注释成本高、耗时长，并且容易受到各种人类偏见的影响，而这些人类偏见很难解释清楚（Yuan, 2021）。此外，针对此类人工标注进行训练的视觉问答系统的脆弱性已被充分理解并记录在案（Agrawal et al., 2018；Kafla 和 Kanan, 2017）。

为了克服数据限制，我们转向了一个创造视觉问答示例的潜在来源：图像英语标题对（陈等，2015年；夏尔马等，2018年）。大规模图像标题数据集存在数百万（昌平诺等，2021）、数亿（拉德福德等，2021）甚至数十亿（贾等，2021）个例子。标题主要以陈述句的形式出现，例如“两只熊正在冰上躺着”。然而，将陈述性标题转换为视觉问答问题/答案对的任务仍然很大程度上未被探索。它需要根据标题文本自动诱导适合于视觉问答任务的答案候选项及其相应的问题（图1）。我们注意到将陈述形式转换为疑问形式加上答案似乎至关重要，因为有证据表明在声明语言数据上训练的视觉-语言模型不能成功地适应或转移“out-of-the-box”到视觉问答（王等，2021）。

<img width="218" alt="image" src="https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/614e3959-e8da-4259-b97b-39a031e2e12f">

（图 1：给定一个英文标题（以及相应的图像），我们的 VQ2A 方法可以生成高质量的问题-答案对。这些图像-问题-答案三元组数据可以自动产生，并且以百万计的数据量使用，用于有效地训练视觉问答系统。）


<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work


<a id="3.网络结构"></a>
## 3.网络结构


<a id="4.实验Experiments"></a>
## 4.实验 Experiments

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion










