# Learning Transferable Visual Models From Natural Language Supervision

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.引言和动机 Introduction and Motivating Work**](#1.引言和动机IntroductionandMotivatingWork)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
&emsp;[**2.1 自然语言监督**](#2.1自然语言监督)  
&emsp;[**2.2 创建一个足够大的数据集**](#2.2创建一个足够大的数据集)  
&emsp;[**2.3 选择高效的预训练方法**](#2.3选择高效的预训练方法)  
&emsp;[**2.4 选择高效的预训练方法**](#2.4选择和缩放模型)  
&emsp;[**2.5 训练**](#2.5训练)  
[**3.网络结构**](#3.网络结构)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  https://arxiv.org/pdf/2103.00020.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
最先进的计算机视觉系统经过训练，可以预测一组固定的预定义对象类别。这种受限的监督形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从原始文本中学习有关图像的信息是一种有前途的替代方法，它利用了更广泛的监督来源。我们证明，预测哪个标题与哪个图像相配的简单预训练任务是一种高效且可扩展的方式，可以从头开始学习具有最先进性能的图像表示，使用了从互联网收集的4亿个（图像，文本）对的数据集。在预训练之后，自然语言用于引用学习到的视觉概念（或描述新概念），从而实现模型对下游任务的zero-shot迁移。我们通过在30多个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，涵盖了OCR、视频动作识别、地理定位和许多类型的细粒度物体分类等任务。该模型对大多数任务都有显著迁移效果，并且通常与完全监督的基准模型竞争，而无需进行任何特定数据集的训练。例如，我们在ImageNetzero-shot任务上的准确性与原始的ResNet-50相当，而不需要使用其训练所需的128万个训练样本。预训练权重：https://github.com/OpenAI/CLIP

<a id="1.引言和动机IntroductionandMotivatingWork"></a>
## 1.引言和动机 Introduction and Motivating Work
在过去几年中，直接从原始文本学习的预训练方法彻底改变了自然语言处理领域（Dai＆Le，2015; Peters等，2018; Howard＆Ruder，2018; Radford等，2018; Devlin等，2018; Raffel等，2019）。无任务的目标，如自回归和掩码语言建模，在计算、模型容量和数据方面跨越了多个数量级，并不断提高能力。以“文本到文本”作为标准化的输入-输出接口（McCann等，2018; Radford等，2019; Raffel等，2019）的发展，使得无任务架构可以zero-shot迁移到下游数据集，无需专门的输出头或数据集特定的定制。像GPT-3（Brown等，2020）这样的旗舰系统现在在许多任务上与专门定制的模型竞争，同时几乎不需要数据集特定的训练数据。

这些结果表明，现代预训练方法在规模庞大的网络文本集合中获得的聚合监督优势超过了高质量的众包标注NLP数据集。然而，在其他领域，如计算机视觉，仍然常见的做法是在众包标注的数据集（如ImageNet，Deng等，2009）上对模型进行预训练。可扩展的直接从网络文本学习的预训练方法是否能在计算机视觉领域取得类似的突破？先前的工作是令人鼓舞的。

20多年前，Mori等人（1999）探索了通过训练模型来预测与图像配对的文本文档中的名词和形容词，从而改进基于内容的图像检索。Quattoni等人（2007）证明了可以通过在训练用于预测与图像相关联的标题中的单词的分类器的权重空间中进行流形学习，来学习更具数据效率的图像表示。Srivastava＆Salakhutdinov（2012）通过在低级图像和文本标签特征之上训练多模式深度玻尔兹曼机，探索了深度表示学习。Joulin等人（2016）现代化了这一工作，并证明了训练用于预测图像标题中的单词的CNN可以学习有用的图像表示。他们将YFCC100M数据集（Thomee等，2016）中图像的标题、描述和标签元数据转换为一项词袋多标签分类任务，并展示了预训练AlexNet（Krizhevsky等，2012）来预测这些标签所学习的表示与基于ImageNet的预训练在迁移任务上表现相似。然后，Li等人（2017）扩展了这种方法，除了预测单个单词外，还预测了短语n-gram，并通过根据其学习的视觉n-gram字典对目标类进行打分并预测具有最高分数的类来展示了他们的系统zero-shot迁移到其他图像分类数据集的能力。采用更近期的架构和预训练方法，VirTex（Desai＆Johnson，2020）、ICMLM（Bulent Sariyildiz等，2020）和ConVIRT（Zhang等，2020）最近展示了基于变压器的语言建模、掩码语言建模和对比目标从文本中学习图像表示的潜力。

虽然作为概念验证令人兴奋，但利用自然语言监督进行图像表示学习仍然很少见。这可能是因为在常见基准测试上的表现远远低于替代方法。例如，Li等人（2017）在zero-shot设置下只达到了ImageNet 11.5％的准确率。这远低于当前最先进技术（Xie等，2020）的88.4％准确率，甚至低于经典计算机视觉方法（Deng等，2012）的50％准确率。相反，更狭窄但针对性更强的弱监督使用已经改善了性能。Mahajan等人（2018）表明，在Instagram图片上预测与ImageNet相关的标签是一项有效的预训练任务。当对ImageNet进行微调时，这些预训练模型提高了超过5％的准确率，并在当时改进了整体最先进技术水平。Kolesnikov等人（2019）和Dosovitskiy等人（2020）还通过预训练模型以预测JFT-300M数据集的有噪标记类别，在更广泛的转移基准测试中展示了大幅提升。

这一研究方向代表了当前实用的折中方式，介于从有限数量的监督“黄金标签”学习和从实际上无限数量的原始文本学习之间。然而，这种方法也并非没有妥协。这两项工作都精心设计并在过程中限制了它们的监督范围，分别为1000和18291个类别。自然语言通过其普遍性能够表达并因此监督更广泛的视觉概念。两种方法还使用静态softmax分类器来执行预测，并缺乏动态输出的机制。这严重限制了它们的灵活性，限制了它们的“zero-shot”能力。

这些弱监督模型与最近直接从自然语言学习图像表示的探索之间的一个关键区别是规模。Mahajan等人（2018）和Kolesnikov等人（2019）在数百万到数十亿张图片上对模型进行了加速训练，而VirTex、ICMLM和ConVIRT则在一到两十万张图片上进行了几天的训练。在本研究中，我们缩小了这个差距，并研究了在大规模自然语言监督下训练的图像分类器的行为。受互联网上公开可用的大量此类数据的启发，我们创建了一个新的数据集，包含4亿个（图片，文本）对，并证明了从头开始训练的简化版本ConVIRT（我们称之为CLIP，即对比语言-图像预训练）是一种有效的自然语言监督学习方法。我们通过训练一系列覆盖了近2个数量级的计算资源的八个模型来研究CLIP的可扩展性，并观察到迁移性能是计算资源的一个平滑可预测函数。我们发现，类似于GPT系列，CLIP在预训练期间学会了执行广泛的任务，包括OCR、地理定位、动作识别等等。我们通过对CLIP在超过30个现有数据集上的zero-shot迁移性能进行基准测试来衡量这一点，并发现它可以与之前的特定任务监督模型相竞争。我们还通过线性探针表示学习分析验证了这些结果，并显示CLIP优于最好的公开可用ImageNet模型，同时计算效率更高。此外，我们还发现zero-shot的CLIP模型比等精度的受监督ImageNet模型更加稳健，这表明无样本评估对于无关任务的模型更具代表性。这些结果具有重要的政策和伦理意义，我们在第7节中进行了讨论。

<a id="2.方法Approach"></a>
## 2.方法 Approach

<a id="2.1自然语言监督"></a>
### 2.1 自然语言监督 Natural Language Supervision
我们方法的核心是从自然语言中包含的监督中学习知觉。正如在引言中讨论的那样，这并不是一个新的想法，然而用于描述这一领域工作的术语各有不同，甚至看似矛盾，并且陈述的动机也是多种多样的。Zhang等人（2020），Gomez等人（2017），Joulin等人（2016）和Desai＆Johnson（2020）都提出了一些方法，这些方法从文本和图像配对中学习视觉表示，但分别将他们的方法描述为无监督、自监督、弱监督和监督。

我们强调这一系列工作的共同点并不是特定方法中的任何细节，而是对自然语言作为训练信号的重视。所有这些方法都是从自然语言监督中学习。尽管早期的工作在使用主题模型和n-gram表示时曾经纠结于自然语言的复杂性，但深度上下文表示学习的改进表明我们现在有能力有效地利用这一丰富的监督来源（McCann等，2017）。

从自然语言中学习具有与其他训练方法相比的几个潜在优势。与标准的众包标注图像分类相比，通过自然语言监督更容易扩展，因为它不需要注释以经典的“机器学习兼容格式”（如规范的1-of-N多数投票“金标签”）。相反，能够处理自然语言的方法可以 passively 从互联网上大量的文本中获得监督信息。从自然语言中学习还比大多数无监督或自监督学习方法具有重要优势，因为它不仅“只是”学习了一个表示，而且还将该表示与语言连接起来，从而实现了灵活的zero-shot迁移。在接下来的小节中，我们将详细介绍我们采用的具体方法。

<a id="2.2创建一个足够大的数据集"></a>
### 2.2 创建一个足够大的数据集 Creating a Sufficiently Large Dataset
创建一个足够大的数据集
现有的工作主要使用了三个数据集，MS-COCO（Lin等，2014），Visual Genome（Krishna等，2017）和YFCC100M（Thomee等，2016）。尽管MS-COCO和Visual Genome是高质量的众包标注数据集，但按照现代标准，它们的规模较小，每个数据集大约有10万张训练照片。相比之下，其他计算机视觉系统的训练数据可以达到35亿张Instagram照片（Mahajan等，2018）。YFCC100M数据集拥有1亿张照片，是一个可能的替代选择，但每张图片的元数据稀疏且质量参差不齐。许多图片使用自动生成的文件名，如“20160716 113957.JPG”作为“标题”，或包含相机曝光设置的“描述”。在筛选出仅保留具有自然语言标题和/或英文描述的图片后，该数据集的规模缩小了6倍，只剩下1500万张照片。这大致与ImageNet的大小相当。

自然语言监督的一个主要动机是公开互联网上大量此类数据的可用性。由于现有数据集并不能充分反映这种可能性，仅考虑它们的结果将低估这一研究方向的潜力。为了解决这个问题，我们构建了一个新的数据集，其中包含4亿个（图像，文本）对，这些数据是从互联网上各种公开来源收集而来。为了尝试尽可能涵盖广泛的视觉概念，我们在构建过程中搜索包含50万个查询之一的文本的（图像，文本）对。我们通过每个查询最多包含2万个（图像，文本）对来实现结果的近似类平衡。最终的数据集与用于训练GPT-2的WebText数据集具有类似的总字数。我们将这个数据集称为WIT，即WebImageText。 

<a id="2.3选择高效的预训练方法"></a>
### 2.3 选择高效的预训练方法 Selecting an Efficient Pre-Training Method
目前最先进的计算机视觉系统使用了大量的计算资源。Mahajan等人（2018）需要19个GPU年来训练他们的ResNeXt101-32x48d模型，而Xie等人（2020）需要33个TPUv3核心年来训练他们的Noisy Student EfficientNet-L2模型。考虑到这两个系统仅被训练用于预测1000个ImageNet类别，从自然语言中学习一个开放的视觉概念集合似乎是一项艰巨的任务。在我们的努力过程中，我们发现训练效率是成功扩展自然语言监督的关键，我们根据这个指标选择了最终的预训练方法。

我们最初的方法与VirTex类似，从零开始联合训练图像CNN和文本Transformer来预测图像的标题。然而，我们在有效扩展这种方法时遇到了困难。如图2所示，一个6300万参数的Transformer语言模型，其计算资源已经是ResNet-50图像编码器的两倍，学习识别ImageNet类别的速度比一个更简单的基准模型慢三倍，该基准模型预测相同文本的词袋编码。

这两种方法都有一个关键的相似之处。它们尝试预测每个图像附带的文本的确切词语。这是一项困难的任务，因为伴随图像出现的描述、评论和相关文本种类繁多。最近在图像对比表示学习方面的研究发现，对比目标可以学习比其等效的预测目标更好的表示（Tian等，2019）。其他研究发现，虽然图像的生成模型可以学习到高质量的图像表示，但其计算资源需求比具有相同性能的对比模型要高一个数量级以上（Chen等，2020a）。鉴于这些发现，我们探索了训练一个系统来解决可能更容易的代理任务，即仅预测哪个文本与哪个图像配对，而不是准确预测该文本的确切词语。从图2中的相同词袋编码基准开始，我们将预测目标替换为对比目标，并观察到在零-shot迁移到ImageNet的速率上进一步提高了4倍。

给定一个N个（图像，文本）对的批次，CLIP被训练来预测在一个批次中发生的N×N个可能的（图像，文本）配对中的哪一个。为了做到这一点，CLIP通过联合训练图像编码器和文本编码器来最大化批次中N个真实配对的图像和文本嵌入的余弦相似性，同时最小化N2-N个错误配对的嵌入的余弦相似性，从而学习一个多模态嵌入空间。我们优化这些相似度分数的对称交叉熵损失。在图3中，我们包含了CLIP实现核心的伪代码。据我们所知，这种批次构建技术和目标首次在深度度量学习领域被引入，作为多类N对损失（Sohn，2016），并被Oord等人（2018）作为InfoNCE损失推广用于对比表示学习，在医学图像领域由Zhang等人（2020）最近用于对比（文本，图像）表示学习。

由于我们的预训练数据集非常庞大，过拟合并不是一个主要关注点，因此与Zhang等人（2020）的实现相比，训练CLIP的细节得到了简化。我们从头开始训练CLIP，而不是使用ImageNet权重初始化图像编码器或使用预训练权重初始化文本编码器。我们也没有使用表示和对比嵌入空间之间的非线性投影，这一变化是由Bachman等人（2019）引入的，并被Chen等人（2020b）推广。相反，我们只使用线性投影将每个编码器的表示映射到多模态嵌入空间。我们没有注意到这两个版本之间在训练效率上的差异，并推测非线性投影可能会随着当前图像的细节在自监督表示学习方法中被协同适应。我们还删除了Zhang等人（2020）中的文本转换函数tu，该函数从文本中均匀抽样出一个句子，因为CLIP预训练数据集中的许多（图像，文本）对只包含一个句子。我们还简化了图像转换函数tv。在训练过程中，我们仅使用从调整大小的图像中随机裁剪出的正方形作为数据增强。最后，用于控制softmax中logits范围的温度参数τ 直接在训练过程中进行优化，作为一个对数参数化的乘法标量，以避免成为超参数。

<a id="2.4选择和缩放模型"></a>
### 2.4 选择和缩放模型 Choosing and Scaling a Model
我们考虑了两种不同的架构作为图像编码器。对于第一种架构，我们使用ResNet-50（He等人，2016a）作为图像编码器的基础架构，因为它被广泛采用并且性能已经得到证实。我们对原始版本进行了几处修改，采用了He等人（2019）提出的ResNetD改进和Zhang（2019）的抗锯齿rect-2模糊池化。我们还用注意力池化机制替换了全局平均池化层。注意力池化被实现为一个“变压器风格”的多头查询-键-值（QKV）注意力的单层，其中查询是由图像的全局平均池化表示条件化而来。对于第二种架构，我们尝试了最近推出的Vision Transformer（ViT）（Dosovitskiy等人，2020）。我们紧密遵循了他们的实现，只对组合的补丁和位置嵌入在变压器之前添加了额外的层归一化，并使用了稍微不同的初始化方案。

文本编码器采用了Transformer架构（Vaswani等人，2017），并进行了Radford等人（2019）描述的架构修改。我们使用了一个6300万参数、12层512宽的模型作为基准尺寸，其中包括8个注意力头。Transformer对文本进行小写字节对编码（BPE）表示，词汇量为49152（Sennrich等人，2015）。为了提高计算效率，最大序列长度限制在76。文本序列使用[SOS]和[EOS]标记进行标注，而transformer在[EOS]标记处的最高层激活被视为文本的特征表示。这些激活经过层归一化处理，然后线性投影到多模态嵌入空间。文本编码器采用了掩码自注意力机制，以保留使用预训练语言模型进行初始化或将语言建模作为辅助目标的能力，但对此的探索留作未来的工作。

虽然以往的计算机视觉研究通常通过增加宽度（Mahajan等，2018）或深度（He等，2016a）来扩展模型，但对于ResNet图像编码器，我们采用了谭乐（Tan & Le，2019）的方法，发现将额外的计算资源分配到宽度、深度和分辨率的所有维度上，优于仅将其分配给模型的单个维度。虽然谭乐（Tan & Le，2019）针对其EfficientNet架构调整了分配给每个维度的计算比例，但我们使用了一个简单的基准方法，即将额外的计算资源平均分配到增加模型宽度、深度和分辨率上。对于文本编码器，我们仅将模型的宽度按照已计算的ResNet宽度增加比例进行调整，并且根据我们的发现，我们并未对模型的深度进行任何调整，因为我们发现CLIP的性能对于文本编码器的容量不太敏感。

<a id="2.5训练"></a>
### 2.5 训练 Training
我们训练了一系列的5个ResNets和3个Vision Transformers。对于ResNets，我们训练了一个ResNet-50，一个ResNet-101，然后又训练了3个遵循EfficientNet风格的模型，它们的计算资源分别是ResNet-50的4倍、16倍和64倍。它们分别被标记为RN50x4、RN50x16和RN50x64。对于Vision Transformers，我们训练了一个ViT-B/32，一个ViT-B/16和一个ViT-L/14。我们将所有模型训练了32个epochs。我们使用Adam优化器（Kingma和Ba，2014），应用于除增益和偏差之外的所有权重上的解耦权重衰减正则化（Loshchilov和Hutter，2017），并使用余弦调度（Loshchilov和Hutter，2016）来衰减学习率。初始超参数是在将ResNet-50基线模型训练1个epoch时，使用网格搜索、随机搜索和手动调整的组合来设置的。由于计算约束，超参数在较大的模型上经过启发式调整。可学习的温度参数τ的初始值设定为等效于0.07（来自Wu等人，2018），并进行剪裁，以防止将logits按比例放大超过100，我们发现这对于防止训练不稳定是必要的。我们使用非常大的minibatch大小为32,768进行训练。我们使用混合精度（Micikevicius等，2017）来加速训练和节省内存。为了节省额外的内存，使用了梯度检查点（Griewank和Walther，2000；Chen等，2016）、半精度Adam统计量（Dhariwal等，2020）和半精度随机舍入的文本编码器权重。嵌入相似性的计算也被分片处理，每个GPU只计算其本地嵌入批次所需的子集对之间的相似性。最大的ResNet模型RN50x64在592个V100 GPU上训练了18天，而最大的Vision Transformer在256个V100 GPU上训练了12天。对于ViT-L/14，我们还在更高的336像素分辨率下预训练了一个额外的epoch，以提高性能，类似于FixRes（Touvron等，2019）。我们将这个模型标记为ViT-L/14@336px。除非另有说明，本文中报告的所有结果都使用我们发现性能最佳的这个模型“CLIP”。

<a id="3.实验Experiments"></a>
## 3.实验 Experiments

### 3.1 Zero-Shot Transfer
### 3.1.1 MOTIVATION
在计算机视觉中，zero-shot学习通常指的是在图像分类中对未见过的物体类别进行泛化的研究（Lampert等，2009）。然而，我们在更广泛的意义上使用这个术语，并研究对未见过的数据集的泛化能力。我们将这个视角作为执行未见任务的代理，正如Larochelle等人（2008）在零数据学习论文中所期望的那样。虽然无监督学习领域的许多研究侧重于机器学习系统的表示学习能力，但我们认为研究zero-shot迁移是衡量机器学习系统任务学习能力的一种方式。从这个角度来看，数据集评估了机器学习系统在特定分布上执行任务的性能。然而，许多流行的计算机视觉数据集主要是由研究界创建的，用于引导通用图像分类方法的发展，而不是衡量特定任务的性能。尽管可以说SVHN数据集衡量了在Google街景照片的分布上的街道编号转录任务，但不清楚CIFAR-10数据集衡量的是什么"真正的"任务。然而，CIFAR-10数据集是从TinyImages（Torralba等，2008）中抽取的分布。在这类数据集上，zero-shot迁移更多地是评估CLIP对分布变化和领域泛化的鲁棒性，而不是任务泛化。请参阅第3.3节中针对此问题的分析。

据我们所知，Visual N-Grams（Li等，2017）首次研究了按照上述方式对现有图像分类数据集进行zero-shot迁移。这也是我们所知道的唯一一项研究，它使用通用预训练模型研究了对标准图像分类数据集的zero-shot迁移，并成为衡量CLIP的最佳参考点。他们的方法学习了一个包含142,806个视觉n-gram（由1到5个词组成）的字典的参数，并使用Jelinek-Mercer平滑的差分版本优化这些n-gram，以最大化给定图像的所有文本n-gram的概率。为了进行zero-shot迁移，他们首先将数据集每个类别名称的文本转换为其n-gram表示，然后根据模型计算其概率，并预测得分最高的类别。

我们对研究zero-shot迁移作为任务学习评估的关注，受到了在自然语言处理领域展示任务学习的工作的启发。据我们所知，Liu等人（2018）首次将任务学习确定为语言模型训练的一个“意外的副作用”，当一个用于生成维基百科文章的语言模型学会可靠地在不同语言之间进行姓名转录。虽然GPT-1（Radford等，2018）侧重于预训练作为一种迁移学习方法，以改进监督微调，但它还包括一个消融研究，证明了在预训练过程中四种启发式zero-shot迁移方法的性能稳步提高，而无需任何监督调整。这项分析成为了GPT-2（Radford等，2019）的基础，后者专注于通过zero-shot迁移研究语言模型的任务学习能力。

### 3.1.2 USING CLIP FOR ZERO-SHOT TRANSFER
CLIP被预训练用于预测图像和文本片段是否在其数据集中配对。为了进行zero-shot分类，我们重复利用了这一能力。对于每个数据集，我们使用数据集中所有类别的名称作为潜在的文本配对，并根据CLIP来预测最有可能的（图像，文本）配对。稍微详细地说，我们首先通过它们各自的编码器计算图像的特征嵌入和可能文本集的特征嵌入。然后计算这些嵌入的余弦相似度，乘以一个温度参数τ，并通过softmax归一化为概率分布。需要注意的是，这个预测层是一个多项式逻辑回归分类器，具有L2归一化的输入、L2归一化的权重，无偏置，以及温度缩放。按照这种解释方式，图像编码器是计算图像特征表示的计算机视觉骨干，而文本编码器是一个超网络（Ha等，2016），它基于描述类别代表的视觉概念的文本生成线性分类器的权重。Lei Ba等人（2015）首次引入了这种形式的zero-shot图像分类器，而从自然语言生成分类器的想法至少可以追溯到Elhoseiny等人（2013）。继续这种解释，CLIP预训练的每一步都可以被看作是优化随机创建的代理，以匹配包含每个类别一个示例，并通过自然语言描述定义了32,768个总类别的计算机视觉数据集的性能。对于zero-shot评估，一旦文本编码器计算出zero-shot分类器，我们就会将其缓存，并在所有后续预测中重复使用。这样可以实现生成成本在数据集中所有预测中的摊销。

### 3.1.3 INITIAL COMPARISON TO VISUAL N-GRAMS
在表1中，我们将Visual N-Grams与CLIP进行了比较。最好的CLIP模型将在ImageNet上的准确率从概念验证的11.5%提高到76.2%，并且与原始的ResNet-50性能相匹敌，尽管CLIP没有使用该数据集的1.28百万个众包标记的训练样本。此外，CLIP模型的前5准确率明显高于其前1准确率，该模型的前5准确率达到95%，与Inception-V4（Szegedy等，2016）相匹敌。在zero-shot设置中与强大的完全监督基线性能相匹配的能力表明，CLIP是朝着灵活和实用的zero-shot计算机视觉分类器迈出的重要一步。正如上文所述，与Visual N-Grams的比较旨在为CLIP的性能提供背景，并不应解释为直接比较CLIP和Visual N-Grams的方法，因为许多性能相关的差异没有得到控制。例如，我们在一个比Visual N-Grams大10倍的数据集上训练，使用的视觉模型每次预测需要近100倍的计算资源，很可能使用了超过他们1000倍的训练计算资源，并且使用了当Visual N-Grams发表时还不存在的基于Transformer的模型。作为更为接近的比较，我们在与Visual N-Grams训练数据集相同的YFCC100M数据集上训练了一个CLIP ResNet-50，并发现它在一天的V100 GPU训练时间内就达到了他们报告的ImageNet性能。这个基准还是从头开始训练的，而不是像Visual N-Grams那样从预训练的ImageNet权重初始化。

CLIP在另外两个报告的数据集上也表现优于Visual N-Grams。在Yahoo数据集上，CLIP将错误数量减少了95%，而在SUN数据集上，CLIP的准确率是Visual N-Grams的两倍以上。为了进行更全面的分析和压力测试，我们实现了一个更大的评估套件，详见附录A。总共，我们从Visual N-Grams报告的3个数据集扩展到超过30个数据集，并与超过50个现有的计算机视觉系统进行比较，以提供结果的背景。

### 3.1.4. PROMPT ENGINEERING AND ENSEMBLING
大多数标准的图像分类数据集将命名或描述类别的信息作为附带的内容来处理，这使得基于自然语言的zero-shot迁移成为一个事后才考虑的问题。绝大多数数据集只用数字标签注释图像，并包含一个文件将这些标签映射回英文名称。一些数据集（如Flowers102和GTSRB）在其发布版本中似乎根本不包含这种映射，完全阻止了zero-shot迁移。我们观察到，对于许多数据集，这些标签可能是有些随意选择的，并且没有预料到与zero-shot迁移相关的问题，而zero-shot迁移依赖于任务描述以实现成功的迁移。

一个常见的问题是多义性。当类别的名称是CLIP文本编码器唯一提供的信息时，由于缺乏上下文，它无法区分所指的词义。在某些情况下，相同单词的多个意思可能被包括在同一个数据集的不同类别中！这在ImageNet数据集中发生，其中既包含建筑起重机，也包含飞行的鹤。另一个例子可以在Oxford-IIIT宠物数据集的类别中找到，其中单词"boxer"从上下文明显是指狗的品种，但对于缺乏上下文的文本编码器来说，它同样有可能指的是一种运动员类型。

我们遇到的另一个问题是，在我们的预训练数据集中，图像配对的文本通常不只是一个单词。通常，文本是对图像进行某种描述的完整句子。为了弥合这种分布差异，我们发现使用提示模板“一张{label}的照片。”作为一个良好的默认选择，有助于指定文本是关于图像内容的。这通常比仅使用标签文本的基准性能要好。例如，仅使用此提示就能将在ImageNet上的准确率提高1.3%。

类似于围绕GPT3进行的“提示工程”讨论（Brown等，2020；Gao等，2020），我们还观察到，通过定制每个任务的提示文本，可以显著提高zero-shot性能。以下是一些例子，但并非详尽无遗。我们发现，在几个细粒度图像分类数据集上，明确指定类别有助于提高性能。例如，在Oxford-IIIT宠物数据集上，使用“一张{label}的照片，一种宠物类别。”以帮助提供上下文效果很好。同样，在Food101上指定食物类型，在FGVC飞机数据集上指定飞机类型也有帮助。对于OCR数据集，我们发现在待识别的文本或数字周围加上引号可以提高性能。最后，我们发现在卫星图像分类数据集上，明确指定这些图像的形式有助于提高性能，我们使用了“一张{label}的卫星照片。”的变体。

我们还尝试了通过多个zero-shot分类器的集成来提高性能的另一种方法。这些分类器是通过使用不同的上下文提示（例如“A photo of a big {label}”和“A photo of a small {label}”）计算得出的。我们构建集成模型时是在嵌入空间而不是概率空间上操作的。这使我们能够缓存一组平均文本嵌入，因此当对许多预测进行摊销时，集成的计算成本与使用单个分类器的成本相同。我们观察到，通过对许多生成的zero-shot分类器进行集成可以可靠地提高性能，并且我们将其应用于大多数数据集。在ImageNet上，我们对80个不同的上下文提示进行集成，这使性能比上文提到的单个默认提示再提高了3.5%。综合考虑提示工程和集成，可以将ImageNet的准确性提高近5%。在图4中，我们可视化了提示工程和集成如何改变一组CLIP模型的性能，相对于Li等人（2017年）所采用的无上下文基准方法直接嵌入类名。

### 3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE
由于针对计算机视觉的任务不可知的zero-shot分类器研究不足，CLIP为我们提供了一个更好地了解这种类型模型的机会。在本节中，我们对CLIP的zero-shot分类器的各种属性进行了研究。作为第一个问题，我们简单地看一下zero-shot分类器的表现如何。为了将其置于上下文中，我们将其与一个简单的现成基准性能进行比较：对经典ResNet-50模型的特征进行全监督、正则化的逻辑回归分类器拟合。在图5中，我们展示了这27个数据集的比较情况。有关数据集和设置的详细信息，请参阅附录A。

zero-shotCLIP在大多数情况下略微优于基准性能，并在27个数据集中有16个取得了胜利。观察各个数据集揭示了一些有趣的行为。在细粒度分类任务中，我们观察到性能存在较大差异。在斯坦福汽车和Food101这两个数据集上，zero-shotCLIP的表现超过了对ResNet-50特征进行逻辑回归的20%，而在Flowers102和FGVCAircraft这两个数据集上，zero-shotCLIP的表现不及10%。在OxfordPets和Birdsnap上，性能要接近得多。我们怀疑这些差异主要是由于WIT和ImageNet之间的任务特定监督量不同造成的。在ImageNet、CIFAR10/100、STL10和PascalVOC2007等“通用”目标分类数据集上，性能相对类似，zero-shotCLIP在所有情况下略有优势。在STL10上，CLIP实现了99.3%的整体准确率，尽管没有使用任何训练样本，但似乎达到了最新的技术水平。zero-shotCLIP在衡量视频动作识别的两个数据集上明显优于ResNet-50。在Kinetics700上，CLIP的表现比ResNet-50高出14.5%。在UCF101上，zero-shotCLIP也比ResNet-50的特征高出7.7%。我们推测这是由于自然语言相对于ImageNet中以名词为中心的物体监督来说，为涉及动词的视觉概念提供了更广泛的监督。

观察zero-shotCLIP明显表现不佳的领域，我们发现在一些专业化、复杂或抽象的任务上，zero-shotCLIP相当薄弱，比如卫星图像分类（EuroSAT和RESISC45）、淋巴结肿瘤检测（PatchCamelyon）、合成场景中的物体计数（CLEVRCounts）、与自动驾驶相关的任务，如德国交通标志识别（GTSRB）和识别与最近车辆的距离（KITTI Distance）。这些结果凸显了zero-shotCLIP在更复杂任务上的能力不足。相比之下，非专家人员可以稳健地完成其中几个任务，比如计数、卫星图像分类和交通标志识别，这表明还有很大的改进空间。然而，我们需要注意的是，对于学习者在没有先前经验的情况下面对的困难任务，如几乎所有人类（可能包括CLIP）对淋巴结肿瘤分类的情况，衡量zero-shot迁移而不是少样本迁移是否具有意义尚不清楚。

将zero-shot性能与完全监督模型进行比较可以揭示CLIP的任务学习能力，而与少样本方法进行比较则更直接，因为zero-shot是其极限。在图6中，我们将zero-shotCLIP与少样本逻辑回归在许多图像模型的特征上进行了可视化比较，包括最好的公开可用的ImageNet模型、自监督学习方法以及CLIP本身。虽然直观上预期zero-shot的表现不及一样本，但我们发现zero-shotCLIP和4样本逻辑回归在相同的特征空间上表现相当。这可能是zero-shot和少样本方法之间一个重要差异的结果。首先，CLIP的zero-shot分类器是通过自然语言生成的，这允许直接指定（“传达”）视觉概念。相比之下，“正常”的监督学习必须间接地从训练示例中推断概念。无上下文示例学习的缺点是许多不同的假设都可以与数据一致，尤其是在一样本情况下。单个图像通常包含许多不同的视觉概念。尽管能力强大的学习者能够利用视觉线索和启发式方法，比如假设被演示的概念是图像中的主要对象，但这并不保证。

解决zero-shot和少样本性能差异的一个潜在方法是使用CLIP的zero-shot分类器作为少样本分类器权重的先验。虽然向生成的权重添加L2惩罚是对这一思想的直接实现，但我们发现超参数优化通常会选择这种正则化项的一个非常大的值，导致得到的少样本分类器“仅仅”就是zero-shot分类器。研究如何更好地结合zero-shot迁移的能力和少样本学习的灵活性是未来工作的一个有希望的方向。

当将zero-shotCLIP与其他模型的特征上的少样本逻辑回归进行比较时，zero-shotCLIP大致匹配了我们评估套件中表现最佳的16样本分类器的性能。该16样本分类器使用了在ImageNet-21K上训练的BiT-M ResNet-152x2的特征。我们确信在JFT-300M上训练的BiT-L模型会表现得更好，但这些模型尚未公开发布。在16样本环境中，BiT-M ResNet-152x2表现最佳，这有些令人惊讶，因为正如第3.2节所分析的那样，Noisy Student EfficientNet-L2在完全监督的情况下在27个数据集上的平均表现要比它高出近5%。

除了研究zero-shotCLIP和少样本逻辑回归的平均性能外，我们还对各个数据集的性能进行了检查。在图7中，我们展示了在相同特征空间上，为了达到zero-shotCLIP的性能水平，逻辑回归分类器需要每个类别的有标签样本数量的估计值。由于zero-shotCLIP也是一个线性分类器，这个估计值反映了在这种情况下zero-shot迁移的有效数据利用率。为了避免训练数千个线性分类器，我们基于每个数据集上的1、2、4、8、16个样本（如果可能的话），以及完全监督的线性分类器的性能的对数线性插值来估计有效数据利用率。我们发现，zero-shot迁移的数据利用率在不同数据集之间差异很大，从每个类别不到1个有标签样本到184个有标签样本不等。其中，Flowers102和EuroSAT两个数据集的性能低于一次样本模型。一半的数据集每个类别只需要少于5个样本，中位数为5.4个样本。然而，平均估计的数据利用率为每个类别20.8个样本。这是因为在20%的数据集中，监督分类器需要许多有标签样本才能达到相同的性能。在ImageNet上，zero-shotCLIP的性能与在相同特征空间上训练的16次样本线性分类器相匹配。

如果我们假设评估数据集足够大，以至于在其上训练的线性分类器的参数被很好地估计，那么，由于CLIP的zero-shot分类器也是一个线性分类器，完全监督分类器的性能大致为zero-shot迁移可以实现的上限。在图8中，我们比较了CLIP在各个数据集上的zero-shot性能与完全监督线性分类器的性能。虚线y=x代表了一个“最佳”zero-shot分类器，它与完全监督等效分类器的性能相匹配。对于大多数数据集，zero-shot分类器的性能仍然比完全监督分类器低10%到25%，这表明在改进CLIP的任务学习和zero-shot迁移能力方面仍有很大的空间。

zero-shot性能与完全监督性能之间存在0.82的正相关性（p值<10^-6），这表明CLIP在连接底层表示和任务学习与zero-shot迁移方面相对一致。然而，在5个数据集（STL10、CIFAR10、Food101、OxfordPets和Caltech101）上，zero-shotCLIP只能接近完全监督性能。在这5个数据集上，zero-shot准确率和完全监督准确率都超过90%。这表明在其底层表示质量较高的任务中，CLIP可能更有效地进行zero-shot迁移。线性回归模型预测每提高1%的完全监督性能，zero-shot性能提高1.28%。然而，95%置信区间仍包括小于1的值（0.93-1.79）。

在过去几年中，关于深度学习系统的经验研究已经证明，性能可以通过重要因素如训练计算资源和数据集大小来预测（Hestness等，2017；Kaplan等，2020）。迄今为止，GPT模型系列在训练计算资源增加1000倍的情况下，zero-shot性能一直在稳步提升。在图9中，我们检查CLIP的zero-shot性能是否遵循类似的扩展模式。我们绘制了5个ResNet CLIP模型在36个不同数据集上进行的39次评估的平均错误率，并发现CLIP在44倍模型计算资源增加的情况下也存在类似的对数-对数线性扩展趋势。尽管总体趋势是平滑的，但我们发现个别评估的性能可能会更加嘈杂。我们不确定这是由于子任务的个别训练运行之间的高方差（如D'Amour等人在2020年的研究中所记录的）掩盖了稳步提升的趋势，还是性能在某些任务上实际上是非单调的计算函数。

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion











