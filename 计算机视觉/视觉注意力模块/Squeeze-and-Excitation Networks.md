# 网络名称（此处应替换为具体的网络名称）

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.Squeeze-and-Excitation Blocks**](#3.Squeeze-and-ExcitationBlocks)  
[**4.模型和计算复杂度 Model and Computational Complexity**](#4.模型和计算复杂度ModelandComputationalComplexity)  
[**5.实施方案 Implementation**](#5.实施方案Implementation)  
 


## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  https://github.com/hujie-frank/SENet  
论文链接：  https://arxiv.org/pdf/1709.01507v2.pdf

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
卷积神经网络是建立在卷积操作之上的，通过在局部感受野内融合空间和通道信息来提取信息特征。为了增强网络的表征能力，最近的一些方法表明增强空间编码的好处。在这项工作中，我们专注于通道关系，并提出了一种新颖的架构单元，我们称之为“Squeeze-and-Excitation”（SE）块，它通过明确地建模通道之间的相互依赖关系，自适应地重新校准通道特征响应。我们证明通过堆叠这些块，我们可以构建出在具有挑战性的数据集上表现非常优秀的SENet架构。至关重要的是，我们发现SE块能够在极小的额外计算成本下，显著提高现有最先进的深度架构的性能。SENet奠定了我们在ILSVRC 2017分类比赛中获得一等奖并将前5错误率降低到2.251%的基础，相对于2016年的获奖作品，实现了约25%的改进。代码和模型可在 https://github.com/hujie-frank/SENet 上找到。

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
卷积神经网络（CNNs）已被证明是处理各种视觉任务的有效模型[21, 27, 33, 45]。对于每个卷积层，都会学习一组滤波器，以表达沿输入通道的局部空间连接模式。换句话说，期望卷积滤波器通过在局部感受野内融合空间和通道信息来获得信息丰富的组合。通过堆叠一系列卷积层，交替插入非线性操作和下采样，CNN能够以全局感受野捕获具有强大图像描述能力的分层模式。最近的研究表明，通过显式嵌入学习机制来帮助捕获空间相关性，而无需额外的监督，可以改善网络的性能。Inception架构[16, 43]普及了这种方法之一，它显示出网络可以通过在其模块中嵌入多尺度过程来实现竞争性的准确性。更近期的工作则致力于更好地建模空间依赖关系[1, 31]并整合空间注意力[19]。

在这篇论文中，我们研究了架构设计的另一个方面——通道关系，引入了一种新的架构单元，我们称之为“Squeeze-and-Excitation”（SE）块。我们的目标是通过明确地建模卷积特征通道之间的相互依赖关系来提高网络的表征能力。为实现这一目标，我们提出了一种机制，使网络能够执行特征重新校准，通过这种机制，网络可以学习利用全局信息来有选择性地增强特定的通道特征。

SE构建块的基本结构如图1所示。对于任何给定的变换$` F_{tr}：X→U `$，其中$` X  \in R^{H^{'}×W^{'}×C^{'}} `$，$`U  \in R^{H×W×C}`$（例如，一个卷积或一组卷积），我们可以构建相应的SE块，以执行特征重新校准，具体操作如下。首先，特征U经过一个挤压操作，该操作在空间维度H×W上聚合特征图，生成一个通道描述符。这个描述符嵌入了通道特征响应的全局分布，使得网络的下层可以利用来自全局感受野的信息。然后进行激励操作，通过基于通道依赖关系的自门控机制学习每个通道的样本特定激活，来控制每个通道的激励。最后，对特征图U进行重新加权，生成SE块的输出，然后可以直接馈送到后续层中。  
![image](https://github.com/Cloud-Jowen/Paper_Note/assets/56760687/8eaf8579-e505-4965-a75d-7bc84d9a1ee5)  


一个SE网络可以通过简单地堆叠一系列SE构建块来生成。SE块也可以被用作架构中任何深度处原始块的即插即用替代品。然而，虽然构建块的模板是通用的，正如我们在第6.4节中所展示的，它在不同深度的角色会根据网络的需求进行调整。在早期层中，它学会以与类别无关的方式激发信息丰富的特征，增强共享的较低级表示的质量。在后续层中，SE块变得越来越专业化，并以高度特定于类别的方式响应不同的输入。因此，通过特征重新校准所进行的益处

可以通过简单堆叠一系列SE构建块来生成SE网络。SE块还可以作为原始块在架构的任何深度的即插即用替代品。然而，尽管构建块的模板是通用的，正如我们在第6.4节中所示，它在不同深度的角色会根据网络的需求进行调整。在早期层中，它学会以与类别无关的方式激发信息丰富的特征，增强共享的较低级表示的质量。在后续层中，SE块变得越来越专业化，并以高度特定于类别的方式响应不同的输入。因此，由SE块进行的特征重新校准所带来的益处可以在整个网络中累积起来。

设计新的CNN架构是一个具有挑战性的工程任务，通常涉及许多新的超参数和层配置的选择。相比之下，上面概述的SE块的设计是简单的，并且可以直接与现有的最先进的架构一起使用，这些架构的模块可以通过直接用它们的SE块进行加强替换。

此外，正如在第4节中所示，SE块计算轻量，并且只会对模型复杂性和计算负担产生轻微增加。为了支持这些说法，我们开发了几个SENet，并在ImageNet 2012数据集[34]上进行了广泛评估。为了证明它们的普适性，我们还展示了超出ImageNet范围的结果，表明所提出的方法不限于特定的数据集或任务。

使用SENets，我们在ILSVRC 2017分类竞赛中获得了第一名。我们的表现最佳的模型组合在测试集上实现了2.251%的top-5错误率。与前一年的冠军条目（top-5错误率为2.991%）相比，这代表了近25%的相对改进。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work
**深度架构** VGGNet [39] 和Inception模型 [43] 展示了增加深度的好处。批量归一化（Batch normalization，BN）[16] 通过插入单元来调节层输入，稳定了学习过程，从而改善了梯度传播。ResNet [10, 11] 通过使用基于恒等映射的跳跃连接展示了学习更深层网络的有效性。Highway网络 [40] 使用门控机制来调节快捷连接。对网络层之间连接的重新构造 [5, 14] 已被证明进一步改善了深度网络的学习和表示属性。

另一条研究方向探索了调整网络模块组件的功能形式的方法。分组卷积可用于增加基数（变换集的大小）[15, 47]。多分支卷积可以被解释为对这一概念的泛化，从而实现更灵活的操作符组合[16, 42, 43, 44]。最近，以自动方式学习的组合[26, 54, 55]已经显示出竞争性能。交叉通道相关性通常被映射为特征的新组合，可以独立于空间结构[6, 20]或者通过使用标准卷积滤波器[24]和1×1卷积联合进行。其中大部分工作集中在减少模型和计算复杂性的目标上，反映了一个假设，即通道关系可以被表述为实例-不可知函数与局部感受野的组合。相反，我们认为，为单元提供一种机制来明确地建模通道之间的动态、非线性依赖关系，利用全局信息可以简化学习过程，并显著增强网络的表示能力。

**注意力机制和门控机制** 广义上讲，注意力可以被视为一种工具，用于偏向将可用的处理资源分配给输入信号中最具信息量的组件 [17, 18, 22, 29, 32]。这种机制的好处已经在多个任务中得到验证，从图像的定位和理解 [3, 19] 到基于序列的模型 [2, 28]。通常，它与门控函数（例如softmax或sigmoid）和顺序技术结合使用 [12, 41]。最近的研究表明，它适用于诸如图像字幕生成 [4, 48] 和唇读 [7] 等任务。在这些应用中，它经常用于一个或多个代表高级抽象的层之上，以实现不同模态之间的适应性。Wang等人[46]介绍了一种强大的主干-掩膜注意力机制，采用了一个沙漏模块[31]。这个高容量单元被插入到深度残差网络的中间阶段。相比之下，我们提出的SE模块是一种轻量级的门控机制，专门用于以高效的方式建模通道之间的关系，并设计用于增强整个网络中基本模块的表征能力。

<a id="3.Squeeze-and-ExcitationBlocks"></a>
## 3. Squeeze-and-Excitation Blocks


<a id="4.模型和计算复杂度ModelandComputationalComplexity"></a>
## 4.模型和计算复杂度 Model and Computational Complexity
为了使提出的SE模块在实践中可行，它必须提供一种在模型复杂度和性能之间有效的平衡，这对于可扩展性非常重要。我们在所有实验中将缩减比例r设置为16，除非另有说明（更多讨论可以在第6.4节中找到）。为了说明该模块的成本，我们以ResNet-50和SE-ResNet-50之间的比较为例，其中SE-ResNet-50的准确性优于ResNet-50，并接近于更深的ResNet101网络（如表2所示）。ResNet-50需要在单次前向传递中使用约3.86 GFLOPs处理224×224像素的输入图像。每个SE模块在压缩阶段利用全局平均池化操作，在激发阶段使用两个小的全连接层，然后进行廉价的通道缩放操作。总体而言，SE-ResNet-50需要约3.87 GFLOPs，相对于原始的ResNet-50增加了0.26%。

在实践中，对于一个训练mini-batch包含256张图像，在ResNet-50上进行一次正向和反向传递需要190毫秒，而在SE-ResNet-50上需要209毫秒（这两个时间都是在一个拥有8个NVIDIA Titan X GPU的服务器上进行的）。我们认为，这代表了一个合理的开销，特别是由于现有的GPU库中全局池化和小内积运算没有被优化。此外，由于它对嵌入式设备应用的重要性，我们还对每个模型进行了CPU推理时间基准测试：对于一个224×224像素的输入图像，ResNet-50需要164毫秒，而SE-ResNet-50需要167毫秒。 SE模块所需的额外计算开销很小，但由于其对模型性能的贡献，这是合理的。

接下来，我们考虑所提出的模块引入的额外参数。它们全部包含在门控机制的两个全连接层中，构成了总网络容量的一小部分。更准确地说，引入的额外参数数量为：  

```math

\frac{2}{r} \sum_{s=1}^{S}  N_s \cdot C_s^2

```

其中，r表示缩减比例，S表示阶段数（每个阶段指的是在具有相同空间维度的特征图上操作的一组块），Cs表示输出通道的维度，Ns表示阶段s中重复的块数。相对于ResNet-50所需的约2500万参数，SE-ResNet-50引入了大约250万个额外参数，增加了约10%。其中，大部分参数来自网络的最后一个阶段，在这个阶段中激发操作涉及到最大的通道维度。然而，我们发现，可以以较小的性能成本（在ImageNet上<0.1%的top-1错误）移除SE模块的最后阶段，将相对参数增加降低到约4%，这在参数使用是一个关键考虑因素的情况下可能会很有用（请参阅第6.4节中的进一步讨论）。

<a id="5.实施方案Implementation"></a>
## 5.实施方案 Implementation
每个普通网络及其对应的SE版本在相同的优化方案下进行训练。在ImageNet上的训练过程中，我们采用了标准做法，对图像进行随机尺寸裁剪[43]至224×224像素（Inception-ResNet-v2和SE-Inception-ResNet-v2为299×299像素），并进行随机水平翻转。输入图像通过通道均值减法进行归一化处理。此外，我们采用了[36]中描述的数据平衡策略进行小批量采样。网络在我们的分布式学习系统"ROCS"上进行训练，该系统设计用于高效并行训练大型网络。优化过程使用带有动量0.9的同步SGD，并使用mini-batch大小为1024。初始学习率设置为0.6，每30个epoch减小10倍。所有模型从头开始训练100个epoch，使用[9]中描述的权重初始化策略。

在测试时，我们对验证集应用中心裁剪评估，从每个图像中裁剪出224×224像素，其中较短的边先调整为256（对于Inception-ResNet-v2和SE-Inception-ResNet-v2调整为352）。

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion










