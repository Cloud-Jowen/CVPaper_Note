# End-to-End Object Detection with Transformers

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
&emsp;[**2.1 集合预测**](#2.1集合预测)  
&emsp;[**2.2 Transformer和并行解码**](#2.2Transformer和并行解码)  
&emsp;[**2.3 目标检测**](#2.3目标检测)  
[**3.DETR**](#3.DETR)  
[**4.实验 Experiments**](#4.实验Experiments)  
&emsp;[**4.1 和Faster R-CNN比较**](#4.1和FasterR-CNN比较)  
&emsp;[**4.2 消融实验**](#4.2消融实验)   
&emsp;[**4.3 分析**](#4.3分析)   
&emsp;[**4.4 用于全景分割的DETR**](#4.4用于全景分割的DETR)   
[**5.结论 Conclusion**](#5.结论Conclusion)   



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  https://github.com/facebookresearch/detr 
论文链接：  https://arxiv.org/abs/2005.12872

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
我们提出了一种将目标检测视为直接集合预测问题的新方法。我们的方法简化了检测流程，有效地消除了许多需要手动设计的组件，如非最大抑制程序或锚点生成，这些组件明确地编码了我们对任务的先验知识。新框架的主要组成部分被称为DEtection TRansformer（DETR），其中包括一个基于集合的全局损失，通过二部匹配强制产生唯一的预测，以及一个Transformer编码器-解码器架构。给定一组学习到的固定小目标查询，DETR推理关于对象之间的关系和全局图像上下文，直接并行输出最终的预测集。这个新模型在概念上是简单的，不像许多其他现代检测器一样需要专门的库。DETR在具有挑战性的COCO目标检测数据集上展现了与经过充分优化的Faster RCNN基线相当的准确性和运行时性能。此外，DETR可以轻松推广到以统一方式生成全景分割。我们展示了它在很大程度上胜过竞争基线。训练代码和预训练模型网址：https://github.com/facebookresearch/detr 

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
目标检测的目标是为感兴趣的每个对象预测一组边界框和类别标签。现代检测器通过在大量提议[37,5]、锚点[23]或窗口中心[53,46]上定义替代回归和分类问题来间接解决这个集合预测任务。它们的性能受到后处理步骤的显著影响，以折叠近似重复的预测，以及受锚点集设计和将目标框分配给锚点的启发式方法的影响[52]。为了简化这些流程，我们提出了一种直接集合预测方法，以绕过替代任务。端到端的理念已经在诸如机器翻译或语音识别等复杂结构化预测任务中取得了显著进展，但在目标检测领域尚未得到应用：先前的尝试[43,16,4,39]要么增加了其他形式的先验知识，要么在具有挑战性的基准测试中没有证明与强基线竞争力。本文旨在弥合这一差距。

我们将目标检测视为直接集合预测问题，简化了训练流程。我们采用基于变压器的编码器-解码器架构[47]，这是一种用于序列预测的流行架构。变压器的自注意机制明确地模拟了序列中各个元素之间的所有成对交互作用，使得这些架构特别适用于集合预测的特定约束，比如消除重复预测。我们的DEtection TRansformer（DETR，见图1）一次性预测所有对象，并通过一个集合损失函数进行端到端训练，该损失函数在预测对象和地面真实对象之间执行二部匹配。DETR通过丢弃诸如空间锚点或非极大值抑制等多个手动设计的组件，简化了检测流程。与大多数现有的检测方法不同，DETR不需要任何定制层，因此可以在包含标准CNN和变压器类的任何框架中轻松复现。

我们通过将目标检测视为直接集合预测问题来简化训练流程。我们采用了基于Transformer的编码器-解码器架构，Transformer是一种用于序列预测的流行架构。Transformer的自注意机制可以显式地建模序列中各个元素之间的相互作用，使得这些架构特别适用于集合预测的特定约束，例如去除重复预测。

我们的DEtection TRansformer（DETR）一次性预测所有对象，并且使用一种集合损失函数进行端到端训练，该损失函数在预测对象和实际对象之间执行二部图匹配。DETR通过去除诸如空间锚点或非最大抑制等手动设计的先验知识组件简化了检测流程。与大多数现有的检测方法不同，DETR不需要任何定制层，因此可以在包含标准CNN和Transformer类的任何框架中轻松复现。

与大多数关于直接集合预测的先前工作相比，DETR的主要特点是二部图匹配损失和（非自回归）并行解码的结合。相反，先前的工作主要集中在使用RNN进行自回归解码。我们的匹配损失函数能够唯一地将预测与实际对象进行匹配，并且对于预测对象的排列是不变的，因此我们可以并行地发射它们。

我们在最受欢迎的目标检测数据集之一COCO上对DETR进行了评估，并与竞争激烈的Faster R-CNN基线进行了比较。Faster R-CNN经历了许多设计迭代，并且其性能自原始论文以来有了很大提升。我们的实验证明，我们的新模型实现了可比较的性能。具体而言，DETR在大型物体上表现出显著更好的性能，这可能是由Transformer的非局部计算所实现的。然而，在小型物体上，它的性能较低。我们期望未来的工作能够像FPN对Faster R-CNN的发展那样改善这一方面。

DETR的训练设置在多个方面与标准目标检测器不同。这个新模型需要更长的训练计划，并且受益于Transformer中的辅助解码损失。我们彻底探索了哪些组件对所展示的性能至关重要。

DETR的设计理念很容易扩展到更复杂的任务。在我们的实验中，我们展示了在预训练的DETR模型之上训练的简单分割头部，在全景分割（Panoptic Segmentation）这一具有挑战性的像素级识别任务上优于竞争性的基准模型，这是最近变得流行的任务。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work
我们的工作在几个领域建立在先前的研究基础上：集合预测的二部图匹配损失、基于Transformer的编码器-解码器架构、并行解码以及目标检测方法。

<a id="2.1集合预测"></a>
### 2.1 集合预测 Set Prediction
目前没有经典的深度学习模型可以直接预测集合。基本的集合预测任务是多标签分类（例如，在计算机视觉领域的参考文献中可见[40,33]），其中基准方法“one-vs-rest”并不适用于检测等存在元素之间基础结构（即近似相同的框）的问题。这些任务的第一个困难在于避免近似重复。大多数当前的检测器使用诸如非极大值抑制之类的后处理方法来解决这个问题，但直接的集合预测则无需后处理。它们需要全局推理方案，以建模所有预测元素之间的交互以避免冗余。对于恒定大小的集合预测，密集全连接网络[9]是足够的但代价高昂。一个通用的方法是使用自回归序列模型，比如循环神经网络[48]。在所有情况下，损失函数应该对预测的排列具有不变性。通常的解决方案是设计一个基于匈牙利算法[20]的损失，以找到地面真相和预测之间的二部图匹配。这强制执行了排列不变性，并确保每个目标元素有一个唯一的匹配。我们采用了二部图匹配损失方法。然而，与大多数先前的工作形成对比的是，我们远离了自回归模型，而是使用了具有并行解码的Transformer模型，下面将对此进行描述。

<a id="2.2Transformer和并行解码"></a>
### 2.2 Transformer和并行解码 Transformers and Parallel Decoding
Transformer是由Vaswani等人[47]引入的一种基于注意力机制的新型构建模块，用于机器翻译。注意力机制[2]是神经网络层，它可以聚合来自整个输入序列的信息。Transformer引入了自注意力层，类似于非局部神经网络[49]，它扫描序列的每个元素，并通过聚合整个序列的信息来更新它。基于注意力的模型的一个主要优势是其全局计算和完美的记忆，这使得它们比RNN在处理长序列时更加适用。Transformer目前正在取代RNN在自然语言处理、语音处理和计算机视觉等许多问题上的应用[8,27,45,34,31]。

Transformer最初被用于自回归模型，遵循早期的序列到序列模型[44]，逐个生成输出标记。然而，由于推理成本过高（与输出长度成正比，且难以进行批处理），这导致了并行序列生成的发展，在音频[29]、机器翻译[12,10]、词表示学习[8]以及最近的语音识别[6]领域得到了应用。我们还结合了Transformer和并行解码，因为它们在计算成本和执行集合预测所需的全局计算能力之间具有适当的平衡。

<a id="2.3目标检测"></a>
### 2.3 目标检测 Object detection
大多数现代目标检测方法都是相对于一些初始猜测进行预测的。两阶段检测器[37,5]根据提议来预测边界框，而单阶段方法则根据锚点[23]或可能的目标中心网格[53,46]进行预测。最近的工作[52]表明，这些系统的最终性能在很大程度上取决于设置初始猜测的精确方式。在我们的模型中，我们能够消除这个手工制作的过程，并通过直接预测相对于输入图像而不是锚点的绝对边界框来简化检测过程。

**基于集合的损失** 一些目标检测器[9,25,35]使用了二分图匹配损失。然而，在这些早期的深度学习模型中，不同预测之间的关系仅通过卷积或全连接层建模，并且手工设计的NMS后处理可以提高它们的性能。更近期的检测器[37,23,53]使用了地面真值和预测之间的非唯一分配规则以及NMS。

可学习的NMS方法[16,4]和关系网络[17]明确地使用注意力来建模不同预测之间的关系。使用直接的集合损失，它们不需要任何后处理步骤。然而，这些方法使用额外的手工制作的上下文特征，如提议框坐标，以有效地建模检测之间的关系，而我们寻求减少模型中编码的先验知识的解决方案。

**循环检测器** 与我们的方法最相似的是用于目标检测[43]和实例分割[41,30,36,42]的端到端集合预测。与我们类似，它们使用基于CNN激活的编码器-解码器架构和二分图匹配损失来直接生成一组边界框。然而，这些方法仅在小型数据集上进行了评估，并没有与现代基准进行对比。特别地，它们基于自回归模型（更准确地说是RNN），因此无法利用最近的并行解码的Transformer模型。

<a id="3.DETR"></a>
## 3.DETR
目标检测中直接集合预测的两个关键要素是：（1）一种集合预测损失，强制预测的边界框与地面真值边界框之间进行唯一匹配；（2）一种能够在单次传递中预测一组对象并对它们的关系进行建模的架构。我们将在图2中详细描述我们的架构。

<a id="3.1目标检测集合预测损失"></a>
### 3.1 目标检测集合预测损失 Object detection set prediction loss
DETR在解码器中一次推断出一个固定大小的 N 个预测，其中 N 被设置为明显大于图像中典型物体数量的值。训练的主要困难之一是对预测的物体（类别、位置、大小）进行得分，以及与地面真值进行匹配。我们的损失函数能够在预测的物体和地面真值之间产生最优的二分图匹配，并优化特定对象（边界框）的损失。

<a id="4.实验Experiments"></a>
## 4.实验 Experiments
我们展示了DETR在COCO数据集的定量评估中与Faster R-CNN相比取得了竞争性的结果。然后，我们对架构和损失进行了详细的剖析研究，提供了深刻的见解和定性结果。最后，为了表明DETR是一个多才多艺且可扩展的模型，我们展示了全景分割的结果，只需在固定的DETR模型上进行一小部分扩展训练。我们提供了代码和预训练模型，以便在 https://github.com/facebookresearch/detr 上重现我们的实验。

**数据集** 我们在COCO 2017数据集上进行实验，该数据集包含118,000张训练图像和5,000张验证图像，用于目标检测和全景分割的注释。每个图像都用边界框和全景分割进行了标注。每个图像平均有7个实例，在训练集中单个图像最多可达63个实例，这些实例在同一图像上大小不等。若未特别说明，我们报告的AP指的是边界框AP，即在多个阈值上的综合度量。与Faster R-CNN进行比较时，我们报告了最后一个训练周期的验证AP，对于剖析研究，我们报告了最后10个周期的验证结果的中位数。

**技术细节** 我们使用AdamW进行DETR的训练[26]，将初始变换器的学习率设置为10^(-4)，主干网络的学习率设置为10^(-5)，权重衰减设置为10^(-4)。所有变换器的权重都使用Xavier初始化[11]，而主干网络则使用来自torchvision的ImageNet预训练的ResNet模型[15]，其中的批归一化层被冻结。我们报告了两种不同的主干网络：ResNet50和ResNet-101。相应的模型分别称为DETR和DETR-R101。根据[21]的方法，我们还通过在主干网络的最后一个阶段添加扩张(dilation)并从该阶段的第一个卷积中移除一个步长来增加特征分辨率。相应的模型分别称为DETR-DC5和DETR-DC5-R101（扩张的C5阶段）。这种修改使分辨率提高了两倍，因此改善了对小物体的性能，但编码器的自注意力计算成本增加了16倍，导致整体计算成本增加了2倍。表1中给出了这些模型和Faster R-CNN的FLOP（浮点操作数）的全面比较。

我们使用尺度增强(scale augmentation)，调整输入图像的大小，使最短边至少为480像素，最长边不超过1333像素[50]。为了通过编码器的自注意力机制学习全局关系，我们在训练过程中还应用了随机裁剪增强，将性能提升了约1个AP值。具体而言，在训练时，图像有50%的概率被裁剪成一个随机的矩形区域，然后再次调整大小为800-1333像素。变换器使用默认的0.1丢弃率进行训练。在推理阶段，一些槽位可能会预测为空类别。为了优化AP，我们将这些槽位的预测覆盖为得分第二高的类别，并使用相应的置信度。这相比于过滤掉空槽位可以提高2个AP点。其他训练超参数可以在A.4节中找到。对于我们的消融实验，我们使用了300个epoch的训练计划，其中在200个epoch之后学习率下降10倍，一个epoch表示对所有训练图像进行一次遍历。在16个V100 GPU上训练基准模型需要3天时间，每个GPU处理4张图片（因此总批量大小为64）。对于与Faster R-CNN进行比较的更长训练计划，我们训练500个epoch，并在400个epoch之后降低学习率。与较短的训练计划相比，这个训练计划可以增加1.5个AP值。

<a id="4.1和FasterR-CNN比较"></a>
## 4.1 和Faster R-CNN比较 Comparison with Faster R-CNN
Transformer通常使用Adam或Adagrad优化器进行训练，采用非常长的训练时间表和丢弃(dropout)，DETR也是如此。然而，Faster R-CNN则使用SGD进行训练，数据增强很少，我们不清楚是否有成功应用Adam或dropout的案例。尽管存在这些差异，我们试图加强Faster R-CNN基线。为了与DETR保持一致，我们将广义IoU[38]添加到框损失中，并采用相同的随机裁剪增强和已知可以改善结果的长时间训练[13]。结果如表1所示。在顶部部分，我们展示了使用3倍训练计划在Detectron2 Model Zoo [50]中训练的Faster R-CNN模型的结果。在中间部分，我们展示了相同模型使用9倍训练计划（109个epoch）和上述增强训练的结果（标有“+”），总体上提高了1-2个AP。在表1的最后部分，我们展示了多个DETR模型的结果。为了在参数数量上可比较，我们选择了一个具有6个transformer和6个decoder层，宽度为256，带有8个注意力头的模型。像具有FPN的Faster R-CNN一样，该模型共有41.3M参数，其中23.5M位于ResNet-50中，17.8M位于transformer中。尽管Faster R-CNN和DETR都可能通过更长时间的训练进一步改善，但我们可以得出结论，DETR在具有相同参数数量的情况下可以与Faster R-CNN相竞争，在COCO验证子集上实现了42的AP。DETR实现这一点的方式是通过提高APL（+7.8），但请注意，模型在APS方面仍然落后（-5.5）。具有相同参数数量和类似FLOP计数的DETR-DC5具有更高的AP，但在APS方面仍然明显落后。具有ResNet-101骨干的Faster R-CNN和DETR显示出可比较的结果。

<a id="4.2消融实验"></a>
## 4.2 消融实验 Ablations
变压器解码器中的注意力机制是模型不同检测特征表示之间关系的关键组成部分。在我们的消融分析中，我们探索了我们架构和损失的其他组成部分如何影响最终性能。为了进行研究，我们选择了基于ResNet-50的DETR模型，具有6个编码器层和6个解码器层，宽度为256。该模型有4130万个参数，在短期和长期时间表上分别达到40.6和42.0 AP，并以28 FPS的速度运行，与具有相同骨干网络的Faster R-CNN-FPN类似。

**编码器层的数量** 我们通过改变编码器层的数量来评估全局图像级自注意力的重要性（见表2）。没有编码器层，整体AP下降了3.9个百分点，在大型物体上下降了6.0个百分点。我们推测，通过使用全局场景推理，编码器对于解开物体之间的联系非常重要。在图3中，我们可视化了经过训练模型的最后一个编码器层的注意力图，着重关注图像中的一些点。编码器似乎已经对实例进行了分离，这可能简化了解码器对对象的提取和定位。

**解码器层的数量** 我们在每个解码层之后应用辅助损失（见第3.2节），因此，预测FFN被设计训练以从每个解码器层的输出中预测对象。我们通过评估在解码的每个阶段将会预测出的对象来分析每个解码器层的重要性（见图4）。在每一层之后，AP和AP50都有所提高，总共在第一层和最后一层之间有非常显著的+8.2/9.5 AP改善。由于DETR采用了基于集合的损失，因此在设计上不需要NMS。为了验证这一点，我们针对每个解码器的输出运行了具有默认参数的标准NMS过程[50]。NMS提高了对第一个解码器的预测的性能。这可以解释为变压器的单个解码层无法计算输出元素之间的任何交叉相关性，因此很容易对同一对象进行多次预测。在第二层和随后的层中，激活上的自注意机制使模型能够抑制重复的预测。我们观察到，随着深度的增加，NMS带来的改进逐渐减小。在最后几层，我们观察到AP略微下降，因为NMS错误地移除了真正的正面预测。

类似于可视化编码器注意力，我们在图6中可视化了解码器的注意力，使用不同颜色为每个预测的对象着色注意力图。我们观察到解码器的注意力是相当局部化的，这意味着它主要关注对象的极端部分，如头部或腿部。我们假设在编码器通过全局注意力分离实例之后，解码器只需要关注极端部分以提取类别和对象边界。

**FFN的重要性** 变压器中的FFN可以看作是1×1卷积层，使得编码器类似于增强注意力的卷积网络[3]。我们尝试完全移除它，只保留变压器层中的注意力。通过将网络参数数量从41.3M减少到28.7M，只在变压器中保留10.8M，性能下降了2.3 AP，因此我们得出结论，FFN对于取得良好结果是重要的。

**位置编码的重要性** 在我们的模型中有两种类型的位置编码：空间位置编码和输出位置编码（对象查询）。我们尝试了各种固定和可学习编码的组合，结果可以在表3中找到。输出位置编码是必需的，不能被移除，因此我们尝试将其一次传递给解码器输入，或者在每个解码器注意力层中添加到查询中。在第一个实验中，我们完全移除了空间位置编码，并在输入时传递输出位置编码，有趣的是，模型仍然实现了超过32的AP，比基准下降了7.8的AP。然后，我们像原始变压器[47]一样，一次性传递固定的正弦空间位置编码和输出位置编码到输入，发现与直接在注意力中传递位置编码相比，这导致了1.4的AP下降。传递给注意力的可学习空间编码产生了类似的结果。令人惊讶的是，我们发现在编码器中不传递任何空间编码只导致轻微的1.3 AP下降。当我们将编码传递给注意力时，它们在所有层之间共享，并且输出编码（对象查询）始终是可学习的。

根据这些消融实验，我们得出结论：变压器组件，包括编码器中的全局自注意力、FFN、多个解码器层以及位置编码，都显著地对最终的目标检测性能产生影响。

**损失消融** 为了评估匹配成本和损失的不同组件的重要性，我们训练了几个模型并将它们打开和关闭。损失有三个组件：分类损失、边界框距离损失和GIoU [38] 损失。分类损失对训练至关重要，不能被关闭，因此我们训练了一个没有边界框距离损失的模型，以及一个没有GIoU损失的模型，并与基准模型进行比较，基准模型使用了这三种损失进行训练。结果见表4。单独使用GIoU损失进行简单的损失消融（每次使用相同的权重），但其他组合方式可能会得到不同的结果。


<a id="4.3分析"></a>
## 4.3 分析 Analysis
**解码器输出槽分析** 在图7中，我们可视化了DETR对COCO 2017验证集中所有图像预测的不同槽位的边界框。DETR为每个查询槽学习了不同的专业知识。我们观察到，每个槽位有几种操作模式，着重于不同的区域和框大小。特别是，所有槽位都具有预测整个图像边界框的模式（以红色点的形式显示在图的中间）。我们假设这与COCO数据集中对象的分布有关。


**对未见实例数量的泛化** COCO中的一些类别在同一图像中的实例数量不多。例如，在训练集中没有一张图像中有超过13只长颈鹿的情况。我们创建了一个合成图像3来验证DETR的泛化能力（见图5）。我们的模型能够在图像上找到所有24只长颈鹿，这显然超出了分布范围。这个实验证实了每个对象查询中没有强烈的类别特异性。

<a id="4.4用于全景分割的DETR"></a>
### 4.4 用于全景分割的DETR DETR for panoptic segmentation
全景分割[19]最近引起了计算机视觉界的广泛关注。类似于将Faster R-CNN [37]扩展为Mask R-CNN [14]，DETR可以通过在解码器输出之上添加一个掩模头来自然地进行扩展。在本节中，我们证明这样的头部可以用于通过统一的方式处理物体和背景类别来生成全景分割[19]。我们在COCO数据集的全景注释上进行实验，该数据集除了有80个物体类别外，还有53个背景类别。

我们训练DETR来预测COCO数据集中的物体和背景类别周围的边界框，使用相同的方法。预测边界框是必需的，因为匈牙利匹配是使用边界框之间的距离计算的。我们还添加了一个掩模头，用于为每个预测的边界框预测一个二进制掩模，参见图8。它以变压器解码器的输出作为输入，为每个对象计算多头（具有M个头）的注意力分数，将该嵌入层上的注意力热图生成M个小分辨率的对象。为了进行最终预测并增加分辨率，采用类似FPN的结构。我们在补充中详细描述了该架构。掩模的最终分辨率为4倍，并且每个掩模都使用DICE/F-1损失[28]和Focal损失[23]进行独立监督。

掩模头可以通过联合训练，或者分为两个步骤来训练。在分为两个步骤的方法中，我们首先仅对DETR进行边界框的训练，然后冻结所有权重，仅对掩模头进行为期25个时期的训练。实验证明，这两种方法给出了类似的结果，我们选择使用后一种方法进行报告，因为这样可以缩短总的训练时间。

为了预测最终的全景分割，我们只需要在每个像素点上对掩模分数进行argmax操作，并将相应的类别分配给生成的掩模。这个过程确保最终的掩模没有重叠，因此DETR不需要使用常用的启发式方法[19]来对齐不同的掩模。

**训练细节** 我们按照用于边界框检测的方法，训练DETR、DETR-DC5和DETR-R101模型，来预测COCO数据集中物体和背景类别周围的边界框。新的掩模头进行了为期25个时期的训练（详见补充说明）。在推理过程中，我们首先过滤掉置信度低于85%的检测结果，然后计算每个像素点的argmax，确定每个像素点属于哪个掩模。然后，我们将相同物体类别的不同掩模预测合并为一个，并过滤掉空的掩模（小于4个像素）。

**主要结果** 定性结果如图9所示。在表5中，我们将我们的统一全景分割方法与几种将物体和背景区别对待的已有方法进行了比较。我们报告了全景质量(Panoptic Quality，PQ)以及物体(PQth)和背景(PQst)的详细情况。我们还报告了在进行任何全景后处理之前(在我们的情况下，是在像素级别进行argmax操作之前)计算的掩码AP(基于物体类别)。我们展示了DETR在COCO-val 2017上的表现优于已发表的结果，以及我们强大的PanopticFPN基线(使用与DETR相同的数据增强进行训练，以便进行公平比较)。结果详细情况显示，DETR在背景类别上特别占主导地位，我们假设编码器注意力允许的全局推理是这一结果的关键因素。对于物体类别，尽管在掩码AP计算中与基线相比存在高达8 mAP的严重差异，DETR获得了竞争性的PQth。我们还在COCO数据集的测试集上评估了我们的方法，并获得了46 PQ。我们希望我们的方法能够激发未来在全景分割领域探索完全统一模型的研究。

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion
我们提出了DETR，一种基于Transformer和二分图匹配损失的目标检测系统的新设计，用于直接集合预测。该方法在具有挑战性的COCO数据集上实现了与优化的Faster R-CNN基准模型相当的结果。DETR易于实现，并且具有灵活的架构，可以轻松扩展到全景分割，并取得了竞争性的结果。此外，DETR在大型物体上的性能明显优于Faster R-CNN，这可能归功于自注意力对全局信息的处理。

这种新的检测器设计也带来了新的挑战，特别是在训练、优化和小物体性能方面。目前的检测器需要经过数年的改进来解决类似问题，我们期待未来的工作能够成功地解决DETR的这些问题。










