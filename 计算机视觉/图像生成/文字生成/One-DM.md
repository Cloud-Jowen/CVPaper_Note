# One-DM: One-Shot Diffusion Mimicker for Handwritten Text Generation

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
&emsp;[**3.1**](#3.1)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  https://github.com/dailenson/One-DM  
论文链接：  https://arxiv.org/pdf/2409.04004

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
现有的手写文本生成方法通常需要超过十份的手写字体作为风格参考，但在实际应用中，用户更倾向于使用单个参考样本的手写生成模型，以提高便利性和效率。这种方法被称为“一次生成”，显著简化了过程，但因其从单个样本准确捕捉书写者风格的难度较大，尤其是当提取字符边缘的精细细节时，存在一定的挑战性，尤其是在稀疏前景和不希望的背景噪声之间。为了应对这一问题，我们提出了一种名为“One-shot Diffusion Mimicker”（One-DM）的方法来生成仅需一个参考样本即可模仿任何书法风格的手写文本。受事实启发，即个体样本中的高频信息往往包含独特的风格模式（例如，字符倾斜和字母连接），我们开发了一个新颖的风格增强模块，通过将单个样本中的高频成分纳入其中来改进风格提取。然后，我们将风格特征与文本内容融合为合并条件，引导扩散模型生成高质量的手写文本图像。广泛的实验表明，我们的方法可以在多个语言中成功地使用单个样本进行手写文本生成，并且在使用超过十个样本的情况下优于先前的方法。我们的源代码可在https://github.com/dailenson/One-DM 上获得。  

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
在数字时代，手写文本生成将传统的个性化书写与自动过程的效率相结合，提供了一个数字化格式来保存个人手写的原汁原味。这项任务旨在自动产生不仅对应特定文本内容而且模仿给定范例作者书写的风格的手写文本图像（例如，字符倾斜、草书连接、笔画厚度和墨水颜色）这为手部有缺陷的人提供了极大的便利，也促进了手写体设计过程的加速。  

在过去的文献中，已经有一些关于手写文本生成的研究。其中一些研究使用了对抗生成网络（GAN）来实现这一目标。例如，ScrabbleGAN[13]利用随机噪声作为样式输入，并基于字符级别的标签对内容输入进行条件设置，从而实现了具有随机采样样式的手写单词合成。最近的一项研究表明，去噪扩散概率模型（DDPM）等扩散模型[37, 50, 66]甚至可以比GAN产生更高质量的图像生成结果[11]。这激发了一些尝试，如WordStylist[44]、GC-DDPM[12]和CTIG-DM[70]，它们试图将去噪过程与固定的作者ID联系起来，以生成具有可控风格的手写文本图像。然而，这些方法的主要限制在于它们无法模仿未见过的作者的手写风格[12,13,44,70]。

要模仿任何给定的书写风格，一些以前的方法（例如 [5、27 和 46]）需要用户提供几个样本作为样式参考。它们使用一个样式编码器从给定的样本中提取书写风格，从而为生成样本提供了灵活的控制。然而，传统的少量样本生成流程不方便、效率低且耗时，如图 1 所示。因此用户更倾向于仅需单个样本作为样式参考的一次性生成方法。我们的目标是研究具有重要实际价值的更具挑战性的一次性生成任务，并努力实现高质量的手写文本图像生成，以满足所需的内容和风格。 

![image](https://github.com/user-attachments/assets/1f9eeb13-2fb4-4e5b-9219-7a465f07027b)  
(图1：单次和少量手写文本生成方法之间的用户体验比较。它揭示了单次设置导致更好的用户体验。)

单次生成任务的主要挑战在于准确地从仅有的一个风格参考图像中提取用户的书写风格。如图2所示，字符作为抽象符号通常只占参考图像的很小一部分。此外，参考图像往往包含嘈杂的背景信息，这在提取个人书写风格时是一个很大的障碍。先前的一次性生成方法[9,14,15]简单地遵循少量样本方法的架构设计，使用通用CNN编码器直接从单个样本中提取书写风格。然后将提取到的风格与文本内容结合并输入到CNN解码器中以生成所需的书写图像。这些方法由于其较差的样式提取能力，在模仿手写风格方面表现出有限的效果。

![image](https://github.com/user-attachments/assets/3ba38344-aa56-40f1-a39e-4cf47aa2bf27)  
(图2：手写文本样本及其对应的高频成分。我们发现，高频成分具有更明显的字符轮廓，并且清晰地展示了诸如字符倾斜和连笔连接等风格模式。)
(高频成分是怎么搞的)

为了应对上述挑战，我们的主要想法是利用样本的高频信息来增强手写风格的提取。如图2所示，高频信息涵盖了手写字体的整体轮廓，允许对诸如文本倾斜、字母间距和连笔等关键风格模式进行清晰观察。因此，将高频信息纳入有助于更有效地提取手写风格。

基于上述见解，我们提出了一种用于手写文本生成的单次扩散模仿器（One-DM），该模仿器同时受期望风格和任意内容的引导。具体而言，首先开发了一个增强风格模块来并行处理参考图像及其高频成分。考虑到参考图像通常包含背景噪声，我们设计了一个门机制以抑制背景噪声的流入。对于高频成分，例如显示更清晰风格模式的字符倾斜和连笔，我们采用对比学习框架 [28、36、67、68] 进一步获取具有区分性的风格特征，指导使用真实且多样化的风格进行手写文本合成。随后，从两个分支中提取出的风格特征与特定的内容原型在风格内容融合模块中适当地融合。最后，无缝集成的风格和内容特征作为条件输入，引导渐进式合成带有风格的手写文本图像的过程。值得注意的是，我们的 One-DM 只通过我们的风格增强模块有效地模仿了用户的手写风格，超越了少量样本方法，在产生高质量的风格化手写文本方面取得了更好的效果。

综上所述，我们的贡献如下：

-我们提出了一种用于生成样式化手写文本的新型扩散模型，该模型仅需要一个参考样本作为风格输入，并模仿其书写风格来生成任意内容的手写文本。  
-我们引入了参考样本的高频成分，以增强对手写字体风格的提取。所提出的风格增强模块可以有效地捕捉写作风格模式并抑制背景噪声的干扰。  
-在英语、中文和日语的手写数据集上进行了广泛的实验，证明我们的方法即使只使用一个风格参考也优于以前的方法，这些方法的参考数量是我们的15倍。  

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

**手写文本生成** 手写文本通常以两种格式存储：在线轨迹形式或离线图像形式。在线手写生成方法[2，8，31，55，57，69]经常使用循环神经网络（RNN）[2，7，31，55-57，69]、变换器解码器[8]或扩散模型[40，48]来逐步生成书写轨迹。与在线方法不同，离线生成方法的优势在于不需要额外的轨迹监督信息，并且可以生成具有笔画宽度和墨水颜色的真实手写文本，这是在线方法无法实现的。

在深度学习中，先前的离线手写生成方法 [3、5、9、13、22、27、41、46] 主要依赖于生成对抗网络 (GAN)。早期工作 [3、13] 将词向量嵌入 [3] 或拼接字母令牌 [13] 条件化为生成过程以合成手写单词图像。然而，这些方法难以灵活控制书写风格。因此，提出了基于 15 种风格参考的少量样本方法 [5、27、46]。例如，GANwriting [27] 使用卷积神经网络编码器 [18、62、63] 从几个样本中提取用户的书写风格，并将其与特定文本内容相结合以在所需风格下生成手写体。在其后续工作中 [26] ，合成样本被证明有助于训练更稳健的手写文本识别器。此外，HWT [5] 使用变压器编码器 [32、58] 从参考样本中提取丰富的风格模式，从而增强风格模仿性能。最近，VATr [46] 使用符号图像作为内容表示，使超出字符集的字符生成成为可能。

同时，提出了一种单样本生成方法[9、14、15]。尽管这些技术仅使用一个样本就可以模仿手写风格，但在样式化生成结果方面仍落后于少量样本的方法。此外，以前的手写生成方法[3、5、9、13-15、27]通常依赖复杂的语义表示，例如循环嵌入[3]和字母级令牌[5、9、13-15、27]。SLOGAN [41]从易于获得的印刷图像中提取文本内容。然而，由于其固定的作者ID，它在泛化到未见过的书写风格时面临着挑战。同样，一些基于扩散的方法[12、44、70]将样式ID作为去噪过程的条件，并且无法模拟它们之前没有遇到过的样式。相比之下，我们的One-DM可以从一种样式的样本中有效地获取样式信息，因此可以生成任意样式的手写体。由于空间限制，我们在附录A中讨论了更多相关的工作，包括通用图像生成的扩散方法、对比式方法[59]以及频率式方法[35、45]。

<a id="3.网络结构"></a>
## 3.网络结构

<a id="3.1"></a>
### 3.1


<a id="4.实验Experiments"></a>
## 4.实验 Experiments

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion










