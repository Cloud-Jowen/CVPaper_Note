# One-DM: One-Shot Diffusion Mimicker for Handwritten Text Generation

[**相关链接**](#相关链接)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.方法 Method**](#3.方法Method)  
&emsp;[**3.1 总体方案 Overall Scheme**](#3.1总体方案OverallScheme)   
&emsp;[**3.2 风格增强模块 Style-enhanced Module**](#3.2风格增强模块Style-enhancedModule)   
&emsp;[**3.3 风格内容融合模块 Style-content Fusion Module**](#3.3风格内容融合模块Style-contentFusionModule)  
[**4.实验 Experiments**](#4.实验Experiments)  
&emsp;[**4.1 实验设置 Experimental Settings**](#4.1实验设置ExperimentalSettings)  
&emsp;[**4.2 主要结论 Main Results**](#4.2主要结论MainResults)  
&emsp;[**4.3 分析 Analysis**](#4.3分析Analysis)  
&emsp;[**4.4 与SOTA工业方法的比较 Comparisons with SOTA Industrial Methods**](#4.4与SOTA工业方法的比较ComparisonswithSOTAIndustrialMethods)  
&emsp;[**在其他语言上的应用 Applications to Other Languages**](#4.5在其他语言上的应用ApplicationstoOtherLanguages)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 相关链接
参考博文链接：  
参考视频链接：  
源码链接：  https://github.com/dailenson/One-DM  
论文链接：  https://arxiv.org/pdf/2409.04004

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
现有的手写文本生成方法通常需要超过十份的手写字体作为风格参考，但在实际应用中，用户更倾向于使用单个参考样本的手写生成模型，以提高便利性和效率。这种方法被称为“一次生成”，显著简化了过程，但因其从单个样本准确捕捉书写者风格的难度较大，尤其是当提取字符边缘的精细细节时，存在一定的挑战性，尤其是在稀疏前景和不希望的背景噪声之间。为了应对这一问题，我们提出了一种名为“One-shot Diffusion Mimicker”（One-DM）的方法来生成仅需一个参考样本即可模仿任何书法风格的手写文本。受事实启发，即个体样本中的高频信息往往包含独特的风格模式（例如，字符倾斜和字母连接），我们开发了一个新颖的风格增强模块，通过将单个样本中的高频成分纳入其中来改进风格提取。然后，我们将风格特征与文本内容融合为合并条件，引导扩散模型生成高质量的手写文本图像。广泛的实验表明，我们的方法可以在多个语言中成功地使用单个样本进行手写文本生成，并且在使用超过十个样本的情况下优于先前的方法。我们的源代码可在https://github.com/dailenson/One-DM 上获得。  

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
在数字时代，手写文本生成将传统的个性化书写与自动过程的效率相结合，提供了一个数字化格式来保存个人手写的原汁原味。这项任务旨在自动产生不仅对应特定文本内容而且模仿给定范例作者书写的风格的手写文本图像（例如，字符倾斜、草书连接、笔画厚度和墨水颜色）这为手部有缺陷的人提供了极大的便利，也促进了手写体设计过程的加速。  

在过去的文献中，已经有一些关于手写文本生成的研究。其中一些研究使用了对抗生成网络（GAN）来实现这一目标。例如，ScrabbleGAN[13]利用随机噪声作为样式输入，并基于字符级别的标签对内容输入进行条件设置，从而实现了具有随机采样样式的手写单词合成。最近的一项研究表明，去噪扩散概率模型（DDPM）等扩散模型[37, 50, 66]甚至可以比GAN产生更高质量的图像生成结果[11]。这激发了一些尝试，如WordStylist[44]、GC-DDPM[12]和CTIG-DM[70]，它们试图将去噪过程与固定的作者ID联系起来，以生成具有可控风格的手写文本图像。然而，这些方法的主要限制在于它们无法模仿未见过的作者的手写风格[12,13,44,70]。

要模仿任何给定的书写风格，一些以前的方法（例如 [5、27 和 46]）需要用户提供几个样本作为样式参考。它们使用一个样式编码器从给定的样本中提取书写风格，从而为生成样本提供了灵活的控制。然而，传统的少量样本生成流程不方便、效率低且耗时，如图 1 所示。因此用户更倾向于仅需单个样本作为样式参考的一次性生成方法。我们的目标是研究具有重要实际价值的更具挑战性的一次性生成任务，并努力实现高质量的手写文本图像生成，以满足所需的内容和风格。 

![image](https://github.com/user-attachments/assets/1f9eeb13-2fb4-4e5b-9219-7a465f07027b)  
(图1：单次和少量手写文本生成方法之间的用户体验比较。它揭示了单次设置导致更好的用户体验。)

单次生成任务的主要挑战在于准确地从仅有的一个风格参考图像中提取用户的书写风格。如图2所示，字符作为抽象符号通常只占参考图像的很小一部分。此外，参考图像往往包含嘈杂的背景信息，这在提取个人书写风格时是一个很大的障碍。先前的一次性生成方法[9,14,15]简单地遵循少量样本方法的架构设计，使用通用CNN编码器直接从单个样本中提取书写风格。然后将提取到的风格与文本内容结合并输入到CNN解码器中以生成所需的书写图像。这些方法由于其较差的样式提取能力，在模仿手写风格方面表现出有限的效果。

![image](https://github.com/user-attachments/assets/3ba38344-aa56-40f1-a39e-4cf47aa2bf27)  
(图2：手写文本样本及其对应的高频成分。我们发现，高频成分具有更明显的字符轮廓，并且清晰地展示了诸如字符倾斜和连笔连接等风格模式。)
(高频成分是怎么搞的)

为了应对上述挑战，我们的主要想法是利用样本的高频信息来增强手写风格的提取。如图2所示，高频信息涵盖了手写字体的整体轮廓，允许对诸如文本倾斜、字母间距和连笔等关键风格模式进行清晰观察。因此，将高频信息纳入有助于更有效地提取手写风格。

基于上述见解，我们提出了一种用于手写文本生成的单次扩散模仿器（One-DM），该模仿器同时受期望风格和任意内容的引导。具体而言，首先开发了一个增强风格模块来并行处理参考图像及其高频成分。考虑到参考图像通常包含背景噪声，我们设计了一个门机制以抑制背景噪声的流入。对于高频成分，例如显示更清晰风格模式的字符倾斜和连笔，我们采用对比学习框架 [28、36、67、68] 进一步获取具有区分性的风格特征，指导使用真实且多样化的风格进行手写文本合成。随后，从两个分支中提取出的风格特征与特定的内容原型在风格内容融合模块中适当地融合。最后，无缝集成的风格和内容特征作为条件输入，引导渐进式合成带有风格的手写文本图像的过程。值得注意的是，我们的 One-DM 只通过我们的风格增强模块有效地模仿了用户的手写风格，超越了少量样本方法，在产生高质量的风格化手写文本方面取得了更好的效果。

综上所述，我们的贡献如下：

-我们提出了一种用于生成样式化手写文本的新型扩散模型，该模型仅需要一个参考样本作为风格输入，并模仿其书写风格来生成任意内容的手写文本。  
-我们引入了参考样本的高频成分，以增强对手写字体风格的提取。所提出的风格增强模块可以有效地捕捉写作风格模式并抑制背景噪声的干扰。  
-在英语、中文和日语的手写数据集上进行了广泛的实验，证明我们的方法即使只使用一个风格参考也优于以前的方法，这些方法的参考数量是我们的15倍。  

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work

**手写文本生成** 手写文本通常以两种格式存储：在线轨迹形式或离线图像形式。在线手写生成方法[2，8，31，55，57，69]经常使用循环神经网络（RNN）[2，7，31，55-57，69]、transformer解码器[8]或扩散模型[40，48]来逐步生成书写轨迹。与在线方法不同，离线生成方法的优势在于不需要额外的轨迹监督信息，并且可以生成具有笔画宽度和墨水颜色的真实手写文本，这是在线方法无法实现的。

在深度学习中，先前的离线手写生成方法 [3、5、9、13、22、27、41、46] 主要依赖于生成对抗网络 (GAN)。早期工作 [3、13] 将词向量嵌入 [3] 或拼接字母token [13] 条件化为生成过程以合成手写单词图像。然而，这些方法难以灵活控制书写风格。因此，提出了基于 15 种风格参考的少量样本方法 [5、27、46]。例如，GANwriting [27] 使用卷积神经网络编码器 [18、62、63] 从几个样本中提取用户的书写风格，并将其与特定文本内容相结合以在所需风格下生成手写体。在其后续工作中 [26] ，合成样本被证明有助于训练更稳健的手写文本识别器。此外，HWT [5] 使用transformer编码器 [32、58] 从参考样本中提取丰富的风格模式，从而增强风格模仿性能。最近，VATr [46] 使用符号图像作为内容表示，使超出字符集的字符生成成为可能。

近期，单样本生成方法涌现[9、14、15]。尽管这些技术仅使用一个样本就可以模仿手写风格，但在样式化生成结果方面仍落后于少量样本的方法。此外，以前的手写生成方法[3、5、9、13-15、27]通常依赖复杂的语义表示，例如循环嵌入[3]和字母级token[5、9、13-15、27]。SLOGAN [41]从易于获得的印刷图像中提取文本内容。然而，由于其固定的作者ID，它在泛化到未见过的书写风格时面临着挑战。同样，一些基于扩散的方法[12、44、70]将样式ID作为去噪过程的条件，并且无法模拟它们之前没有遇到过的样式。相比之下，我们的One-DM可以从一种样式的样本中有效地获取样式信息，因此可以生成任意样式的手写体。由于空间限制，我们在附录A中讨论了更多相关的工作，包括通用图像生成的扩散方法、对比式方法[59]以及频率式方法[35、45]。

<a id="3.方法Method"></a>
## 3.方法 Method
**问题陈述** 我们旨在合成由文本内容和书写风格控制的手写文本图像。给定任意字符串 A 和来自作者 w<sub>s</sub> 的单个样式样本 I<sub>s</sub>，生成的手写单词图像 X<sub>s</sub> 应复制 w<sub>s</sub> 的独特书写风格，并保持 A 的内容不变，其中文本内容 $A = {a_i}_{i=1}^L$ 覆盖长度 L ，a<sub>i</sub> 不受任何限制地从字符集中选择。 

为了应对这一任务，我们引入高频信息来增强书写风格的提取。现有的方法 [5、9、14、15、27、46] 常常使用简单的卷积神经网络或transformer编码器直接处理样式图像，往往导致不理想的风格提取。相比之下，我们通过创新Laplacian高频率提取和门控机制引入一种新颖的One-shot diffusion mimicker（One-DM）。我们的One-DM能够有效地从单个参考中捕获样式特征，并抑制背景噪声。 

<a id="3.1总体方案OverallScheme"></a>
### 3.1 总体方案 Overall Scheme
我们的高级想法集中在从风格参考图像中提取高频信息以增强风格模式的提取。一种直接的方法是使用一个普通的transformer编码器来从风格图像及其相应的高频图像中提取风格特征。这种简单的解决方案遇到了两个主要问题：（1）缺乏有效的监督目标仍然使准确学习原始图像中的作者风格模式具有挑战性，（2）从原始图像中捕获到的风格特征仍保留有噪声背景， 

为了解决上述问题，我们开发了一种更有效的方法，如图3所示。我们的方法包括一个风格增强模块、内容编码器、风格内容融合模块和条件扩散模块。首先，我们使用拉普拉斯核作为高频滤波器从风格参考中提取高频成分。然后，两个并行的风格编码器同时从风格参考及其高频信息中提取相应的风格特征。由于风格参考中经常存在不希望有的背景噪声，因此我们设计了一个门机制来促进有信息风格特征的传输，并抑制噪声的传输。在高频成分中的风格模式相对干净且更加明显，有助于观察个体风格，例如字符倾斜。对于观察，我们提出一种对比学习目标，称为 LaplacianNCE ，以强制执行从高频成分中进行更具区分性的风格学习。 

![image](https://github.com/user-attachments/assets/cfd0fe81-2f1c-4c7d-a30c-e74653d9667b)
图3：所提方法的概述。风格参考首先通过高通滤波器提取其高频成分。随后，空间和高频风格编码器分别从风格参考及其高频信息中独立提取风格特征 F<sub>spa</sub> 和 F<sub>fre</sub> 。经过门机制过滤后的 F<sub>spa</sub> 与F<sub>fre</sub> 和内容特征 E 在融合模块中进行融合，并作为条件输入引导扩散生成过程。

关于内容指导，我们把给定的字符串 A 转换为统一字体图像，就像在 VATr [46]中一样。简而言之，Unifont的关键优势是它涵盖了所有Unicode字符，允许我们的方法将任何用户输入转换成相应的图像。我们进一步将渲染结果喂入一个结合ResNet18和Transformer编码器的内容编码器。这个过程首先使用ResNet18并行处理每个字符图像，然后将它们连接起来形成单词序列特征。Transformer编码器随后处理这些特征以提取具有全局上下文的信息内容特征 $E = {e_i}_{i=1}^L \in R^{L * c}$ ，其中c是通道维度。获得风格特征和内容指导后，我们无缝融合了它们，使用一种风格-内容融合模块。用融合的结果来引导条件扩散模型的去噪过程来合成所需的书写文本图像。去噪过程由损失函数L_rec监督。 

总结一下，我们方法的整体训练目标结合了两个损失函数：
```math
L = L_{lapNCE} + L_{rec}
```

<a id="3.2风格增强模块Style-enhancedModule"></a>
### 3.2 风格增强模块 Style-enhanced Module
我们提出了一种风格增强模块，通过将参考图像I<sub>s</sub>中的高频成分 $H_s \in R^{h×w×c}$ （其中 $I_s \in R^{h×w×c}$）整合到风格提取中，以提高清晰的风格模式。如图2所示，在像字符倾斜和形状连接这样的高频内容中，更清晰的风格模式被呈现出来。如图3所示，我们使用拉普拉斯核作为高频滤波器来从 I<sub>s</sub> 中提取 H<sub>s</sub> 。拉普拉斯核在不需要快速傅里叶变换（FFT）和频率域参数分离的情况下，擅长提取高频信息。然后，两个风格编码器 E<sub>spa</sub> 和 E<sub>fre</sub> 分别处理 I<sub>s</sub> 和 H<sub>s</sub> 。这种独立处理导致了独特的风格特征：来自 E<sub>spa</sub> 编码器的 $`F_{spa} = \left \{  f_{spa}^i\right \}_{i=1}^d \in R^{d×c} `$ 和来自 E<sub>fre</sub> 编码器的和 $`F_{fre} = \left \{  f_{fre}^i\right \}_{i=1}^d \in R^{d×c} `$，其中 $`d=h×w`$ 。虽然结构相同，但 E<sub>spa</sub> 和 E<sub>fre</sub> 不共享权重。然后，提出的 L<sub>lapNCE</sub> 迫使 E<sub>fre</sub> 编码器专注于从 H<sub>s</sub> 中提取有鉴别力的风格特征。设计了一个门机制，以选择性地过滤出参考样式特征中的背景噪声，仅允许有意义的样式模式通过。 

**Laplacian对比学习** 本文提出的 L<sub>lapNCE</sub> 的目标是引导高频风格编码器 E<sub>fre</sub> 从高频信息中学习更具有区分性的风格特征，因此我们提出将提取的属于同一作者的风格特征 F<sub>fre</sub> 拉近，而将来自不同作者的风格特征拉开距离。我们将我们的 L<sub>lapNCE</sub> 表示如下：
```math
L_{lapNCE} = - \frac{1}{N}\sum_{i \in M}^{}\frac{1}{\left |  P(i)\right | }\sum_{p \in P(i)}^{} log \frac{exp\left ( z_i · z_p  / \tau \right ) }{ {\textstyle \sum_{a \in A(i)}^{}exp\left ( z_i · z_p  / \tau \right )} }  
```
具体来说，$`i \in M = \left \{ 1...N \right \} `$ 是mini-batch中大小为 N 的任何元素的索引，$`A(i) = M `$ \ $`\left \{ i \right \}`$是与 i 不同的其他索引。Z<sub>i</sub> 是一个属于作者 w<sub>i</sub> 的锚样本，并且$`P\left ( i \right ) = \left \{ p \in A(i):w_p =w_i \right \}`$是它的内批正样本集，而$`A(i)`$ \ $`P(i)`$是它的负样本集。这里，$`z = Proj\left ( F_{fre} \right )`$ ，其中 Proj 是一个可学习的多层感知机（MLP），τ 是一个标量温度参数，·表示内积符号。 

**门机制** 如图2所示，参考图像中字符的笔画区域通常稀疏，并且背景噪声会干扰特征提取。为了应对这一挑战，我们提出了一种门机制来选择性地过滤样本信息 I<sub>s</sub> ，如图3所示。具体来说，提取出的样本风格特征 F<sub>spa</sub> 被输入到一个门层中，该层由可学习的全连接层和随后的Sigmoid激活组成，以获得相应的门单元 $`W = \left \{ w_i \right \}_{i=1}^d \in R^d `$。每个单元 w<sub>i</sub> 决定了对应 $`f_{spa}^i`$ 的通过率，在 w<sub>i</sub> 较大的情况下允许更高的通过率。这种设计有效地实现了在抑制多余背景噪声的同时提取出具有信息性的风格特征$`\hat{F}_{spa} = \left \{ \hat{f}_{spa}^i \right \}_{i=1}^{d}`$，其中 $`\hat{f}_{spa}^i = f_{spa}^i · w_i`$。 

<a id="3.3风格内容融合模块Style-contentFusionModule"></a>
### 3.3 风格内容融合模块 Style-content Fusion Module
在获得文本内容特征 E 和两个风格特征 $`\hat{F}_{spa}`$ 、F<sub>fre</sub> 后，我们整合所有特征到两个多头注意力机制中以引导扩散模型的去噪生成过程，如图3所示。具体来说，第一个交叉注意力模块使用文本内容E作为查询来识别样式参考中最相关的样式信息，并由此推断每个字符对应的样式属性。例如，如果文本内容是'a'，它优先搜索样式参考中的'a', 'b', 'd', 'g'等字符的相关样式特征，因为这些字符出现相似的循环结构，暗示着更多的可比样式属性。这个过程（图3中的交叉注意力）表示为： 
```math
O = Atten_1\left ( Q_1 = E,K_1=V_1=\hat{F}_{spa}+F_{fre} \right )
```
随后，我们通过简单地将O和E相加来获取内容和样式指导之间的初始融合嵌入。合并后的中间向量被用作自我关注机制中的查询、键和值，以促进信息的全面交互。最后，混合嵌入g作为扩散过程的条件。第二个多头注意力（图3中的自注意力）定义如下：
```math
g = Atten_2\left ( Q_2 = K_2 = V_2 = O + E \right )
```

<a id="3.4条件扩散模型ConditionalDiffusionModel"></a>
### 3.4 条件扩散模型 Conditional Diffusion Model
条件扩散模型 p<sub>θ</sub> 的目标是生成由获得的条件 g 引导的手写文本的真实图像。具体来说，如图3所示，在g的指导下，p<sub>θ</sub>执行一个去噪生成过程，从采样的高斯噪声 x<sub>T</sub> 开始，逐步去噪以获取所需的手写文本x0： 
```math
p_\theta \left ( x_0|g \right ) = \int p_\theta \left ( x_{0:T}|g \right )d_{x_1:T}, 
```
```math
p_\theta \left ( x_{0:T}|g \right )d_{x_1:T} = p(x_T)\prod_{t=1}^{T}p_\theta(x_{t-1}|xt,g),
```
```math
p_\theta(x_{t-1}|xt,g) = \mathcal{N}(x_{t-1};\mu_\theta(x_t,g,t),\sum_\theta(x_t,g,t)) 
```

去噪过程旨在学习如何逆向执行预先定义的正向过程，如DDPM [20] 中所述。 正向过程被建模为固定马尔可夫链，在此过程中，符合正态分布的噪声逐渐添加到 x<sub>t-1</sub> 以获得 x<sub>t</sub> 。 这可以数学表达如下：
```math
q(x_t|x_{t-1},g) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)
```
其中噪声由方差计划 β<sub>t</sub> 表征。在训练过程中，对最大似然目标应用变分约束来引导生成过程从条件为g的标准高斯噪声 x<sub>T</sub> 中恢复 x<sub>0</sub> 。我们给出的训练目标如下： 
```math
\mathcal{L}_{rec} = \mathbb{E}_{t,q} \left \| \mu_t(x_t,x_0) - \mu_\theta(x_t,g,t) \right \|_2^2
```

其中，$`\mu_t(x_t,x_0)`$ 是后验 $`q(x_t|x_{t-1})`$ 的均值，具有闭式解。我们将在附录B中给出更多细节。
<a id="4.实验Experiments"></a>
## 4.实验 Experiments
<a id="4.1实验设置ExperimentalSettings"></a>
### 4.1 实验设置 Experimental Settings
**数据集** 为了评估我们的One-DM在手写文本生成方面的表现，我们使用了广泛使用的IAM[42]和CVL[30]手写字体数据集。IAM包含来自500个独特作家的62,857个英语单词图像。根据[5,27,46]的方法，我们在训练集中使用了来自339位作者的单词，并用剩余的161位作者进行了测试。CVL包括来自310位作者的英语和德语文字。我们使用了英语部分，总共有84,514个单词，并按照CVL的标准拆分，其中283位作者用于训练，27位作者用于测试。在所有实验中，我们将图像调整为64像素的高度，保持其宽高比不变。

**评估指标** 评估指标。我们使用Fréchet Inception距离（FID）[19]和几何分数（GS）[29]来评估生成质量，遵循设置的[5、27、46]。我们也进行用户研究以量化附录E中生成的手写文本图像的主观质量。 

**实施细节** 在我们的所有实验中，我们仅使用单个风格参考样本。首先，在指导规模为0.25（指导规模是什么）的指导下，我们使用无分类器引导策略[21]训练模型700轮（批大小为384），然后使用文本识别器[49]和CTC损失[17、23、71]对模型进行4500次迭代（批大小为128）的微调，使用四个RTX3090 GPU。微调过程迫使我们的One-DM生成准确内容可读的文字。优化器是AdamW[39]，学习率为1e-4 。在推理过程中，每个风格样本随机采样自目标作者。为了加快采样速度，我们使用去噪扩散隐式模型（DDIM）[54]，共50步。更多详细信息请参见附录C.1。 

**比较方法** 我们比较了我们的One-DM与最先进的手写文本生成方法，包括基于GAN的方法（即GANwriting [27]、HWT [5]、VATr [46]、TS-GAN [9] 和HiGAN+ [15])和扩散模型方法 (即GC-DDPM [12] 和WordStylist [44]) 。为了进行公平的比较，我们在附录C.2中详细说明了所有方法的配置，以生成高度为64像素的图像。此外，在附录D中，我们将One-DM的一个变体与官方的VATr [46] 和HWT [5] 进行比较，该变体可以生成高32像素的图像。 

<a id="4.2主要结论MainResults"></a>
### 4.2 主要结论 Main Results
**风格化手写文本生成** 首先，我们评估我们的One-DM在产生样式化的手写文本图像方面的能力，旨在复制生成图像中的样式和内容。遵循[5,27,46]，我们首先分别计算每个作者的生成样本与真实样本之间的FID，并最终求平均值。与之前的文献[5, 27, 46]一致，我们在IAM数据集上的实验分为四个不同的场景：IV-S、IV-U、OOV-S、OOV-U。在这四个场景中，OOV-U代表训练过程中完全未见过目标样式和单词的最具有挑战性的案例。对于CVL数据集，我们直接报告所有方法在测试集上的结果。 

我们首先在表1中报告了在IAM数据集上的一组量化结果。我们可以观察到，在所有设置下，我们的One-DM都优于所有的竞争对手。值得注意的是，在所有情况下，它显著超过了单次采样方法。令人印象深刻的是，在IV-S和OOV-S设置中，我们的One-DM也明显优于使用15倍更多风格指导样本来改进的少量采样方法（GAN-writing[27]，HWT[5]，VATr[46]）。即使在最具挑战性的OOV-U情况下，我们的One-DM在FID评分上领先第二名的VATr有很大优势（102.75 vs. 108.76），这证明了我们的One-DM在样式化手写文本生成方面的优越性能。同样地，我们的方法在CVL数据集上优于HWT和VATr，实现了最低的FID分数，如表4所示。

![image](https://github.com/user-attachments/assets/ce169c7d-b223-409e-a77b-7648a7595f23)  
(表1：在IAM数据集上，与最先进的方法进行比较。请注意，在GANwriting [27]、HWT [5] 和 VATr [46] 中使用的相同训练集中对所有方法进行了训练。)

![image](https://github.com/user-attachments/assets/433dee89-83d5-48dd-965d-83c86474ccea)
(表4：在CVL数据集上与竞争对手的比较。 )

我们通过图4直观地解释了我们的One-DM的优势，提供了定性的结果。GANwriting难以捕捉参考样本的风格模式，例如字符倾斜，并且偶尔会产生模糊的字符形状。HiGAN+更一致地生成具有正确内容的字符，但生成单词中的字符间距缺乏真实性。WordStylist通常产生带有明显背景噪声的图像。HWT和VATr在内容准确性和风格模仿方面可以生产出令人满意的手写单词；然而，它们的缺点是倾向于创造平滑的字符外观。与HWT和VATr相比，我们的合成样品在更真实的字符墨水颜色和笔画厚度方面表现突出。但是，由我们的One-DM产生的某些样本在墨水颜色上明显不同。我们在附录J中提供全面的解释。此外，我们在图5中进一步展示了我们方法与少量样本方法之间的更多定性比较。

![image](https://github.com/user-attachments/assets/023c7d42-7861-487f-8590-60b4e33c6573)
(图4：在IAM数据集上，我们的方法与最先进的方法之间的定性比较。我们使用相同的引导文本“世界上最大的勇气考验是承受失败而不丧失信心”，指导所有手写生成方法产生文本，并且这些方法都以不同的风格进行输出。更好的放大200%。*表示单次学习方法，而其他的是少量学习方法。 )

![image](https://github.com/user-attachments/assets/ff6ff20c-177d-4a7d-ba1d-e2e9418c894d)
(图5：每行显示了我们在IAM数据集上使用我们的单个DM和少量样本方法的结果；读者被邀请识别我们的方法。答案在论文的结尾处。 )

**风格无关的书写文本生成** 我们进一步评估我们的One-DM，以产生不受风格模仿影响的真实手写文本图像。为此目的，我们在IAM测试集上计算FID和GS，在与ScrabbleGAN[13]（FID：20.72，GS：2.56×10−2）相同的条件下，该方法能够生成随机样式的手写文本。具体来说，每个方法生成25K个随机样本来计算FID，与来自测试集的25K个样本进行比较，并且在与5K测试集样本进行比较时，使用5K个随机样本计算GS。如表1所示，我们的One-DM在FID和GS指标中均取得了最佳结果，进一步证明了其生成高质量手写文本图像的能力。 

<a id="4.3分析Analysis"></a>
### 4.3 分析 Analysis
在本节中，我们对我们的One-DM进行消融研究以分析它。在补充材料中提供了更多的分析，包括不同风格背景下的泛化评估、通过OCR性能的生成质量评估、失败案例分析以及不同设计的影响（例如高频滤波器、风格内容融合机制和风格输入样本长度）。

**对拉普拉斯分支和门机制进行定量评估** 我们通过在IAM数据集上执行各种消融实验来评估我们方法中不同组件的效果，并提供定量结果，如表2所示。我们发现：（1）同时包含拉普拉斯分支和门机制可以提高生成的手写文本图像的质量，分别提高了FID的3.92和2.71。（2）将拉普拉斯分支与门机制集成进一步提升了生成性能。 

**对拉普拉斯分支和门机制进行定性评估** 为了进一步分析我们的一体化 DM，我们进行了视觉消融实验。如表 2 所示，我们可以观察到：首先，在添加了门机制后，背景噪声可以得到一定程度的抑制，从而使得字符具有相对干净的背景；然后，独立地增加拉普拉斯分支有助于模型学习草书连接和其他风格模式。最后，我们的方法将拉普拉斯分支和门机制集成在一起，可以生成最高质量的手写文本图像。 

**关于Laplacian分支的讨论** 我们的Laplacian分支由两个关键组件组成：利用高频图像 H<sub>s</sub> 和 Laplace 对比学习损失 L<sub>lapNCE</sub>。在之前的消融研究中，它们总是结合在一起。我们进一步进行探索性实验来解释为什么它们不能分离。如表3所示，将 H<sub>s</sub> 与 L<sub>lapNCE</sub> 结合起来可以最大化有效性；而将其分开则显著降低了性能。没有 L<sub>lapNCE</sub> 的指导，从 H<sub>s</sub> 中提取鉴别特征是具有挑战性的。同样地，直接将 L<sub>lapNCE</sub> 应用于原始图像会导致不希望出现的风格特征提取，因为原始图像比 H<sub>s</sub> 更少清晰的风格模式。 

**从单一参考样本讨论学习风格** 我们很惊讶，One-DM 只有一个参考样本，甚至超过了少量样本方法的生成性能。我们在下面提供了潜在原因分析。首先，One-DM 学习了一个有意义的风格潜在空间，在这个空间中可以基于已见风格生成新风格（参见图 6）。然后，通过我们的风格增强模块，One-DM 有效地从一个示例中提取出风格特征，并将其映射到特征空间中的一个位置，从而产生高质量的手写文本图像。 

<a id="4.4与SOTA工业方法的比较ComparisonswithSOTAIndustrialMethods"></a>
### 4.4 与SOTA工业方法的比较 Comparisons with SOTA Industrial Methods
为了突出我们方法的优势，我们在IAM数据集上比较了One-DM与两个训练在大量数据集上的工业级图像生成方法（包括许多以文本为中心的图像），包括两个著名的文本到图像的方法DALL-E3 [4] 和Stable Diffusion (SD) [50], 以及两个流行的风格迁移方法Artbreeder1和IP-Adapter2(IP-A.)。进一步实验细节见附录C.3。 

如图7（a）所示，我们的方法在风格模仿和内容保留方面优于工业方法。IP-A的性能最差，经常产生变形图像。Artbreeder可以复制从样式样本中提取的笔画颜色，但无法保留内容。DALL-E3 和SD生成的内容准确，但在参考细节上与风格不符，例如字符间距和笔画宽度，SD 常常生成额外的背景。此外，我们还比较了Fzshouji3，这是一种先进的工业方法，用于中文手写体生成。如图7（b）所示，在复制字符细节和墨水颜色方面，我们的方法优于Fzshouji。 

![image](https://github.com/user-attachments/assets/038643cc-93a7-409d-bd4e-e6faf504ce20)  
(（a）手写文本生成的比较 （b）中文手写文本生成的比较 )

<a id="4.5在其他语言上的应用ApplicationstoOtherLanguages"></a>
### 4.5 在其他语言上的应用 Applications to Other Languages
 在本节中，我们评估One-DM是否可以用于生成除英语以外的语言。我们进一步在中文（即ICDAR-2013竞赛数据库[64]）和日文（即UP_Kuchibue数据库[24]）数据集上进行实验。我们使用FID来评估每个作者生成样本的质量，并取平均值。更多实验细节请参见附录C.4。 

对于中文手写字符生成任务，在表5中，我们发现我们的One-DM比第二好的方法大幅提高了性能，实现了更低的FID得分7.37。从图8（a）中我们可以观察到，我们的One-DM生成的手写字母形状和字迹倾斜都与目标图像非常接近。相比之下，HWT和VATr的手写字母表现出明显的模糊和结构坍塌等缺陷。GANwriting倾向于在其手写字母中遗漏笔画。WordStylist有时难以准确模仿风格模式，往往生成带有错误部首的字母。表5和图8（b）进一步验证了One-DM对日本手写生成的有效性。我们也取得了最低的FID分数，我们生成的日语样例在内容保留和风格模仿方面都非常出色。

![image](https://github.com/user-attachments/assets/93e804d4-6c1d-4fc6-83d6-00f79c983b30)
(表5：在FID方面，与竞争对手的样式化手写字符生成进行定量比较。)

![image](https://github.com/user-attachments/assets/56b64ee2-5c4c-46a4-a4e8-ce7eadd9950b)
(图8：与GANwriting [27]，HWT [5]，VATr [46] 和WordStylist [44]的比较。红色方框突出显示结构保存失败，而蓝色方框则突出显示目标样式模式和生成字符之间的比较。)

我们进一步研究了为什么仅需要一个样本的扩散方法（我们的One-DM和Word Stylist）在生成中文和日文字符方面明显优于基于GAN的方法（即GANwriting，HWT和VATr）。中文和日文字符上基于GAN的方法性能较低可能源于其简单的卷积架构难以处理中文和日文字符复杂的几何结构。相比之下，我们的One-DM将中文和日文字符的生成分解为更简单的过程步骤。例如，在表6中所示，扩散生成过程的早期阶段，模型首先尝试生成粗略的手写中文字符。然后继续在条件引导下对书写风格进行细化（例如，字符形状和笔画颜色），直到合成满意的手写体为止。 
 

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion
本文介绍了一种新颖的One-DM，用于手写文本生成，仅需要一个风格参考即可产生逼真的手写文本图像。我们通过将高频率成分从风格参考中集成到样式提取中来增强样式提取。对于具有明显样式模式的高频成分，我们采用拉普拉斯对比学习来捕获更多有鉴别力的样式特征。此外，门机制改进了从参考中转移信息特征的过程，减少了背景噪声。我们的One-DM在多个语言脚本中优于少量样本方法。未来，我们将探索One-DM在字体生成和矢量字体创建任务中的潜力。 

<a id="5.结论Conclusion"></a>
## 5.附录 Conclusion









