# 聚类检测网络ClusDet-Clustered Object Detection in Aerial Images.

[**网络总结**](#网络总结)  
[**0.摘要 Abstract**](#0.摘要Abstract)  
[**1.介绍 Introduction**](#1.介绍Introduction)  
[**2.相关工作 Related work**](#2.相关工作Relatedwork)  
[**3.网络结构**](#3.网络结构)  
[**4.实验 Experiments**](#4.实验Experiments)  
[**5.结论 Conclusion**](#5.结论Conclusion)  



## 网络总结
参考博文链接：  
参考视频链接：  
源码链接：  
论文链接：  

<a id="0.摘要Abstract"></a>
## 0.摘要 Abstract
在航拍图像中检测物体是具有挑战性的，至少有两个原因：  
**（1）目标物体（如行人）在像素上非常小，使它们很难与周围背景区分开来；**  
**（2）目标通常分布稀疏且不均匀，使得检测非常低效。**  


本文针对这两个问题提出了一种解决方案，灵感来自于观察到这些目标通常是聚集在一起的。具体而言，我们提出了一种名为“Clustered Detection (ClusDet)”的网络，它将对象聚类和检测统一在一个端到端的框架中。  
ClusDet的关键组件包括**聚类提议子网络（CPNet）**、**尺度估计子网络（ScaleNet）** 和 **专用检测网络（DetecNet）**。给定输入图像，CPNet生成对象聚类区域，ScaleNet估计这些区域的对象尺度。然后，每个经过尺度归一化的聚类区域都被送入DetecNet进行对象检测。  


ClusDet相比以前的解决方案具有几个**优点**：  
**（1）它大大减少了最终对象检测的图像块数量，从而实现了高运行时间效率；**  
**（2）基于聚类的尺度估计比以前使用的基于单个对象的估计更准确，因此有效地改善了小目标的检测；**   
**（3）最终的DetecNet专门针对聚类区域，并隐含地模拟先前的上下文信息，从而提高了检测精度。**  
该方法在三个流行的航拍图像数据集（包括VisDrone、UAVDT和DOTA）上进行了测试。在所有实验中，ClusDet与最先进的检测器相比取得了很好的性能。代码在 https://github.com/fyangneil

<a id="1.介绍Introduction"></a>
## 1.介绍 Introduction
随着深度神经网络的发展，目标检测（例如Faster R-CNN [27]、YOLO [25]、SSD [23]）在自然图像（例如MS COCO[22]中的600×400像素图像）方面近年来取得了巨大进展。尽管这些检测器在一般目标检测方面表现出有前途的结果，但它们在航拍图像（例如VisDrone [37]中的2,000×1,500像素图像）上的性能在准确性和效率方面都远远不如人意，这是由两个挑战引起的：（1）相对于图像，目标通常具有较小的尺度；（2）目标通常在整个图像中分布稀疏且不均匀。

与自然图像中的目标相比，尺度挑战导致深度网络在航拍图像中对目标的特征表示效果较差。因此，现代检测器很难有效利用外观信息来区分目标与周围背景或相似对象。为了解决尺度问题，一个自然的解决方案是将航拍图像划分为几个均匀的图像块，然后对每个图像块进行检测[10, 24]。虽然这些方法在一定程度上缓解了分辨率挑战，但由于忽视了目标的稀疏性，执行检测时效率低下。因此，大量的计算资源被低效地应用于稀疏甚至没有目标的区域（见图1）。  
![image](https://github.com/Cloud-Jowen/CVPaper_Note/assets/56760687/3673e1c8-f1d5-4928-937c-f0ffe0114dbe)  

(图1:基于网格的均匀划分和提出的基于聚类的划分的比较。为了叙述目的，我们故意将图像块分为稀疏、普通和聚类三种类型。我们观察到，在基于网格的均匀划分中，超过73%的图像块是稀疏的（包括23%的图像块没有目标），约25%的图像块是普通的，约2%的图像块是聚类的。相比之下，对于基于聚类的划分，约50%的图像块是稀疏的，35%是普通的，约15%属于聚类图像块，这是基于网格划分的7倍之多。)**(2023.10.25 Jowen:图一主要是证明 gt 在图像中是成聚类分布的,按照聚类对图片划分的出的图像块包含的 gt 更多一些)**

我们从图1中观察到，在航拍图像中，目标不仅稀疏且不均匀分布，而且往往在某些区域呈高度聚集。例如，行人通常集中在广场上，车辆集中在高速公路上。因此，提高检测效率的一种直观方法是将检测器聚焦在这些聚集区域上，这些区域有大量的目标存在。  

本文受到这一动机的启发，提出了一种新颖的聚类检测（ClusDet）网络，通过将目标和聚类检测集成到一个统一的框架中来解决上述两个挑战。如图2所示  
![image](https://github.com/Cloud-Jowen/CVPaper_Note/assets/56760687/9c183619-15ae-4554-b28f-d007471fed65)  
图2：聚类对象检测（ClusDet）网络。ClusDet网络由三个关键组件组成：（1）聚类提案子网（CPNet）；（2）尺度估计子网（ScaleNet）；以及（3）专用检测网络（DetecNet）。CPNet用于预测聚类区域。ScaleNet用于估计聚类中物体的尺度。DetecNet在聚类图像块上执行检测。最终的检测结果通过融合来自聚类图像块和全局图像的检测结果生成。ICM（迭代聚类合并）和PP（分区和填充）的详细信息在第3节中给出。

ClusDet包括三个关键组件，包括聚类提议子网络（CPNet）、尺度估计子网络（ScaleNet）和基线检测网络（DetecNet）。根据航拍图像的初始检测结果，CPNet生成一组目标聚类区域。在获取聚类区域之后，它们被裁剪出来进行后续的精细检测。为此，这些区域首先必须被调整大小以适应检测器，这可能会导致聚类区域中的目标过大或过小，从而降低检测性能[30]。为了解决这个问题，我们提出了ScaleNet来估计每个聚类图像块中目标的适当尺度，然后在将其馈送到检测器之前相应地调整图像块的大小，这与[10、24、18]通过直接调整裁剪图像块的方法不同。然后，每个聚类图像块都被馈送到专用的检测器DetecNet进行精细检测。最终的检测结果是通过融合聚类图像块和全局图像上的检测结果来实现的。  

与之前的方法相比，提出的ClusDet具有以下几个优势：（i）由于CPNet的存在，我们只需要处理具有大量目标的聚类区域，这显著降低了计算成本并提高了检测效率；（ii）借助ScaleNet的帮助，每个聚类图像块都经过精细调整以实现更好的后续细节检测，从而提高了准确性；（iii）DetecNet专门用于聚类区域检测，并隐式地对先前的上下文信息进行建模，以进一步提高检测准确性。在三个航拍图像数据集上的广泛实验中，ClusDet在使用单一方法的同时以较少的计算成本实现了最佳性能。

总结起来，该论文的贡献如下：
1）提出了一种新颖的ClusDet网络，用于同时解决航拍图像中目标检测的尺度和稀疏性挑战。
2）提出了一种有效的ScaleNet，以缓解聚类图像块中非均匀尺度问题，实现更好的细节检测。
3）在包括VisDrone [37]、UAVDT [8]和DOTA [33]在内的三个代表性航拍图像数据集上实现了最先进的性能，并且计算成本较低。

<a id="2.相关工作Relatedwork"></a>
## 2.相关工作 Related work
目标检测在过去几十年中得到了广泛的研究，有大量的文献可供参考。接下来，我们首先回顾了与我们的工作最相关的三个研究方向，然后强调了ClusDet与现有方法的区别。

**通用目标检测**。受图像识别（Image Recognition）的成功启发[17]，深度卷积神经网络（CNNs）在目标检测中占据主导地位。根据检测流程，现有的检测器大致可以分为两类：基于区域的检测器和无区域的检测器。基于区域的检测器将检测分为两个步骤，包括提取候选区域（即proposals）和目标检测。在第一阶段，通过提取候选区域显著减少了检测的搜索空间。在第二阶段，这些候选区域进一步被分类到具体的目标类别中。基于区域的检测器的代表包括R-CNN [12]、Fast/er R-CNN [11, 27]、Mask R-CNN [14]和Cascade R-CNN [3]。相反，无区域的检测器，如SSD [23]、YOLO [25]、YOLO9000 [26]、RetinaNet [21]和RefineDet [36]，在没有区域提议的情况下进行检测，这牺牲了一定的准确性，但具有较高的效率。

尽管这些通用检测器在自然图像上（例如PASCAL VOC [9]中的500×400图像和MS COCO [22]中的600×400图像）表现出优秀的性能，但当应用于高分辨率航拍图像（例如VisDrone [37]中的2,000×1,500图像，甚至更大的无人机捕获图像[19]）时，它们的性能会降低。值得注意的是，最近高分辨率图像中的目标检测引起了越来越多的研究关注[32]。

**航拍图像检测**。与自然图像中的检测相比，航拍图像中的检测更具挑战性，因为（1）对象相对于高分辨率航拍图像具有较小的尺度，（2）目标是稀疏和不均匀的，并且集中在某些区域。由于本文侧重于深度学习，我们仅回顾一些使用深度神经网络进行航拍图像检测的相关工作。在[28]中，提出了一种基于简单CNN的方法，用于自动检测航拍图像中的目标。在[2]中，将航拍图像中的检测与语义分割相结合以提高性能。在[31]中，作者直接扩展了Fast/er R-CNN [11, 27]以用于航拍图像中的车辆检测。[6]的工作提出了一种耦合的基于区域的CNN，用于航空器检测。[7]的方法研究了航拍图像检测中Region of Interests (RoI)和对象之间的错位问题，并引入了ROI变换器来解决这个问题。[35]的算法提出了一种适应尺度的候选区域网络，用于航拍图像中的目标检测。

**区域搜索策略**。在目标检测中，通常采用区域搜索策略来处理小目标。[24]的方法提出了自适应地将计算资源引导到稀疏和小目标所在的子区域。[1]的工作引入了一种上下文驱动的搜索方法，以高效地定位包含特定类别对象的区域。[4]的作者提出了通过学习上下文关系来动态探索基于候选区域的目标检测中的搜索空间的方法。[10]的方法提出利用强化学习来顺序选择更高分辨率尺度上的检测区域。在更具体的领域中，如广域航拍运动影像（WAMI）中的车辆检测，[18]的工作建议使用两阶段的时空卷积神经网络从WAMI序列中检测车辆。

**本文方法**。本文旨在解决航拍图像检测中的两个挑战。我们的方法与之前基于区域搜索的检测器（如[24，10]）相关但不同，这些检测器将高分辨率图像分割成小的均匀图像块进行检测。相反，我们的解决方案首先在图像中预测聚类区域，然后提取这些聚类区域进行精细检测，从而显着减少计算成本。虽然[18]的方法也在可能包含对象的图像块上执行检测，但我们的方法与其有很大不同。在[18]中，获得的图像块直接调整大小以适应后续检测器进行检测。相反，受[30]中观察到的极端尺度对象可能会降低检测性能的启发，我们提出了一个ScaleNet来缓解这个问题，在每个图像块上进行精细检测，从而提高检测性能。

<a id="3.网络结构"></a>
## 3.网络结构
### 3.1 概述 Overview
如图2所示，航拍图像的检测由三个阶段组成：聚类区域提取，聚类图像块上的精细检测和检测结果的融合。具体而言，在提取航拍图像的特征后，CPNet将特征映射作为输入并输出聚类区域。为了避免处理过多的聚类图像块，我们提出了一个迭代聚类合并（ICM）模块来减少噪声聚类图像块。然后，将聚类图像块以及全局图像的初始检测结果输入ScaleNet，以估计聚类图像块中对象的适当尺度。利用尺度信息，对聚类图像块进行缩放，并使用DetecNet进行精细检测。最终的检测结果通过标准的非极大值抑制（NMS）融合每个聚类图像块和全局图像的检测结果得到。
### 3.2 簇区域提取 Cluster Region Extraction
聚类区域提取包括两个步骤：使用聚类提案子网络（CPNet）进行初始聚类生成和使用迭代聚类合并（ICM）进行聚类减少。
#### 3.2.1 聚类提案子网络（CPNet） Cluster Proposal Sub-network (CPNet)
聚类区域提取的核心是聚类提案子网络（CPNet）。CPNet在航拍图像的高级特征图上进行操作，旨在预测聚类的位置和尺度。受到区域提案网络（RPN）[27]的启发，我们将CPNet构建为完全卷积网络的一个模块。具体而言，CPNet以来自特征提取骨干网络的高级特征图为输入，并分别利用回归和分类两个子网络。虽然我们的CPNet与RPN有相似的思想，但它们是不同的。RPN用于提出对象的候选区域，而CPNet旨在提出聚类的候选区域。与对象提案相比，聚类的尺寸要大得多，因此CPNet需要比RPN更大的感受野。因此，我们将CPNet附加在特征提取骨干网络的顶部。

值得注意的是，CPNet的学习是一个有监督的过程。然而，目前公开的数据集中没有提供聚类的标注信息。在这项工作中，我们采用了一种简单的策略来生成训练CPNet所需的聚类标注信息。关于生成聚类标注的详细信息，请参考补充材料。

#### 3.2.2 迭代聚类合并（ICM） Iterative Cluster Merging (ICM)
![image](https://github.com/Cloud-Jowen/CVPaper_Note/assets/56760687/ec65ce52-5238-4592-8404-22932bb9a4bd)

(图3：聚类检测的合并示意图。红色框表示来自CPNet的聚类检测，蓝色框表示经过迭代聚类合并（ICM）后的聚类。)  

如图3（a）所示，我们观察到CPNet生成的初始聚类区域是密集且杂乱的。这些密集且杂乱的聚类区域由于重叠度高、尺寸大，很难直接用于细粒度检测，导致实际应用中计算负担极重。为了解决这个问题，我们提出了一个简单但有效的迭代聚类合并（ICM）模块来清理聚类。设B = $`\left \{ B_{i}    \right \}_{i=1}^{N_{B}}`$ 表示CPNet检测到的$` N_B `$个聚类边界框的集合，R = $`\left \{ R_{i}    \right \}_{i=1}^{N_{B}}`$表示对应的聚类分类分数。通过预定义的重叠阈值τop和合并后的最大聚类数量 $` Nmax `$，我们可以使用算法1得到合并后的聚类集合$`B^{'} = \left \{ B_{i}^{'}    \right \}_{i=1}^{N_{B^{'}}}`$，其中$`N_{B^{'}}`$表示合并后的聚类数量。

![image](https://github.com/Cloud-Jowen/CVPaper_Note/assets/56760687/49cf1f01-5bd1-4692-997d-fc2b57bdea18)


简而言之，我们首先找到得分最高的$`Bi`$，然后选择与$`Bi`$重叠度大于阈值$`τop`$的聚类与$`Bi`$合并。所有合并后的聚类都被移除。之后，我们重复上述过程直到$`B`$为空。上述过程对应算法1中的非最大化合并（NMM）。我们多次执行NMM，直到达到预设的$` Nmax `$。关于NMM的详细信息，请参考补充材料。图3（b）展示了最终合并后的聚类，表明提出的ICM模块能够有效地合并密集且杂乱的聚类。

### 3.3 基于图像块的细粒度检测  Fine Detection on Cluster Chip
在获取聚类图像块之后，我们使用专用的检测器对这些图像块进行细粒度检测。与现有方法[24, 18, 10]直接调整图像块大小进行检测不同，我们提出了一个尺度估计子网络（ScaleNet）来估计图像块中物体的尺度，从而避免了极端尺度的物体降低检测性能。根据估计的尺度，ClusDet对每个图像块执行分割和填充（PP）操作以进行检测。

#### 3.3.1 尺度估计子网络 Scale Estimation Sub-network (ScaleNet)
我们将尺度估计视为回归问题，并使用一组全连接网络构建了尺度估计子网络（ScaleNet）。如图4所示，ScaleNet接收三个输入，包括从网络主干提取的特征图、聚类边界框和全局图像上的初始检测结果，并针对聚类芯片中的对象输出相对尺度偏移量。在这里，初始检测结果是从检测子网络中获得的。

![image](https://github.com/Cloud-Jowen/CVPaper_Note/assets/56760687/d40d740c-f600-4f3c-9d33-be6ce4842be4)

图4：尺度估计网络（ScaleNet）的体系结构。聚类检测结果被投影到特征图空间中。每个聚类都被汇集到一个固定大小的特征映射中，并通过全连接层（FC）映射为一个特征向量。该网络对每个聚类都有一个输出，即尺度回归偏移量。

设 $` t_{i}^{*} = (p_{i}-s_{i}^{*}) / p_{i} `$ 为聚类i的相对尺度偏移量，其中 $`p_{i}`$ 和 $`s_{i}^{*}`$ 分别表示检测到的对象的参考尺度和聚类i中真实边界框的平均尺度。因此，ScaleNet的损失可以数学地定义为：
```math
\iota ({\left \{t_i  \right \}}) = \frac{1}{M} \sum_i^M\ell _{reg}(t_{i},t_{i}^{*})
```
其中 $` t_i = (p_i−s_i)/p_i `$ 是估计的相对尺度偏移，$`s_i`$ 是估计的尺度，$`M`$ 是聚类框的数量。$`L_{reg}`$ 是 smooth L1 损失函数 [11]。

#### 3.3.2  分区和填充 Partition and Padding
使用分区和填充（PP）操作，以确保对象的尺度在合理范围内。给定聚类边界框  $` B_i  `$，对应的估计对象尺度 $` S_i  `$ 和检测器的输入尺寸$` S_in  `$，我们可以在检测器的输入空间中估计对象的尺度 $` S_{i}^{in} = S_i \times  \frac{S_{in}}{S_i} `$。如果尺度 $` S_{i}^{in} `$大于一定范围，就按比例填充该聚类，否则将其分成两块相等的片段。需要注意的是，最终检测中会忽略填充区域内的检测结果。该过程的可视化见图5。具体的尺度范围设置在第4节讨论。调整完聚类片段的尺度后，专门的基准检测网络（DetecNet）执行精细的目标检测。DetecNet的架构可以是任何最先进的检测器。检测器的主干网络可以是任何标准的主干网络，例如VGG [29]，ResNet [15]，ResNeXt [34]。


### 3.4 通过局部-全局融合进行最终检测 Final Detection with Local-Global Fusion
通过使用标准的非极大值抑制（NMS）后处理，将聚类图像块的局部检测结果和整个图像的全局检测结果融合起来，可以得到航空图像的最终检测结果（参见图6）。局部检测结果是通过上述提出的方法获得的，而全局检测结果则来自于检测子网络（图2）。值得注意的是，任何现有的现代检测器都可以用于全局检测。

<a id="4.实验Experiments"></a>
## 4.实验 Experiments

### 4.1. 实现细节 Implementation Details
我们基于公开可用的Detectron [13]和Caffe2实现了ClusDet。我们采用带有特征金字塔网络（FPN）[20]的Faster R-CNN（FR-CNN）[27]作为基准检测网络（DetecNet）。CPNet的架构是由一个5×5的卷积层和两个兄弟1×1的卷积层（分别用于回归和分类）实现的。在ScaleNet中，将特征图转换为特征向量的全连接层大小为1024；尺度偏移回归器中全连接层的大小分别为1024和1。在NMM过程中，用于合并聚类的IoU阈值设置为0.7。根据COCO[22]数据集的定义，聚类片段分区和填充中的对象尺度范围设置为[70, 280]像素。

**训练阶段**。在VisDrone [37]和UAVDT [8]数据集上，检测器的输入尺寸设置为600×1,000像素，而在DOTA [33]数据集上设置为1,000×1,000像素。在这三个数据集上，训练数据通过将图像分割成小块来进行增强。在VisDrone [37]和UAVDT [8]数据集上，每个图像被均匀地分成6块和4块，且不存在重叠。设置特定数量的小块的原因是为了使裁剪后的小块大小与COCO [22]数据集中的大小相似。在DOTA [33]数据集上，我们使用作者提供的工具来分割图像。当使用2个GPU在VisDrone [37]和UAVDT [8]数据集上训练模型时，我们将基本学习率设置为0.005，总迭代次数设置为140k。在前120k次迭代后，学习率降至0.0005。然后，在将学习率降至0.00005之前，我们再进行100k次迭代的训练。采用了0.9的动量和0.0005的参数衰减（应用于权重和偏置）。在DOTA [33]数据集上，基本学习率和总迭代次数分别设置为0.005和40k。在30k和35k次迭代后，学习率减小了0.1倍。

**测试阶段**。检测器的输入尺寸在未特别指定时与训练阶段相同。在生成聚类芯片时，VisDrone [37]数据集上的最大聚类数（TopN）经验性地设置为3，UAVDT [8]为2，DOTA [33]为5。在融合检测时，标准非极大值抑制（NMS）的阈值在所有数据集上均设置为0.5。最终检测数量设置为500。

### 4.2. 数据集 Datasets
为了验证所提方法的有效性，我们在三个公开可访问的数据集上进行了大量实验：VisDrone [37]、UAVDT [8]和DOTA [33]。

**VisDrone** VisDrone数据集包括10,209张图像（6,471张用于训练，548张用于验证，3,190张用于测试），涵盖了十种物体的丰富注释。该数据集的图像尺度约为2,000×1,500像素。由于评估服务器目前已关闭，我们无法在测试数据集上测试我们的方法。因此，验证数据集被用作测试数据集来评估我们的方法。

**UAVDT** UAVDT数据集包含23,258张训练数据图像和15,069张测试数据图像。图像的分辨率约为1,080×540像素。该数据集是在多个城市的多个位置使用无人机平台获取的。标注的对象类别包括汽车、公交车和卡车。

**DOTA** DOTA数据集是从多个传感器和平台（例如Google Earth）收集的，具有多种分辨率（从800×800到4,000×4,000像素），涵盖了多个城市。选择了十五种类别进行注释。考虑到ClusDet是基于航空图像中对象的聚类特征，数据集中的一些类别不适合ClusDet，例如环形交叉路口、桥梁。因此，我们只选择了数据集中具有可移动对象的图像来评估我们的方法，即飞机、船、大型车辆、小型车辆和直升机。因此，训练和验证数据分别包含920张图像和285张图像。

### 4.3 对比方法 Compared Methods
我们将我们的ClusDet方法与均匀图像分区（EIP）方法在所有数据集上进行比较。在某些数据集上，如果没有提供EIP，我们将根据数据集的属性来实现它。此外，我们还将我们的方法与所有数据集上的代表性最新方法进行比较。

### 4.4. 评估指标 Evaluation Metric
根据COCO [22]数据集的评估协议，我们使用AP、AP50和AP75作为衡量精度的指标。具体来说，AP是通过对所有类别进行平均计算得出的。AP50和AP75是在所有类别上使用单一IoU阈值0.5和0.75进行计算的。效率是通过需要处理的图像数量以及在推断阶段处理全局图像和其切片的平均时间来衡量的。具体而言，图像数量指的是全局图像和裁剪后的切片的总和。在随后的实验中，图像数量被表示为#img。

### 4.5. Ablation Study
为了验证聚类检测和尺度估计对检测性能的贡献，我们在VisDrone [37]数据集上进行了大量实验。在接下来的实验中，检测器在测试阶段的输入尺寸设置为600 × 1,000像素。为了验证所提出的方法在不同骨干网络下是否能够持续改善性能，我们使用了三种骨干网络进行实验：ResNet-50 [15]、ResNet-101 [15]和ResNeXt-101 [34]。

**EIP的影响**。实验结果列在表1中。我们注意到，相对于COCO数据集（AP=36.7），FRCNN [27]在VisDrone [37]数据集上表现较差。这是因为VisDrone中物体相对图像的尺度要小得多。通过将EIP应用于图像，检测器的性能显著提高，特别是在小物体（APs）上。然而，需要处理的图像数量增加了6倍（3,288张 vs 548张）。此外，尽管应用EIP后整体性能AP得到了改善，但大尺度物体（APl）的性能下降了。这是因为EIP将大对象截断成片段，导致了许多误报。

**聚类检测的影响**。从表1中可以看出，DetecNet+CPNet处理的图像数量要少得多（1,945张 vs 3,288张），但性能优于FRCNN [27]加EIP。这表明CPNet不仅选择了聚类区域以节省计算资源，还隐含地编码了先前的上下文信息以提高性能。此外，我们注意到，与EIP相比，CPNet没有降低大尺度物体（APl）的性能，这可以归因于CPNet引入了对象的空间分布信息到聚类检测网络中，以避免截断大对象。

**尺度估计的影响**。将ScaleNet集成到CPNet和DetecNet后，我们注意到处理的图像数量增加到了2,716张，这是因为PP模块将一些聚类芯片分割成了片段。这在执行检测时缓解了小尺度问题，使得性能（AP）在ResNet50 [15]骨干网络上提高到了26.7。此外，我们看到ScaleNet改善了所有类型的骨干网络上的检测性能。特别是，指标AP50提高了2-3个点。此外，即使在非常强大的骨干网络ResNeXt101 [15]上，APs也提高了1.6个点。这表明ScaleNet在一定程度上缓解了尺度问题。

**超参数TopN的影响**。为了公平地研究TopN的作用，我们只在测试阶段改变设置，避免了训练数据量的影响。从图7中可以看出，在TopN = 4之后，处理的图像数量逐渐增加，但AP并没有太大变化，只是在AP = 27周围波动。这意味着当TopN设置为较高值时，许多聚类区域会被重复计算。这一观察结果还表明，聚类合并操作对降低计算成本至关重要。

### 4.6 Quantitative Results
**VisDrone** 我们提出的方法与代表性的检测器，如Faster RCNN [27]和RetinaNet [21]在表2中展示了检测性能。我们注意到，我们的方法在各种骨干设置下都明显优于现有的最先进方法。此外，我们观察到，在使用多尺度设置（用?表示）测试模型时，性能显著提升，除了使用EIP的方法。这是因为在多尺度测试中，裁剪的芯片被调整到非常大的尺度，导致检测器在背景或目标的局部区域上输出了许多误检。

**UAVDT** 在UAVDT [8]数据集上的实验结果显示在表3中。与FRCNN [27]+FPN [20]不同，其他比较方法的性能是使用[8]提供的实验结果计算的。从表3中，我们观察到，在测试数据上应用EIP并没有改善性能。相反，它显著降低了性能（11.0 vs 6.1）。这种现象的原因是，UAVDT中的目标，即车辆，总是出现在图像的中心，而EIP操作将目标分割成碎片，使得检测器无法正确估计目标的尺度。与FRCNN [27]+FPN [20]（FFPN）相比，我们的ClusDet优于FFPN和FFPN+EIP。性能改善主要得益于不同的图像裁剪操作。在我们的方法中，图像是基于聚类信息进行裁剪的，这样做不太可能截断大量的目标。在UAVDT [8]上的检测器性能要远远低于VisDrone [38]上的性能，这是由于数据极度不平衡造成的。

**DOTA**在DOTA [33]数据集上，我们的ClusDet实现了与最先进方法类似的性能，但处理的图像芯片数量显著减少。这是因为CPNet显著减少了用于精细检测的芯片数量。尽管在低IoU（AP50）方面，我们的方法在整体性能方面并未超越最先进的方法，但它获得了更高的AP75值，这表明我们的方法可以更精确地估计目标的尺度。此外，我们观察到，当采用更复杂的骨干网络时，性能变化不大。这可以归因于训练图像有限。没有大量数据的情况下，复杂模型无法发挥其优势。

<a id="5.结论Conclusion"></a>
## 5.结论 Conclusion
我们提出了一个聚类对象检测（ClusDet）网络，将对象聚类和检测统一到一个端到端的框架中。我们展示了ClusDet可以成功地预测图像中的聚类区域，从而显著减少了检测所需的图像块数量，提高了效率。此外，我们提出了一种基于聚类的物体尺度估计网络，以有效地检测小物体。另外，我们通过实验证明，所提出的ClusDet网络隐式地模拟了先前的上下文信息，以提高检测精度。通过大量实验，我们展示了我们的方法在三个公共数据集上获得了最先进的性能。









